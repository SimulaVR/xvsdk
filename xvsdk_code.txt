Project Path: nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907

Source Tree:

```
nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907
├── src
│   ├── all_stream
│   │   ├── raw2opencv.cpp
│   │   ├── colors.h
│   │   ├── CMakeLists.txt
│   │   ├── fps_count.hpp
│   │   └── all_stream.cpp
│   ├── demo-api
│   │   ├── pipe_srv.h
│   │   ├── demo-api.cpp
│   │   ├── CMakeLists.txt
│   │   └── fps_count.hpp
│   ├── CMakeLists.txt
│   └── pipe_srv
│       ├── pipe_srv.h
│       └── pipe_srv.cpp
├── include
│   └── xvsdk
│       ├── xv-types.h
│       └── xv-sdk.h
├── share
│   ├── doc
│   │   └── xvsdk
│   │       ├── structxv_1_1DeviceSetting.html
│   │       ├── inherit_graph_41.map
│   │       ├── inherit_graph_60.map
│   │       ├── structxv_1_1DepthImage-members.html
│   │       ├── structxv_1_1RgbImage.html
│   │       ├── inherit_graph_12.png
│   │       ├── inherit_graph_65.map
│   │       ├── classxv_1_1IrisStream.html
│   │       ├── structxv_1_1TransformQuat.html
│   │       ├── inherit_graph_25.png
│   │       ├── functions_v.html
│   │       ├── structxv_1_1ExpSetting-members.html
│   │       ├── classxv_1_1details_1_1PosePred__.html
│   │       ├── inherit_graph_37.md5
│   │       ├── inherit_graph_27.png
│   │       ├── inherit_graph_4.png
│   │       ├── structxv_1_1XV__ET__INIT__PARAM-members.html
│   │       ├── unionxv_1_1XV__ET__POINT__2D-members.html
│   │       ├── inherit_graph_55.map
│   │       ├── inherit_graph_8.map
│   │       ├── inherit_graph_63.map
│   │       ├── tab_h.png
│   │       ├── inherit_graph_19.md5
│   │       ├── structxv_1_1AwbSetting-members.html
│   │       ├── structxv_1_1ExpSetting.html
│   │       ├── inherit_graph_14.png
│   │       ├── inherit_graph_29.md5
│   │       ├── inherit_graph_63.png
│   │       ├── structxv_1_1Triple-members.html
│   │       ├── inherit_graph_1.map
│   │       ├── structxv_1_1Object_1_1keypoint.html
│   │       ├── inherit_graph_18.map
│   │       ├── inherit_graph_28.md5
│   │       ├── classxv_1_1ColorCamera-members.html
│   │       ├── modules.html
│   │       ├── unionxv_1_1XV__ET__POINT__3D-members.html
│   │       ├── inherit_graph_48.png
│   │       ├── functions_func.html
│   │       ├── classxv_1_1GPSStream.html
│   │       ├── structxv_1_1MicData-members.html
│   │       ├── doc.png
│   │       ├── inherit_graph_26.png
│   │       ├── inherit_graph_67.map
│   │       ├── inherit_graph_35.md5
│   │       ├── structxv_1_1DateTime-members.html
│   │       ├── inherit_graph_41.md5
│   │       ├── inherit_graph_15.png
│   │       ├── inherit_graph_39.map
│   │       ├── inherit_graph_28.png
│   │       ├── inherit_graph_67.md5
│   │       ├── structxv_1_1GazeCalibrationData.html
│   │       ├── inherit_graph_70.png
│   │       ├── namespacemembers.html
│   │       ├── structxv_1_1Triple.html
│   │       ├── structxv_1_1SgbmImage-members.html
│   │       ├── structxv_1_1ThermalImage-members.html
│   │       ├── structxv_1_1PoseF-members.html
│   │       ├── structxv_1_1GrayScaleImage.html
│   │       ├── classxv_1_1Camera.html
│   │       ├── inherit_graph_29.map
│   │       ├── inherit_graph_62.png
│   │       ├── structxv_1_1PdmCameraCalibration-members.html
│   │       ├── index.html
│   │       ├── inherit_graph_8.png
│   │       ├── inherit_graph_39.png
│   │       ├── classxv_1_1Slam.html
│   │       ├── inherit_graph_51.md5
│   │       ├── inherit_graph_11.png
│   │       ├── classxv_1_1ThermalCamera.html
│   │       ├── inherit_graph_54.md5
│   │       ├── functions_enum.html
│   │       ├── inherit_graph_13.md5
│   │       ├── inherit_graph_29.png
│   │       ├── inherit_graph_36.map
│   │       ├── inherit_graph_65.md5
│   │       ├── inherit_graph_32.md5
│   │       ├── structxv_1_1ObjectDescriptor.html
│   │       ├── dynsections.js
│   │       ├── inherit_graph_66.md5
│   │       ├── inherit_graph_23.png
│   │       ├── structxv_1_1EyetrackingImage.html
│   │       ├── functions_u.html
│   │       ├── graph_legend.png
│   │       ├── tab_b.png
│   │       ├── classxv_1_1details_1_1PoseRot__.html
│   │       ├── inherit_graph_52.md5
│   │       ├── structxv_1_1Event.html
│   │       ├── classxv_1_1details_1_1TransformQuat__-members.html
│   │       ├── inherit_graph_6.map
│   │       ├── inherit_graph_44.map
│   │       ├── inherit_graph_18.md5
│   │       ├── functions_i.html
│   │       ├── structxv_1_1TransformQuatF-members.html
│   │       ├── dir_e68e8157741866f444e17edd764ebbae.html
│   │       ├── inherit_graph_21.png
│   │       ├── inherit_graph_55.png
│   │       ├── inherit_graph_43.map
│   │       ├── structxv_1_1GazeCalibrationData-members.html
│   │       ├── classxv_1_1DeviceStatusStream-members.html
│   │       ├── structxv_1_1XV__ET__INIT__PARAM.html
│   │       ├── classxv_1_1GazeStream-members.html
│   │       ├── inherit_graph_27.md5
│   │       ├── structxv_1_1ThermalImage.html
│   │       ├── structxv_1_1keypoint-members.html
│   │       ├── structxv_1_1XV__IRIS__DATA-members.html
│   │       ├── structxv_1_1FisheyeImages-members.html
│   │       ├── classxv_1_1FisheyeCameras-members.html
│   │       ├── inherit_graph_31.map
│   │       ├── structxv_1_1PointMatches.html
│   │       ├── inherit_graph_60.png
│   │       ├── inherit_graph_3.png
│   │       ├── graph_legend.html
│   │       ├── inherit_graph_46.md5
│   │       ├── classxv_1_1details_1_1Pose__-members.html
│   │       ├── inherit_graph_11.map
│   │       ├── structxv_1_1Calibration.html
│   │       ├── inherit_graph_38.md5
│   │       ├── inherit_graph_32.png
│   │       ├── inherit_graph_31.png
│   │       ├── inherit_graph_50.md5
│   │       ├── inherit_graph_37.map
│   │       ├── inherit_graph_47.md5
│   │       ├── classxv_1_1EventStream.html
│   │       ├── inherit_graph_42.md5
│   │       ├── inherit_graph_9.png
│   │       ├── classxv_1_1ImuSensor-members.html
│   │       ├── inherit_graph_10.png
│   │       ├── inherit_graph_62.md5
│   │       ├── inherit_graph_26.md5
│   │       ├── inherit_graph_34.map
│   │       ├── doxygen.css
│   │       ├── inherit_graph_59.map
│   │       ├── inherit_graph_5.md5
│   │       ├── inherit_graph_7.map
│   │       ├── structxv_1_1ColorImage.html
│   │       ├── structxv__ETCoefficient.html
│   │       ├── classxv_1_1MicStream.html
│   │       ├── graph_legend.md5
│   │       ├── inherit_graph_58.png
│   │       ├── inherit_graph_21.map
│   │       ├── nav_f.png
│   │       ├── inherit_graph_36.md5
│   │       ├── inherit_graph_38.png
│   │       ├── inherit_graph_20.png
│   │       ├── inherit_graph_56.png
│   │       ├── inherit_graph_46.map
│   │       ├── inherit_graph_69.md5
│   │       ├── classxv_1_1GestureStream-members.html
│   │       ├── structxv_1_1DepthImage.html
│   │       ├── structxv_1_1GPSDistanceData.html
│   │       ├── nav_h.png
│   │       ├── inherit_graph_45.md5
│   │       ├── inherit_graph_47.png
│   │       ├── inherit_graph_28.map
│   │       ├── inherit_graph_1.png
│   │       ├── inherit_graph_10.md5
│   │       ├── inherit_graph_30.map
│   │       ├── functions_w.html
│   │       ├── unionxv_1_1XV__ET__POINT__2D.html
│   │       ├── classxv_1_1OrientationStream-members.html
│   │       ├── functions_z.html
│   │       ├── inherit_graph_56.map
│   │       ├── structxv_1_1PoseF.html
│   │       ├── group__xv__functions.html
│   │       ├── inherit_graph_42.png
│   │       ├── classxv_1_1details_1_1PoseQuat__-members.html
│   │       ├── structxv_1_1CalibrationStatus-members.html
│   │       ├── structxv_1_1XV__ET__EYE__EXDATA.html
│   │       ├── structxv_1_1GestureData.html
│   │       ├── tab_s.png
│   │       ├── menu.js
│   │       ├── classxv_1_1GPSStream-members.html
│   │       ├── structxv_1_1Transform.html
│   │       ├── inherit_graph_71.map
│   │       ├── tabs.css
│   │       ├── structxv_1_1Pose.html
│   │       ├── structxv_1_1Event-members.html
│   │       ├── xv-types_8h_source.html
│   │       ├── files.html
│   │       ├── classxv_1_1ThermalCamera-members.html
│   │       ├── inherit_graph_71.png
│   │       ├── inherit_graph_52.map
│   │       ├── folderclosed.png
│   │       ├── inherit_graph_70.md5
│   │       ├── inherit_graph_50.png
│   │       ├── inherit_graph_57.md5
│   │       ├── classxv_1_1TofCamera.html
│   │       ├── structxv_1_1GestureData-members.html
│   │       ├── functions_c.html
│   │       ├── structxv_1_1XV__ET__GAZE__POINT-members.html
│   │       ├── unionxv__ETPoint2D.html
│   │       ├── sync_off.png
│   │       ├── structxv_1_1UnifiedCameraModel-members.html
│   │       ├── inherit_graph_26.map
│   │       ├── inherit_graph_55.md5
│   │       ├── inherit_graph_66.map
│   │       ├── structxv_1_1sgbm__config.html
│   │       ├── inherit_graph_2.md5
│   │       ├── inherit_graph_43.png
│   │       ├── classxv_1_1SgbmCamera-members.html
│   │       ├── inherit_graph_6.png
│   │       ├── functions_h.html
│   │       ├── structxv_1_1MicData.html
│   │       ├── structxv_1_1AfSetting.html
│   │       ├── splitbar.png
│   │       ├── functions_q.html
│   │       ├── classxv_1_1GazeStream.html
│   │       ├── nav_g.png
│   │       ├── functions_~.html
│   │       ├── inherit_graph_15.map
│   │       ├── classxv_1_1Orientation-members.html
│   │       ├── inherit_graph_4.map
│   │       ├── classxv_1_1Speaker-members.html
│   │       ├── functions_m.html
│   │       ├── inherit_graph_25.md5
│   │       ├── classxv_1_1GPSDistanceStream.html
│   │       ├── hierarchy.html
│   │       ├── inherit_graph_54.png
│   │       ├── inherit_graph_64.md5
│   │       ├── inherit_graph_65.png
│   │       ├── classxv_1_1Display.html
│   │       ├── inherit_graph_44.md5
│   │       ├── inherit_graph_45.map
│   │       ├── classxv_1_1details_1_1PoseQuat__.html
│   │       ├── inherit_graph_31.md5
│   │       ├── functions_r.html
│   │       ├── structxv_1_1DepthColorImage.html
│   │       ├── classxv_1_1IrisStream-members.html
│   │       ├── inherit_graph_53.md5
│   │       ├── inherit_graph_0.md5
│   │       ├── inherit_graph_35.png
│   │       ├── inherit_graph_32.map
│   │       ├── structxv_1_1CnnRawWrapper.html
│   │       ├── functions_d.html
│   │       ├── inherit_graph_61.map
│   │       ├── classxv_1_1OrientationStream.html
│   │       ├── classxv_1_1Device.html
│   │       ├── inherit_graph_60.md5
│   │       ├── functions_s.html
│   │       ├── functions_vars.html
│   │       ├── classxv_1_1GestureStream.html
│   │       ├── structxv_1_1Imu.html
│   │       ├── classxv_1_1Orientation.html
│   │       ├── inherit_graph_40.md5
│   │       ├── classxv_1_1EyetrackingCamera-members.html
│   │       ├── classxv_1_1details_1_1Transform__-members.html
│   │       ├── inherit_graph_69.map
│   │       ├── functions_e.html
│   │       ├── bdwn.png
│   │       ├── structxv_1_1Version-members.html
│   │       ├── inherit_graph_0.png
│   │       ├── structxv_1_1PdmCameraCalibration.html
│   │       ├── inherit_graph_24.map
│   │       ├── classxv_1_1Stream.html
│   │       ├── structxv_1_1IspAecSetting-members.html
│   │       ├── inherit_graph_48.md5
│   │       ├── structxv_1_1PolynomialDistortionCameraModel.html
│   │       ├── inherit_graph_16.md5
│   │       ├── inherit_graph_17.map
│   │       ├── inherit_graph_70.map
│   │       ├── folderopen.png
│   │       ├── inherit_graph_12.map
│   │       ├── inherit_graph_37.png
│   │       ├── classxv_1_1Stream-members.html
│   │       ├── inherit_graph_3.md5
│   │       ├── inherit_graph_21.md5
│   │       ├── inherit_graph_22.md5
│   │       ├── structxv_1_1IspAecSetting.html
│   │       ├── structxv_1_1XV__ET__PUPIL__INFO.html
│   │       ├── functions_o.html
│   │       ├── inherit_graph_71.md5
│   │       ├── inherit_graph_74.png
│   │       ├── inherit_graph_17.png
│   │       ├── classxv_1_1ObjectDetector-members.html
│   │       ├── structxv_1_1PointMatches-members.html
│   │       ├── inherit_graph_40.png
│   │       ├── structxv_1_1ObjectDescriptor-members.html
│   │       ├── inherit_graph_19.png
│   │       ├── classxv_1_1Device-members.html
│   │       ├── structxv_1_1DeviceSetting-members.html
│   │       ├── inherit_graph_57.map
│   │       ├── structxv_1_1RgbImage-members.html
│   │       ├── unionxv__ETPoint3D.html
│   │       ├── classxv_1_1EyetrackingCamera.html
│   │       ├── structxv_1_1XV__ET__EYE__DATA__EX-members.html
│   │       ├── inherit_graph_51.map
│   │       ├── inherit_graph_48.map
│   │       ├── structxv_1_1Imu-members.html
│   │       ├── annotated.html
│   │       ├── inherit_graph_6.md5
│   │       ├── functions.html
│   │       ├── structxv_1_1XV__IRIS__DATA.html
│   │       ├── inherit_graph_36.png
│   │       ├── functions_y.html
│   │       ├── inherit_graph_0.map
│   │       ├── structxv_1_1SlamMap.html
│   │       ├── classxv_1_1GPSDistanceStream-members.html
│   │       ├── inherit_graph_13.png
│   │       ├── inherit_graph_58.md5
│   │       ├── jquery.js
│   │       ├── inherit_graph_5.map
│   │       ├── inherit_graph_73.map
│   │       ├── functions_t.html
│   │       ├── inherit_graph_24.md5
│   │       ├── structxv_1_1Pose-members.html
│   │       ├── structxv_1_1Object_1_1keypoint-members.html
│   │       ├── inherit_graph_7.png
│   │       ├── inherit_graph_17.md5
│   │       ├── inherit_graph_72.png
│   │       ├── inherit_graph_11.md5
│   │       ├── inherit_graph_13.map
│   │       ├── namespacemembers_func.html
│   │       ├── structxv_1_1XV__ET__EYE__EXDATA-members.html
│   │       ├── inherit_graph_44.png
│   │       ├── inherit_graph_73.png
│   │       ├── structxv_1_1Version.html
│   │       ├── functions_n.html
│   │       ├── inherit_graph_67.png
│   │       ├── inherits.html
│   │       ├── inherit_graph_27.map
│   │       ├── inherit_graph_75.map
│   │       ├── inherit_graph_73.md5
│   │       ├── classxv_1_1Display-members.html
│   │       ├── group__packed__struct.html
│   │       ├── structxv_1_1UcmCameraCalibration-members.html
│   │       ├── inherit_graph_64.map
│   │       ├── inherit_graph_50.map
│   │       ├── inherit_graph_33.md5
│   │       ├── inherit_graph_20.map
│   │       ├── classxv_1_1FisheyeCameras.html
│   │       ├── open.png
│   │       ├── inherit_graph_45.png
│   │       ├── inherit_graph_49.map
│   │       ├── structxv_1_1PointCloud.html
│   │       ├── inherit_graph_66.png
│   │       ├── structxv_1_1XV__ET__PUPIL__INFO-members.html
│   │       ├── inherit_graph_53.map
│   │       ├── structxv_1_1SlamMap-members.html
│   │       ├── classxv_1_1Slam-members.html
│   │       ├── inherit_graph_59.md5
│   │       ├── inherit_graph_56.md5
│   │       ├── inherit_graph_14.md5
│   │       ├── inherit_graph_34.md5
│   │       ├── inherit_graph_63.md5
│   │       ├── inherit_graph_35.map
│   │       ├── structxv__ETInitParam.html
│   │       ├── inherit_graph_24.png
│   │       ├── structxv_1_1Calibration-members.html
│   │       ├── structxv_1_1Object-members.html
│   │       ├── inherit_graph_62.map
│   │       ├── classxv_1_1DeviceStatusStream.html
│   │       ├── classxv_1_1EventStream-members.html
│   │       ├── inherit_graph_74.map
│   │       ├── functions_l.html
│   │       ├── classxv_1_1ObjectDetector.html
│   │       ├── bc_s.png
│   │       ├── structxv_1_1TransformQuatF.html
│   │       ├── functions_g.html
│   │       ├── structxv_1_1CalibrationStatus.html
│   │       ├── namespacexv_1_1details.html
│   │       ├── inherit_graph_1.md5
│   │       ├── closed.png
│   │       ├── inherit_graph_53.png
│   │       ├── inherit_graph_10.map
│   │       ├── tab_a.png
│   │       ├── inherit_graph_16.map
│   │       ├── classes.html
│   │       ├── inherit_graph_68.md5
│   │       ├── inherit_graph_18.png
│   │       ├── inherit_graph_19.map
│   │       ├── structxv_1_1Transform-members.html
│   │       ├── inherit_graph_40.map
│   │       ├── structxv_1_1ColorImage-members.html
│   │       ├── inherit_graph_61.png
│   │       ├── inherit_graph_57.png
│   │       ├── inherit_graph_41.png
│   │       ├── group__xv__android__functions.html
│   │       ├── xv-sdk_8h_source.html
│   │       ├── menudata.js
│   │       ├── structxv_1_1SgbmImage.html
│   │       ├── inherit_graph_25.map
│   │       ├── structxv_1_1XV__ET__GAZE__POINT.html
│   │       ├── classxv_1_1CameraModel.html
│   │       ├── structxv_1_1XV__ET__COEFFICIENT.html
│   │       ├── inherit_graph_22.png
│   │       ├── doxygen.png
│   │       ├── dir_d44c64559bbebec7f509842c48db8b23.html
│   │       ├── classxv_1_1ColorCamera.html
│   │       ├── classxv_1_1Speaker.html
│   │       ├── classxv_1_1TofCamera-members.html
│   │       ├── inherit_graph_30.png
│   │       ├── inherit_graph_33.png
│   │       ├── inherit_graph_9.md5
│   │       ├── inherit_graph_4.md5
│   │       ├── structxv_1_1DateTime.html
│   │       ├── structxv_1_1TransformF.html
│   │       ├── classxv_1_1details_1_1Transform__.html
│   │       ├── structxv_1_1sgbm__config-members.html
│   │       ├── inherit_graph_9.map
│   │       ├── inherit_graph_75.png
│   │       ├── inherit_graph_16.png
│   │       ├── inherit_graph_2.png
│   │       ├── search
│   │       │   ├── classes_7.js
│   │       │   ├── functions_14.js
│   │       │   ├── classes_7.html
│   │       │   ├── functions_1.html
│   │       │   ├── classes_c.html
│   │       │   ├── variables_11.html
│   │       │   ├── all_4.html
│   │       │   ├── all_1.html
│   │       │   ├── search_m.png
│   │       │   ├── groups_3.js
│   │       │   ├── variables_f.html
│   │       │   ├── functions_1.js
│   │       │   ├── functions_e.js
│   │       │   ├── all_17.html
│   │       │   ├── classes_a.html
│   │       │   ├── functions_8.js
│   │       │   ├── groups_2.js
│   │       │   ├── all_d.js
│   │       │   ├── functions_2.html
│   │       │   ├── variables_3.js
│   │       │   ├── variables_2.html
│   │       │   ├── functions_10.html
│   │       │   ├── classes_3.html
│   │       │   ├── all_d.html
│   │       │   ├── all_18.html
│   │       │   ├── variables_12.js
│   │       │   ├── functions_3.html
│   │       │   ├── variables_b.html
│   │       │   ├── groups_0.html
│   │       │   ├── classes_0.js
│   │       │   ├── variables_13.html
│   │       │   ├── variables_5.html
│   │       │   ├── classes_d.html
│   │       │   ├── all_a.js
│   │       │   ├── variables_11.js
│   │       │   ├── classes_9.js
│   │       │   ├── classes_4.html
│   │       │   ├── variables_14.html
│   │       │   ├── all_c.html
│   │       │   ├── functions_6.js
│   │       │   ├── variables_10.html
│   │       │   ├── all_14.js
│   │       │   ├── enumvalues_1.js
│   │       │   ├── functions_16.html
│   │       │   ├── classes_6.js
│   │       │   ├── enums_0.js
│   │       │   ├── all_e.js
│   │       │   ├── all_4.js
│   │       │   ├── functions_11.html
│   │       │   ├── functions_5.js
│   │       │   ├── all_16.html
│   │       │   ├── functions_3.js
│   │       │   ├── variables_0.js
│   │       │   ├── functions_15.html
│   │       │   ├── variables_9.js
│   │       │   ├── variables_9.html
│   │       │   ├── all_6.html
│   │       │   ├── classes_8.html
│   │       │   ├── variables_2.js
│   │       │   ├── all_f.js
│   │       │   ├── functions_7.js
│   │       │   ├── variables_e.html
│   │       │   ├── variables_7.html
│   │       │   ├── all_15.js
│   │       │   ├── classes_10.html
│   │       │   ├── classes_1.html
│   │       │   ├── namespaces_0.js
│   │       │   ├── functions_9.html
│   │       │   ├── classes_0.html
│   │       │   ├── all_2.html
│   │       │   ├── variables_7.js
│   │       │   ├── variables_d.js
│   │       │   ├── classes_8.js
│   │       │   ├── all_b.js
│   │       │   ├── classes_2.js
│   │       │   ├── classes_9.html
│   │       │   ├── classes_4.js
│   │       │   ├── namespaces_0.html
│   │       │   ├── functions_d.js
│   │       │   ├── all_18.js
│   │       │   ├── search_r.png
│   │       │   ├── enumvalues_0.html
│   │       │   ├── variables_5.js
│   │       │   ├── functions_c.html
│   │       │   ├── functions_f.js
│   │       │   ├── classes_5.js
│   │       │   ├── classes_b.html
│   │       │   ├── functions_5.html
│   │       │   ├── functions_4.html
│   │       │   ├── groups_2.html
│   │       │   ├── all_9.js
│   │       │   ├── search.css
│   │       │   ├── functions_9.js
│   │       │   ├── all_8.js
│   │       │   ├── classes_a.js
│   │       │   ├── enums_0.html
│   │       │   ├── all_b.html
│   │       │   ├── classes_b.js
│   │       │   ├── functions_14.html
│   │       │   ├── groups_1.html
│   │       │   ├── all_2.js
│   │       │   ├── variables_c.js
│   │       │   ├── variables_a.html
│   │       │   ├── variables_6.js
│   │       │   ├── classes_2.html
│   │       │   ├── variables_f.js
│   │       │   ├── functions_d.html
│   │       │   ├── classes_e.html
│   │       │   ├── variables_8.html
│   │       │   ├── variables_3.html
│   │       │   ├── all_11.html
│   │       │   ├── all_8.html
│   │       │   ├── all_a.html
│   │       │   ├── functions_e.html
│   │       │   ├── variables_13.js
│   │       │   ├── functions_6.html
│   │       │   ├── functions_16.js
│   │       │   ├── functions_11.js
│   │       │   ├── all_13.html
│   │       │   ├── all_5.html
│   │       │   ├── variables_c.html
│   │       │   ├── all_10.js
│   │       │   ├── variables_a.js
│   │       │   ├── all_12.js
│   │       │   ├── all_6.js
│   │       │   ├── all_5.js
│   │       │   ├── classes_6.html
│   │       │   ├── functions_a.js
│   │       │   ├── variables_4.js
│   │       │   ├── all_16.js
│   │       │   ├── all_10.html
│   │       │   ├── variables_10.js
│   │       │   ├── all_3.html
│   │       │   ├── variables_e.js
│   │       │   ├── variables_d.html
│   │       │   ├── classes_5.html
│   │       │   ├── functions_12.js
│   │       │   ├── variables_6.html
│   │       │   ├── variables_8.js
│   │       │   ├── classes_e.js
│   │       │   ├── variables_b.js
│   │       │   ├── all_3.js
│   │       │   ├── classes_c.js
│   │       │   ├── functions_a.html
│   │       │   ├── all_0.html
│   │       │   ├── all_17.js
│   │       │   ├── variables_14.js
│   │       │   ├── all_15.html
│   │       │   ├── functions_15.js
│   │       │   ├── classes_f.js
│   │       │   ├── mag_sel.png
│   │       │   ├── all_e.html
│   │       │   ├── groups_1.js
│   │       │   ├── close.png
│   │       │   ├── functions_c.js
│   │       │   ├── classes_d.js
│   │       │   ├── functions_2.js
│   │       │   ├── variables_0.html
│   │       │   ├── search.js
│   │       │   ├── classes_10.js
│   │       │   ├── all_7.js
│   │       │   ├── functions_12.html
│   │       │   ├── groups_3.html
│   │       │   ├── variables_4.html
│   │       │   ├── nomatches.html
│   │       │   ├── all_14.html
│   │       │   ├── variables_1.js
│   │       │   ├── all_7.html
│   │       │   ├── functions_4.js
│   │       │   ├── all_f.html
│   │       │   ├── all_19.js
│   │       │   ├── groups_0.js
│   │       │   ├── functions_0.html
│   │       │   ├── functions_0.js
│   │       │   ├── all_1.js
│   │       │   ├── searchdata.js
│   │       │   ├── functions_10.js
│   │       │   ├── functions_7.html
│   │       │   ├── classes_f.html
│   │       │   ├── enumvalues_0.js
│   │       │   ├── functions_b.js
│   │       │   ├── variables_1.html
│   │       │   ├── all_11.js
│   │       │   ├── enumvalues_1.html
│   │       │   ├── functions_f.html
│   │       │   ├── all_12.html
│   │       │   ├── functions_b.html
│   │       │   ├── functions_13.html
│   │       │   ├── all_0.js
│   │       │   ├── functions_8.html
│   │       │   ├── all_c.js
│   │       │   ├── classes_1.js
│   │       │   ├── all_19.html
│   │       │   ├── all_9.html
│   │       │   ├── classes_3.js
│   │       │   ├── functions_13.js
│   │       │   ├── variables_12.html
│   │       │   ├── search_l.png
│   │       │   └── all_13.js
│   │       ├── inherit_graph_2.map
│   │       ├── inherit_graph_23.md5
│   │       ├── functions_p.html
│   │       ├── structxv_1_1GPSDistanceData-members.html
│   │       ├── inherit_graph_58.map
│   │       ├── structxv_1_1TransformF-members.html
│   │       ├── inherit_graph_47.map
│   │       ├── structxv_1_1keypoint.html
│   │       ├── unionxv_1_1XV__ET__POINT__3D.html
│   │       ├── inherit_graph_49.png
│   │       ├── inherit_graph_34.png
│   │       ├── structxv_1_1AfSetting-members.html
│   │       ├── structxv_1_1EyetrackingImage-members.html
│   │       ├── structxv_1_1UnifiedCameraModel.html
│   │       ├── classxv_1_1details_1_1PoseRot__-members.html
│   │       ├── inherit_graph_54.map
│   │       ├── namespaces.html
│   │       ├── inherit_graph_7.md5
│   │       ├── inherit_graph_46.png
│   │       ├── structxv_1_1DepthColorImage-members.html
│   │       ├── structxv_1_1PointCloud-members.html
│   │       ├── inherit_graph_3.map
│   │       ├── inherit_graph_15.md5
│   │       ├── inherit_graph_72.md5
│   │       ├── classxv_1_1details_1_1PosePred__-members.html
│   │       ├── inherit_graph_68.map
│   │       ├── structxv_1_1UcmCameraCalibration.html
│   │       ├── inherit_graph_33.map
│   │       ├── inherit_graph_72.map
│   │       ├── inherit_graph_8.md5
│   │       ├── inherit_graph_22.map
│   │       ├── structxv_1_1Object.html
│   │       ├── inherit_graph_75.md5
│   │       ├── structxv_1_1AwbSetting.html
│   │       ├── inherit_graph_23.map
│   │       ├── structxv_1_1XV__ET__EYE__DATA__EX.html
│   │       ├── inherit_graph_38.map
│   │       ├── inherit_graph_51.png
│   │       ├── functions_f.html
│   │       ├── structxv_1_1TransformQuat-members.html
│   │       ├── structxv_1_1FisheyeImages.html
│   │       ├── functions_b.html
│   │       ├── inherit_graph_12.md5
│   │       ├── inherit_graph_68.png
│   │       ├── structxv_1_1Plane.html
│   │       ├── inherit_graph_59.png
│   │       ├── classxv_1_1SgbmCamera.html
│   │       ├── classxv_1_1details_1_1TransformQuat__.html
│   │       ├── structxv_1_1XV__ET__COEFFICIENT-members.html
│   │       ├── inherit_graph_43.md5
│   │       ├── sync_on.png
│   │       ├── inherit_graph_49.md5
│   │       ├── classxv_1_1ImuSensor.html
│   │       ├── inherit_graph_52.png
│   │       ├── inherit_graph_61.md5
│   │       ├── inherit_graph_30.md5
│   │       ├── inherit_graph_5.png
│   │       ├── inherit_graph_20.md5
│   │       ├── functions_x.html
│   │       ├── classxv_1_1details_1_1Pose__.html
│   │       ├── inherit_graph_64.png
│   │       ├── inherit_graph_42.map
│   │       ├── group__Version.html
│   │       ├── structxv_1_1GrayScaleImage-members.html
│   │       ├── inherit_graph_39.md5
│   │       ├── inherit_graph_14.map
│   │       ├── inherit_graph_69.png
│   │       ├── classxv_1_1CameraModel-members.html
│   │       ├── structxv_1_1Plane-members.html
│   │       ├── structxv_1_1CnnRawWrapper-members.html
│   │       ├── inherit_graph_74.md5
│   │       ├── group__xv__rotation__conversions.html
│   │       ├── structxv_1_1PolynomialDistortionCameraModel-members.html
│   │       ├── classxv_1_1Camera-members.html
│   │       └── classxv_1_1MicStream-members.html
│   ├── python-wrapper
│   │   ├── PythonDemo.py
│   │   └── xvsdk.py
│   ├── ros-wrapper
│   │   └── xv_sdk
│   │       ├── src
│   │       │   ├── xv_sdk_node.cpp
│   │       │   └── xv_sdk_wrapper.cpp
│   │       ├── srv
│   │       │   ├── GetDevices.srv
│   │       │   ├── SaveMapAndSwitchCslam.srv
│   │       │   ├── LoadMapAndSwithcCslam.srv
│   │       │   ├── GetPose.srv
│   │       │   ├── GetPoseAt.srv
│   │       │   ├── GetOrientation.srv
│   │       │   └── GetOrientationAt.srv
│   │       ├── msg
│   │       │   ├── PoseStampedConfidence.msg
│   │       │   ├── FisheyeImages.msg
│   │       │   ├── Lost.msg
│   │       │   ├── Plane.msg
│   │       │   ├── OrientationStamped.msg
│   │       │   └── Planes.msg
│   │       ├── config
│   │       │   └── default.yaml
│   │       ├── test
│   │       │   └── test_xv_sdk.cpp
│   │       ├── launch
│   │       │   └── xv_sdk.launch
│   │       ├── CMakeLists.txt
│   │       ├── package.xml
│   │       ├── include
│   │       │   └── xv_sdk
│   │       │       ├── xv_sdk.hpp
│   │       │       ├── xv-sdk-ex.h
│   │       │       └── xv-sdk-private.h
│   │       ├── README.md
│   │       ├── LICENSE
│   │       └── rviz
│   │           ├── sgbmdemo.rviz
│   │           ├── demo-slam.rviz
│   │           ├── tofdemo.rviz
│   │           └── demo.rviz
│   ├── ros2-wrapper
│   │   ├── xv_ros2_msgs
│   │   │   ├── srv
│   │   │   │   ├── GetDevices.srv
│   │   │   │   ├── SaveMapAndSwitchCslam.srv
│   │   │   │   ├── LoadMapAndSwithcCslam.srv
│   │   │   │   ├── GetPose.srv
│   │   │   │   ├── GetPoseAt.srv
│   │   │   │   ├── GetOrientation.srv
│   │   │   │   └── GetOrientationAt.srv
│   │   │   ├── msg
│   │   │   │   └── OrientationStamped.msg
│   │   │   ├── CMakeLists.txt
│   │   │   └── package.xml
│   │   ├── README.md
│   │   └── xv_sdk_ros2
│   │       ├── src
│   │       │   ├── xv_ros2_node.cpp
│   │       │   ├── xv_dev_wrapper.cpp
│   │       │   └── main.cpp
│   │       ├── launch
│   │       │   └── xv_sdk_node_launch.py
│   │       ├── CMakeLists.txt
│   │       ├── package.xml
│   │       └── include
│   │           ├── xv_ros2_node.h
│   │           └── xv_dev_wrapper.h
│   └── xvsdk
│       ├── all_stream
│       │   ├── raw2opencv.cpp
│       │   ├── colors.h
│       │   ├── CMakeLists.txt
│       │   ├── fps_count.hpp
│       │   └── all_stream.cpp
│       ├── demo-api
│       │   ├── pipe_srv.h
│       │   ├── demo-api.cpp
│       │   ├── CMakeLists.txt
│       │   └── fps_count.hpp
│       ├── CMakeLists.txt
│       └── pipe_srv
│           ├── pipe_srv.h
│           └── pipe_srv.cpp
├── include2
│   └── xv-sdk-ex.h
├── lib
│   ├── libxslam-usb-sdk.a
│   ├── libxslam-slam_core.so
│   ├── libxslam-vsc-sdk.so.4.0
│   ├── x86_64-linux-gnu
│   │   ├── libxvuvc.so.0.0.6
│   │   ├── libxvuvc.so
│   │   ├── libxvuvc.so.0
│   │   └── libxvuvc.a
│   ├── libxslam-edge-sdk.so
│   ├── libsony_iu456.so.1.0.0
│   ├── pkgconfig
│   │   └── xvsdk.pc
│   ├── libxslam-edge-sdk.so.2.0.0
│   ├── libxslam-slam_algo-sdk.so
│   ├── libxslam-hid-sdk.so
│   ├── libxslam-slam_lib-ange.so
│   ├── libxslam-uvc-sdk.so.2.0
│   ├── libxslam-usb-sdk.so.1.6
│   ├── libxslam-vsc-sdk.so.4.0.0
│   ├── libxslam-usb-sdk.so
│   ├── libapriltag.so
│   ├── libxslam-vsc-sdk.so
│   ├── libxslam-hid-sdk.a
│   ├── libxslam-hid-sdk.so.1.7.0
│   ├── libxvsdk.so
│   ├── libsony_iu456.so
│   ├── libxvisio-CInterface-wrapper.so
│   ├── libxslam-vsc-sdk.a
│   ├── libxslam-uvc-sdk.a
│   ├── libxslam-edge-sdk.so.2.0
│   ├── libapriltag.so.3
│   ├── libxslam-edge-sdk.a
│   ├── libapriltag.so.3.1.0
│   ├── libxvsdk.so.3.2.0
│   ├── cmake
│   │   └── xvsdk
│   │       ├── xvsdkConfigVersion.cmake
│   │       └── xvsdkConfig.cmake
│   ├── libxvsdk.so.3.2
│   ├── libsony_iu456.so.1.0
│   ├── libxslam-hid-sdk.so.1.7
│   ├── libxslam-uvc-sdk.so
│   ├── libxslam-uvc-sdk.so.2.0.0
│   └── libxslam-usb-sdk.so.1.6.0
└── bin
    ├── all_stream
    ├── demo-api
    └── pipe_srv

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/src/all_stream/colors.h`:

```h
#ifndef COLORS_H
#define COLORS_H

#include <vector>

static std::vector<std::vector<unsigned char>> colors = {{0,0,0},
{255,4,0},
{255,8,0},
{255,12,0},
{255,17,0},
{255,21,0},
{255,25,0},
{255,29,0},
{255,34,0},
{255,38,0},
{255,42,0},
{255,46,0},
{255,51,0},
{255,55,0},
{255,59,0},
{255,64,0},
{255,68,0},
{255,72,0},
{255,76,0},
{255,81,0},
{255,85,0},
{255,89,0},
{255,93,0},
{255,98,0},
{255,102,0},
{255,106,0},
{255,110,0},
{255,115,0},
{255,119,0},
{255,123,0},
{255,128,0},
{255,132,0},
{255,136,0},
{255,140,0},
{255,145,0},
{255,149,0},
{255,153,0},
{255,157,0},
{255,162,0},
{255,166,0},
{255,170,0},
{255,174,0},
{255,179,0},
{255,183,0},
{255,187,0},
{255,191,0},
{255,196,0},
{255,200,0},
{255,204,0},
{255,209,0},
{255,213,0},
{255,217,0},
{255,221,0},
{255,226,0},
{255,230,0},
{255,234,0},
{255,238,0},
{255,243,0},
{255,247,0},
{255,251,0},
{255,255,0},
{251,255,0},
{247,255,0},
{243,255,0},
{238,255,0},
{234,255,0},
{230,255,0},
{226,255,0},
{221,255,0},
{217,255,0},
{213,255,0},
{209,255,0},
{204,255,0},
{200,255,0},
{196,255,0},
{191,255,0},
{187,255,0},
{183,255,0},
{179,255,0},
{174,255,0},
{170,255,0},
{166,255,0},
{162,255,0},
{157,255,0},
{153,255,0},
{149,255,0},
{145,255,0},
{140,255,0},
{136,255,0},
{132,255,0},
{128,255,0},
{123,255,0},
{119,255,0},
{115,255,0},
{110,255,0},
{106,255,0},
{102,255,0},
{98,255,0},
{93,255,0},
{89,255,0},
{85,255,0},
{81,255,0},
{76,255,0},
{72,255,0},
{68,255,0},
{64,255,0},
{59,255,0},
{55,255,0},
{51,255,0},
{46,255,0},
{42,255,0},
{38,255,0},
{34,255,0},
{29,255,0},
{25,255,0},
{21,255,0},
{17,255,0},
{12,255,0},
{8,255,0},
{4,255,0},
{0,255,0},
{0,255,4},
{0,255,8},
{0,255,12},
{0,255,17},
{0,255,21},
{0,255,25},
{0,255,29},
{0,255,34},
{0,255,38},
{0,255,42},
{0,255,46},
{0,255,51},
{0,255,55},
{0,255,59},
{0,255,64},
{0,255,68},
{0,255,72},
{0,255,76},
{0,255,81},
{0,255,85},
{0,255,89},
{0,255,93},
{0,255,98},
{0,255,102},
{0,255,106},
{0,255,110},
{0,255,115},
{0,255,119},
{0,255,123},
{0,255,128},
{0,255,132},
{0,255,136},
{0,255,140},
{0,255,145},
{0,255,149},
{0,255,153},
{0,255,157},
{0,255,162},
{0,255,166},
{0,255,170},
{0,255,174},
{0,255,179},
{0,255,183},
{0,255,187},
{0,255,191},
{0,255,196},
{0,255,200},
{0,255,204},
{0,255,209},
{0,255,213},
{0,255,217},
{0,255,221},
{0,255,226},
{0,255,230},
{0,255,234},
{0,255,238},
{0,255,243},
{0,255,247},
{0,255,251},
{0,255,255},
{0,251,255},
{0,247,255},
{0,243,255},
{0,238,255},
{0,234,255},
{0,230,255},
{0,226,255},
{0,221,255},
{0,217,255},
{0,213,255},
{0,209,255},
{0,204,255},
{0,200,255},
{0,196,255},
{0,191,255},
{0,187,255},
{0,183,255},
{0,179,255},
{0,174,255},
{0,170,255},
{0,166,255},
{0,162,255},
{0,157,255},
{0,153,255},
{0,149,255},
{0,145,255},
{0,140,255},
{0,136,255},
{0,132,255},
{0,128,255},
{0,123,255},
{0,119,255},
{0,115,255},
{0,110,255},
{0,106,255},
{0,102,255},
{0,98,255},
{0,93,255},
{0,89,255},
{0,85,255},
{0,81,255},
{0,76,255},
{0,72,255},
{0,68,255},
{0,64,255},
{0,59,255},
{0,55,255},
{0,51,255},
{0,46,255},
{0,42,255},
{0,38,255},
{0,34,255},
{0,29,255},
{0,25,255},
{0,21,255},
{0,17,255},
{0,12,255},
{0,8,255},
{0,4,255},
{0,0,255},
{4,0,255},
{8,0,255},
{12,0,255},
{17,0,255},
{21,0,255},
{25,0,255},
{29,0,255},
{34,0,255},
{38,0,255},
{42,0,255},
{46,0,255},
{51,0,255},
{55,0,255},
{59,0,255},
{64,0,255}};

#endif // COLORS_H

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/src/demo-api/pipe_srv.h`:

```h
#ifndef __VSC_CLIENT_H_
#define __VSC_CLIENT_H_

#ifdef _WIN32

#else
#include <stdio.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <signal.h>


#define VSC_SRV_RECV_PIPE	"./vscSrvRfifo"   // client write,
#define VSC_SRV_SEND_PIPE	"./vscSrvWfifo"   // client read

#define SRV_CMD_FLAG1	('C')
#define SRV_CMD_FLAG2	('M')
#define SRV_CMD_FLAG3	('D')

#define IS_PIPE_SRV_CMD(cmd1, cmd2, cmd3) \
	((cmd1==SRV_CMD_FLAG1) && (cmd2==SRV_CMD_FLAG2) && (cmd3==SRV_CMD_FLAG3))

static int vsc_pipe_init(void)
{
	int ret;

	if (access(VSC_SRV_RECV_PIPE, F_OK) != 0) {
		ret = mkfifo(VSC_SRV_RECV_PIPE, 0666);
		if (0 != ret) {
			printf("mkfifo server recv pipe fail, ret: %d\n", ret);
			return -1;
		}
		printf("create server recv pipe\n");
	} else {
		printf("pipe server recv has already exist\n");
	}

	if (access(VSC_SRV_SEND_PIPE, F_OK) != 0) {
		ret = mkfifo(VSC_SRV_SEND_PIPE, 0666);
		if (0 != ret) {
			printf("mkfifo server send pipe fail, ret: %d\n", ret);
			return -1;
		}
		printf("create server send pipe\n");
	} else {
		printf("pipe server send has already exist\n");
	}

	return 0;
}

/*--------------  server API -------------------------------------- */
static int vsc_server_get_handle(int *sendfd, int *recvfd)
{
	int ret = 0;

	ret = open(VSC_SRV_SEND_PIPE, O_WRONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for server fail, %d\n", ret);
		return ret;
	}
	*sendfd = ret;
	printf("pipe server get send handle: %d\n", *sendfd);

	ret = open(VSC_SRV_RECV_PIPE, O_RDONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for server fail, %d\n", ret);
		return ret;
	}
	*recvfd = ret;
	printf("pipe server get recv handle: %d\n", *recvfd);

	return 0;
}

static void vsc_srv_release_handle(int *sendfd, int *recvfd)
{
	if (*recvfd > 0)
		close(*recvfd);

	if (*sendfd > 0)
		close(*sendfd);
	return;
}
/*------------------------------------------------------------------------- */
static int vsc_cmd_recv_fd = -1;
static int vsc_cmd_send_fd = -1;
static int vsc_pipe_srv_pid = -1;

static int vsc_client_pipe_init(void)
{
	int ret = 0;

	ret = vsc_pipe_init();
	if (ret != 0) return ret;

	ret = open(VSC_SRV_SEND_PIPE, O_RDONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for client recv fail, %d\n", ret);
		return ret;
	}
	vsc_cmd_recv_fd = ret;
	printf("client pipe get recv handle: %d\n", vsc_cmd_recv_fd);

	ret = open(VSC_SRV_RECV_PIPE, O_WRONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for client send fail, %d\n", ret);
		return ret;
	}
	vsc_cmd_send_fd = ret;
	printf("client pipe get send handle:%d\n", vsc_cmd_send_fd);

	printf("command pipe init for client successful\n");
	return 0;
}

static void vsc_client_pipe_deinit(void)
{
	if (vsc_cmd_send_fd > 0)
		close(vsc_cmd_send_fd);

	if (vsc_cmd_recv_fd > 0)
		close(vsc_cmd_recv_fd);

	return;
}
static int vsc_client_pipe_get_srv_pid()
{
	FILE *fp = popen("ps -e | grep \'pipe_srv\' | awk \'{print $1}\'", "r");
	char buffer[10] = {0};

    if (fp == NULL) {
		printf("Cannot find pipe service process\n");
        return -1;
    }

	while (NULL != fgets(buffer, 10, fp)) {
		vsc_pipe_srv_pid = atoi(buffer);
		printf("vsc pipe server pid: %d\n", vsc_pipe_srv_pid);
	}
	pclose(fp);
	return 0;
}

static int vsc_client_pipe_request_cmd(const char *tip_info, int size)
{
	int cmdbuf[16] = {0};
	int retval;

	retval = write(vsc_cmd_send_fd, tip_info, size);
	retval = read(vsc_cmd_recv_fd, cmdbuf, sizeof(cmdbuf));
	if (IS_PIPE_SRV_CMD(cmdbuf[0], cmdbuf[1], cmdbuf[2])) {
		printf("recv cmd %d\n", cmdbuf[3]);
		return cmdbuf[3];
	}
	return -1;
}

static int vsc_client_pipe_terminal_srv(void)
{
	if (vsc_pipe_srv_pid > 0)
		kill(vsc_pipe_srv_pid, SIGTERM);
	return 0;
}
#endif

#endif //__VSC_CLIENT_H_

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/src/pipe_srv/pipe_srv.h`:

```h
#ifndef __VSC_CLIENT_H_
#define __VSC_CLIENT_H_

#ifdef _WIN32

#else
#include <stdio.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <signal.h>


#define VSC_SRV_RECV_PIPE	"./vscSrvRfifo"   // client write,
#define VSC_SRV_SEND_PIPE	"./vscSrvWfifo"   // client read

#define SRV_CMD_FLAG1	('C')
#define SRV_CMD_FLAG2	('M')
#define SRV_CMD_FLAG3	('D')

#define IS_PIPE_SRV_CMD(cmd1, cmd2, cmd3) \
	((cmd1==SRV_CMD_FLAG1) && (cmd2==SRV_CMD_FLAG2) && (cmd3==SRV_CMD_FLAG3))

static int vsc_pipe_init(void)
{
	int ret;

	if (access(VSC_SRV_RECV_PIPE, F_OK) != 0) {
		ret = mkfifo(VSC_SRV_RECV_PIPE, 0666);
		if (0 != ret) {
			printf("mkfifo server recv pipe fail, ret: %d\n", ret);
			return -1;
		}
		printf("create server recv pipe\n");
	} else {
		printf("pipe server recv has already exist\n");
	}

	if (access(VSC_SRV_SEND_PIPE, F_OK) != 0) {
		ret = mkfifo(VSC_SRV_SEND_PIPE, 0666);
		if (0 != ret) {
			printf("mkfifo server send pipe fail, ret: %d\n", ret);
			return -1;
		}
		printf("create server send pipe\n");
	} else {
		printf("pipe server send has already exist\n");
	}

	return 0;
}

/*--------------  server API -------------------------------------- */
static int vsc_server_get_handle(int *sendfd, int *recvfd)
{
	int ret = 0;

	ret = open(VSC_SRV_SEND_PIPE, O_WRONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for server fail, %d\n", ret);
		return ret;
	}
	*sendfd = ret;
	printf("pipe server get send handle: %d\n", *sendfd);

	ret = open(VSC_SRV_RECV_PIPE, O_RDONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for server fail, %d\n", ret);
		return ret;
	}
	*recvfd = ret;
	printf("pipe server get recv handle: %d\n", *recvfd);

	return 0;
}

static void vsc_srv_release_handle(int *sendfd, int *recvfd)
{
	if (*recvfd > 0)
		close(*recvfd);

	if (*sendfd > 0)
		close(*sendfd);
	return;
}
/*------------------------------------------------------------------------- */
static int vsc_cmd_recv_fd = -1;
static int vsc_cmd_send_fd = -1;
static int vsc_pipe_srv_pid = -1;

static int vsc_client_pipe_init(void)
{
	int ret = 0;

	ret = vsc_pipe_init();
	if (ret != 0) return ret;

	ret = open(VSC_SRV_SEND_PIPE, O_RDONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for client recv fail, %d\n", ret);
		return ret;
	}
	vsc_cmd_recv_fd = ret;
	printf("client pipe get recv handle: %d\n", vsc_cmd_recv_fd);

	ret = open(VSC_SRV_RECV_PIPE, O_WRONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for client send fail, %d\n", ret);
		return ret;
	}
	vsc_cmd_send_fd = ret;
	printf("client pipe get send handle:%d\n", vsc_cmd_send_fd);

	printf("command pipe init for client successful\n");
	return 0;
}

static void vsc_client_pipe_deinit(void)
{
	if (vsc_cmd_send_fd > 0)
		close(vsc_cmd_send_fd);

	if (vsc_cmd_recv_fd > 0)
		close(vsc_cmd_recv_fd);

	return;
}
static int vsc_client_pipe_get_srv_pid()
{
	FILE *fp = popen("ps -e | grep \'pipe_srv\' | awk \'{print $1}\'", "r");
	char buffer[10] = {0};

    if (fp == NULL) {
		printf("Cannot find pipe service process\n");
        return -1;
    }

	while (NULL != fgets(buffer, 10, fp)) {
		vsc_pipe_srv_pid = atoi(buffer);
		printf("vsc pipe server pid: %d\n", vsc_pipe_srv_pid);
	}
	pclose(fp);
	return 0;
}

static int vsc_client_pipe_request_cmd(const char *tip_info, int size)
{
	int cmdbuf[16] = {0};
	int retval;

	retval = write(vsc_cmd_send_fd, tip_info, size);
	retval = read(vsc_cmd_recv_fd, cmdbuf, sizeof(cmdbuf));
	if (IS_PIPE_SRV_CMD(cmdbuf[0], cmdbuf[1], cmdbuf[2])) {
		printf("recv cmd %d\n", cmdbuf[3]);
		return cmdbuf[3];
	}
	return -1;
}

static int vsc_client_pipe_terminal_srv(void)
{
	if (vsc_pipe_srv_pid > 0)
		kill(vsc_pipe_srv_pid, SIGTERM);
	return 0;
}
#endif

#endif //__VSC_CLIENT_H_

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/include/xvsdk/xv-types.h`:

```h
#pragma once
#define NOMINMAX

#include <array>
#include <vector>
#include <limits>
#include <string>
#include <memory>

#if defined(__ANDROID__) && !defined(__x86_64__)
#define __XV_DRIVER_ONLY__
#endif

namespace xv {

/**
 * @brief Log levels.
 */
enum class LogLevel {
        debug = 1,
        info,
        warn,
        err,
        critical,
        off
};

enum class SlamStartMode {
    Normal,
    VisionOnly,
    VisionWithGYRO
};

enum DeviceSupport {
    ONLYUSB,
    ONLYDRIVER,
    USBANDDRIVER
};


/**
 * @brief Plug event types.
 */
enum class PlugEventType { Plugin, Unplug };

/**
 * \defgroup packed_struct Align(1) structs.
 * @{
 */
#pragma pack(1)

/**
 * @brief Exposure settings.
 */
struct ExpSetting{
	std::uint32_t iso_value;
	std::uint32_t exp_abs;
	std::uint8_t  exp_mode;
	std::int8_t   exp_level;
	std::uint8_t  exp_anti;    // ANTIBANDING_MODES
	std::uint8_t  iso_mode;
};

/**
 * @brief AWB settings.
 */
struct AwbSetting
{
    std::uint8_t awb_mode;
    std::uint8_t awb_lock;
};

/**
 * @brief AF settings.
 */
struct AfSetting
{
    std::uint8_t af_mode;
    std::uint8_t af_dist;
};

/**
 * @brief Device setting.
 */
struct DeviceSetting
{
    std::uint32_t cmd;   //sensorId:8 + channel:8 + sensorCmd:16

	union {
		std::uint8_t val[24];
		AwbSetting awb;
		ExpSetting exp;
		AfSetting  af;
        std::int16_t val16;
        std::int32_t val32;
	}args;
};

struct IspAecSetting
{
    int32_t maxIsoGainValue;
    int32_t minIsoGainValue;
    int32_t maxExposureTimeUs;
    int32_t minExposureTimeUs;
    uint8_t targetMeanPixelValue;
    int32_t ratioIso;
    int32_t cpt_aec_no_modify_range;
    float exposureTime_control_porprotion_value;
    float exposureTime_control_integral_value;
    float isoGain_control_porprotion_value;
    float isoGain_control_integral_value;
    uint32_t ISP_AEC_MODE;
};

#pragma pack()
/** @} */


/**
 * \defgroup Version Version helper.
 * @{
 */
/**
 * @brief The Version struct
 */
struct Version{
    Version( int major = 0, int minor = 0, int patch = 0 );
    Version( const std::string &s );

    int major = 0; //!< Major number of the version
    int minor = 0; //!< Minor number of the version
    int patch = 0; //!< Patch number of the version

    int key() const;
    std::string toString() const;

    static int (max)();
};

bool operator<(const Version &a, const Version &b);
bool operator<=(const Version &a, const Version &b);
bool operator==(const Version &a, const Version &b);
bool operator!=(const Version &a, const Version &b);
bool operator>=(const Version &a, const Version &b);
bool operator>(const Version &a, const Version &b);

std::ostream& operator<<(std::ostream& o, Version const& v);
/** @} */


template<class F> using Vector2 = std::array<F,2>; //!< \var 2D vector
template<class F> using Vector3 = std::array<F,3>; //!< \var 3D vector
template<class F> using Vector4 = std::array<F,4>; //!< \typedef 4D vector
template<class F> using Matrix3 = std::array<F,9>; //!< \typedef Row major 3x3 matrix

typedef Vector2<bool> Vector2b; //!< \typedef 2D vector of `bool'
typedef Vector3<bool> Vector3b; //!< \typedef 3D vector of `bool'
typedef Vector4<bool> Vector4b; //!< \typedef 4D vector of `bool'
typedef Vector2<double> Vector2d; //!< \typedef 2D vector of `double'
typedef Vector3<double> Vector3d; //!< \typedef 3D vector of `double'
typedef Vector4<double> Vector4d; //!< \typedef 4D vector of `double'
typedef Matrix3<double> Matrix3d; //!< \typedef Row major 3x3 matrix of `double'
typedef Vector2<float> Vector2f; //!< \typedef 2D vector of `float'
typedef Vector3<float> Vector3f; //!< \typedef 3D vector of `float'
typedef Vector4<float> Vector4f; //!< \typedef 4D vector of `float'
typedef Matrix3<float> Matrix3f; //!< \typedef Row major 3x3 matrix of `float'

/**
 * \defgroup xv_rotation_conversions functions to convert between 3D rotations representations
 * @{
 */
/**
 * @brief Convert a rotation matrix to quaternion.
 * @param rot: 3x3 row major rotation matrix
 * @return quaternion [qx,qy,qz,qw]
 */
Vector4f rotationToQuaternion(Matrix3f const& rot);
/**
 * @brief Convert a rotation matrix to quaternion.
 * @param rot: 3x3 row major rotation matrix
 * @return quaternion [qx,qy,qz,qw]
 */
Vector4d rotationToQuaternion(Matrix3d const& rot);

/**
 * @brief Convert quaternion to rotation matrix.
 * @param q: quaternion [qx,qy,qz,qw]
 * @return 3x3 row major rotation matrix
 */
Matrix3f quaternionToRotation(Vector4f const& q);

/**
 * @brief Convert quaternion to rotation matrix.
 * @param q: quaternion [qx,qy,qz,qw]
 * @return 3x3 row major rotation matrix
 */
Matrix3d quaternionToRotation(Vector4d const& q);

/**
 * @brief Deprecated. Same to #quaternionToRotation.
 */
Matrix3f quaternionsToRotation(Vector4f const& q);

/**
 * @brief Convert rotation Euler angles.
 *
 * Be carefull of gimbal lock when using Euler angles.
 *
 * @param rot: 3x3 row major rotation matrix
 * @return [pitch, yaw, roll] in radians
 */
Vector3d rotationToPitchYawRoll(Matrix3d const& rot);

/**
 * @brief Convert rotation Euler angles.
 *
 * Be carefull of gimbal lock when using Euler angles.
 *
 * @param rot: 3x3 row major rotation matrix
 * @return [pitch, yaw, roll] in radians
 */
Vector3f rotationToPitchYawRoll(Matrix3f const& rot);

/** @}
 *
*/

/**
 * Namespace with details (not needed for API usage).
 */
namespace details {

/**
 * Pose of an object in a parent frame coordinate, it correspond to the transformation from object (current) frame coordinates to parent frame coordinates.
 */
template <class F=double>
class Transform_ {

protected:
    Vector3<F> m_translation;
    Matrix3<F> m_rotation;

public:

    static Transform_ Identity() {
        return Transform_({0.,0,0},{1.,0,0,0,1,0,0,0,1});
    }

    Transform_() {};
    Transform_(Vector3<F> const& t, Matrix3<F> const& r={}) : m_translation(t), m_rotation(r) {};

    /**
     * @brief Composition operator for transformations.
     */
    Transform_& operator*=(Transform_ const& q);

    /**
    * @brief Get the translation part of the transformation
    * @return [x,y,z] vector
    */
    Vector3<F> const& translation() const {return m_translation;}
    /**
    * @brief Set the translation part of the transformation
    */
    void setTranslation(Vector3<F> const& v) {m_translation = v;}
    /**
    * @brief Set the translation part of the transformation using pointer to 3D array.
    */
    void setTranslation(F const* v) {std::copy(v, v+3, m_translation.data());}

    /**
    * @brief Get the rotation matrix part of the transformation.
    * @return row major 3x3 matrix
    */
    Matrix3<F> const& rotation() const {return m_rotation;}
    /**
    * @brief Get the rotation matrix part of the transformation.
    */
    void setRotation(Matrix3<F> const& v) {m_rotation = v;}
    /**
    * @brief Set the rotation matrix (row major 3x3) part of the transformation using pointer to array of size 9.
    */
    void setRotation(F const* v) {std::copy(v, v+9, m_rotation.data());}

    /**
    * @brief X coordinate of the translation.
    */
    F x() const { return m_translation[0]; }
    /**
    * @brief Y coordinate of the translation.
    */
    F y() const { return m_translation[1]; }
    /**
    * @brief Z coordinate of the translation.
    */
    F z() const { return m_translation[2]; }
    /**
     * @brief Compute the inverse transformation
     * @return Return the inverse transformation
     */
    Transform_<F> inverse() const;
};

template <class F=double>
Transform_<F> operator*(Transform_<F> lhs, const Transform_<F>& rhs) {
    lhs *= rhs;
    return lhs;
}

/**
 * @brief Transform a vector (from current object coordinates to parent frame coordinates).
 * @param p Point in current object coordinates
 * @return Point in parent frame coordinates.
 */
Vector3d operator*(const Transform_<double>& a, const Vector3d& p);

/**
 * @brief Transform a vector (from current object coordinates to parent frame coordinates).
 * @param p Point in current object coordinates
 * @return Point in parent frame coordinates.
 */
Vector3f operator*(const Transform_<float>& a, const Vector3f& p);

/**
 * @brief Compute the inverse transformation.
 */
Transform_<float> inverse(const Transform_<float>& t);

/**
 * @brief Compute the inverse transformation.
 */
Transform_<double> inverse(const Transform_<double>& t);


template <class F=double>
class TransformQuat_ {

protected:
    Vector3<F> m_translation;
    Vector4<F> m_quaternions;

public:

    static TransformQuat_ Identity() {
        return TransformQuat_({0.,0,0},{0.,0,0,0,1});
    }

    TransformQuat_() {}
    TransformQuat_(Vector3<F> const& t, Vector4<F> const& q={}) : m_translation(t), m_quaternions(q) {}

    TransformQuat_& operator*=(TransformQuat_ const& q);

    Vector3<F> const& translation() const;
    void setTranslation(Vector3<F> const& v);
    void setTranslation(F const* v);

    /**
    * @brief Get the quaternion [qx,qy,qz,qw] of the rotation.
    */
    Vector4<F> const& quaternion() const;
    /**
    * @brief Set the quaternion [qx,qy,qz,qw] of the rotation.
    */
    void setQuaternion(Vector4<F> const& v);
    /**
    * @brief Set the quaternion [qx,qy,qz,qw] of the rotation using pointer to 4D array.
    */
    void setQuaternion(F const* v);

    /**
    * @brief X coordinate of the translation.
    */
    F x() const { return m_translation[0]; }
    /**
    * @brief Y coordinate of the translation.
    */
    F y() const { return m_translation[1]; }
    /**
    * @brief Z coordinate of the translation.
    */
    F z() const { return m_translation[2]; }

    /**
    * @brief qx quaternion composant
    */
    F qx() const { return m_quaternions[0]; }
    /**
    * @brief qx quaternion composant
    */
    F qy() const { return m_quaternions[1]; }
    /**
    * @brief qx quaternion composant
    */
    F qz() const { return m_quaternions[2]; }
    /**
    * @brief qx quaternion composant
    */
    F qw() const { return m_quaternions[3]; }
};
template <class F=double>
TransformQuat_<F> operator*(TransformQuat_<F> lhs, const TransformQuat_<F>& rhs) {
    lhs *= rhs;
    return lhs;
}

template <class F=double>
class PoseQuat_ : public TransformQuat_<F> {
    double m_hostTimestamp = std::numeric_limits<double>::infinity();
    std::int64_t m_edgeTimestampUs = std::numeric_limits<std::int64_t>::min();

public:

    PoseQuat_() {}
    PoseQuat_(double t, TransformQuat_<F>const& tr={}) : m_hostTimestamp(t), TransformQuat_<F> (tr) {}
    PoseQuat_(std::int64_t t, TransformQuat_<F>const& tr={}) : m_edgeTimestampUs(t), TransformQuat_<F> (tr) {}
    PoseQuat_(double t, std::int64_t te,  TransformQuat_<F>const& tr={}) : m_hostTimestamp(t), m_edgeTimestampUs(te), TransformQuat_<F> (tr) {}

    /**
    * @brief Get the edge timestamp of the pose (in microseconds).
    */
    std::int64_t edgeTimestampUs() const {return m_edgeTimestampUs;};
    /**
    * @brief Set the edge timestamp of the pose (in microseconds).
    */
    void setEdgeTimestampUs(std::int64_t t) { m_edgeTimestampUs = t;};
    /**
    * @brief Get the host timestamp corresponding to the pose (in s).
    */
    double hostTimestamp() const {return m_hostTimestamp;};
    /**
    * @brief Set the host timestamp corresponding to the pose (in s).
    */
    void setHostTimestamp(double t) {m_hostTimestamp = t;};
};

template <class F=double>
class PoseRot_ : public Transform_<F> {
    double m_hostTimestamp = std::numeric_limits<double>::infinity();
    std::int64_t m_edgeTimestampUs = std::numeric_limits<std::int64_t>::min();

public:
    PoseRot_() {}
    PoseRot_(double t, Transform_<F>const& tr={}) : m_hostTimestamp(t), Transform_<F> (tr) {}
    PoseRot_(std::int64_t t, Transform_<F>const& tr={}) : m_edgeTimestampUs(t), Transform_<F> (tr) {}
    PoseRot_(double t, std::int64_t te,  Transform_<F>const& tr={}) : m_hostTimestamp(t), m_edgeTimestampUs(te), Transform_<F> (tr) {}

    /**
    * @brief Get the edge timestamp of the pose (in microseconds).
    */
    std::int64_t edgeTimestampUs() const {return m_edgeTimestampUs;};
    /**
    * @brief Set the edge timestamp of the pose (in microseconds).
    */
    void setEdgeTimestampUs(std::int64_t t) { m_edgeTimestampUs = t;};
    /**
    * @brief Get the host timestamp of the pose (in s).
    */
    double hostTimestamp() const {return m_hostTimestamp;};
    /**
    * @brief Set the host timestamp of the pose (in s).
    */
    void setHostTimestamp(double t) {m_hostTimestamp = t;};
};

template <class F=double>
class Pose_ : public details::PoseRot_<F> {

private:

    Vector4<F> m_quaternions;
    double m_confidence = 0;

public:

    static Pose_ Identity() {
        return Pose_({0.,0,0},{1.,0,0,0,1,0,0,0,1});
    }

    Pose_() {}

    /**
    * @brief Construct a pose with a specified confidence.
    */
    Pose_(double c) : m_confidence(c) {}

    /**
    * @brief Construct a pose with a translation, rotation, timestamps and confidence.
    */
    Pose_(Vector3<F> const& translation, Matrix3<F> const& rotation,
          double hostTimestamp = std::numeric_limits<double>::infinity(), std::int64_t edgeTimestamp = (std::numeric_limits<std::int64_t>::min)(), double c=0.)
        : m_confidence(c), PoseRot_<F>::PoseRot_(hostTimestamp, edgeTimestamp, {translation, rotation}), m_quaternions(rotationToQuaternion(rotation)) {}

    /**
    * @brief Get the confidence of the pose. Value in [0,1], 0 means lost.
    */
    double confidence() const {return m_confidence;}

    /**
    * @brief Set the confidence of the pose. Value in [0,1], 0 means lost.
    */
    void setConfidence(double c) { m_confidence = c;}

    /**
    * @brief Get the quaternion of the rotation.
    */
    Vector4<F> const& quaternion() const {return m_quaternions;}
    /**
    * @brief Set the quaternion of the rotation.
    */
    void setQuaternion(Vector4<F> const& v) {
        m_quaternions = v;
        PoseRot_<F>::m_rotation = quaternionToRotation(Pose_<F>::m_quaternions);
    };
    /**
    * @brief Set the quaternion of the rotation using pointer of 4D array.
    */
    void setQuaternion(F const* v) {
        std::copy(v, v+4, m_quaternions.data());
        PoseRot_<F>::m_rotation = quaternionToRotation(Pose_<F>::m_quaternions);
    };
    /**
    * @brief @copybrief xv::details::PoseRot_<F>::setRotation()
    */
    void setRotation(Matrix3<F> const& v) {
        PoseRot_<F>::m_rotation = v;
        m_quaternions = rotationToQuaternion(PoseRot_<F>::m_rotation);
    };
    /**
    * @brief Set the quaternion of the rotation using pointer of 4D array.
    */
    void setRotation(F const* v) {
        std::copy(v, v+9, PoseRot_<F>::m_rotation.data());
        m_quaternions = rotationToQuaternion(PoseRot_<F>::m_rotation);
    };



};

template <class F>
class PosePred_ : public Pose_<F> {

protected:
    Vector3<F> m_linearVelocity = Vector3<F>{0.,0.,0.};
    Vector3<F> m_angularVelocity = Vector3<F>{0.,0.,0.};
    Vector3<F> m_linearAcceleration = Vector3<F>{0.,0.,0.};
    Vector3<F> m_angularAcceleration = Vector3<F>{0.,0.,0.};

public:

    static PosePred_ Identity() {
        return PosePred_({0.,0,0},{1.,0,0,0,1,0,0,0,1});
    }

    PosePred_() {}

    /**
    * @brief Construct a pose with a specified confidence.
    */
    PosePred_(double c) : Pose_<F>::Pose_(c) {}

    /**
    * @brief Construct a pose with a translation, rotation, timestamps and confidence.
    */
    PosePred_(Vector3<F> const& translation, Matrix3<F> const& rotation,
          double hostTimestamp = std::numeric_limits<double>::infinity(), std::int64_t edgeTimestamp = (std::numeric_limits<std::int64_t>::min)(), double c=0.)
        : Pose_<F>::Pose_(translation, rotation, hostTimestamp, edgeTimestamp, c) {}

    Transform_<F> const& transform() const { return *this; }

    /**
     * @brief Get the linear velocity (in m/s) of the pose.
     */
    Vector3<F> const& linearVelocity() const {return m_linearVelocity; }
    /**
     * @brief Get the angular velocity (x,y,z) in rad/s
     */
    Vector3<F> const& angularVelocity() const {return m_angularVelocity; }
    /**
     * @brief Set the linear velocity (in m/s)
     */
    void setLinearVelocity(Vector3<F> const& v) { m_linearVelocity=v; }
    /**
     * @brief Set the angular velocity (x,y,z) in rad/s
     */
    void setLinearVelocity(F const* v) {
            std::copy(v, v+3, m_linearVelocity.data());
    }
    /**
     * @brief Set an estimation of instantaneous angular velocity of the pose.
     * @param v [wx,wy,wz] angular velocity
     */
    void setAngularVelocity(Vector3<F> const& v) { m_angularVelocity=v; }
    /**
     * @brief Set an estimation of instantaneous angular velocity of the pose.
     * @param v [wx,wy,wz] angular velocity
     */
    void setAngularVelocity(F const* v) {
        std::copy(v, v+3, m_angularVelocity.data());
    }

    /**
     * @brief Get the linear acceleration (in m/s/s) of the pose.
     */
    Vector3<F> const& linearAcceleration() const {return m_linearAcceleration; }
    /**
     * @brief Get the angular acceleration (x,y,z) in rad/s/s
     */
    Vector3<F> const& angularAcceleration() const {return m_angularAcceleration; }
    /**
     * @brief Set the linear acceleration (in m/s)
     */
    void setLinearAcceleration(Vector3<F> const& v) { m_linearAcceleration=v; }
    /**
     * @brief Set the angular acceleration (x,y,z) in rad/s/s
     */
    void setLinearAcceleration(F const* v) {
            std::copy(v, v+3, m_linearAcceleration.data());
    }
    /**
     * @brief Set an estimation of instantaneous angular acceleration of the pose.
     * @param v [ax,ay,az] angular acceleration
     */
    void setAngularAcceleration(Vector3<F> const& v) { m_angularAcceleration=v; }
    /**
     * @brief Set an estimation of instantaneous angular acceleration of the pose.
     * @param v [ax,ay,az] angular acceleration
     */
    void setAngularAcceleration(F const* v) {
        std::copy(v, v+3, m_angularAcceleration.data());
    }
};

}

/**
 * @brief Represents a transformation (or pose) with translation and rotation matrix.
 */
struct Transform : public details::Transform_<double> {
    Transform();
    Transform(details::Transform_<double> && o);
    Transform(Vector3d const& t, Matrix3d const& r);
};
/**
 * @brief Represents atransformation (or pose) with translation and rotation matrix in `float` type.
 */
struct TransformF : public details::Transform_<float> {
    TransformF();
    TransformF(details::Transform_<float> && o);
    TransformF(Vector3f const& t, Matrix3f const& r);
};

/**
 * @brief Transform a vector (from current object coordinates to parent frame coordinates).
 * @param p Point in current object coordinates
 * @return Point in parent frame coordinates.
 */
Vector3f operator*(const TransformF& a, const Vector3f& p);
/**
 * @brief Transform a vector (from current object coordinates to parent frame coordinates).
 * @param p Point in current object coordinates
 * @return Point in parent frame coordinates.
 */
Vector3d operator*(const Transform& lhs, const Vector3d& rhs);
/**
 * @brief Compute the inverse transformation.
 */
TransformF inverse(const TransformF& t);
/**
 * @brief Compute the inverse transformation.
 */
Transform inverse(const Transform& t);


/**
 * @brief Represents a `float` typed transformation (or pose) with translation and quaternion for rotation.
 */
struct TransformQuat : public details::TransformQuat_<double> {
    TransformQuat();
    TransformQuat(Vector3d const& t, Vector4d const& q);
};
/**
 * @brief Represents a `float` typed transformation (or pose) with translation and quaternion for rotation in `float` type.
 */
struct TransformQuatF : public details::TransformQuat_<float> {
    TransformQuatF();
    TransformQuatF(Vector3f const& t, Vector4f const& q);
};

/**
 * @brief Polynomial Distortion Model for camera intrisics
 *
 *  Projection and raytrace formula can be found here:
https://docs.opencv.org/3.4.0/d4/d94/tutorial_camera_calibration.html
    distor[0] : k1
    distor[1] : k2
    distor[2] : p1
    distor[3] : p2
    distor[4]: k3
 */
struct PolynomialDistortionCameraModel {
    /**
     * @brief Image width (in pixel)
     */
    int w;
    /**
     * @brief Image height (in pixel)
     */
    int h;
    /**
     * @brief Focal length in width direction (in pixel)
     */
    double fx;
    /**
     * @brief Focal length in height direction (in pixel)
     */
    double fy;
    /**
     * @brief Optical axis intersection in width direction (in pixel)
     */
    double u0;
    /**
     * @brief Optical axis intersection in height direction (in pixel)
     */
    double v0;
    /**
     * @brief Distortion parameters.
     *
     * https://docs.opencv.org/3.4.0/d4/d94/tutorial_camera_calibration.html
        distor[0] : k1
        distor[1] : k2
        distor[2] : p1
        distor[3] : p2
        distor[4]: k3
     */
    std::array<double,5> distor;
    PolynomialDistortionCameraModel();
};

/**
 * @brief Unified Camera Model
 */
struct UnifiedCameraModel {
    /**
     * @brief Image width (in pixel)
     */
    int w;
    /**
     * @brief Image height (in pixel)
     */
    int h;
    /**
     * @brief Focal length in width direction (in pixel)
     */
    double fx;
    /**
     * @brief Focal length in height direction (in pixel)
     */
    double fy;
    /**
     * @brief Optical axis intersection in width direction (in pixel)
     */
    double u0;
    /**
     * @brief Optical axis intersection in height direction (in pixel)
     */
    double v0;
    /**
     * @brief xi parameter of Unified Camera Model
     */
    double xi;
};

/**
 * @brief Calibration parameters of a camera using Unified Camera Model for camera intrinsics
 */
struct UcmCameraCalibration {
    Transform pose;
    UnifiedCameraModel intrinsics;
};

/**
 * @brief Calibration parameters of a camera using Polynomial Distortion Model for camera intrinsics
 */
struct PdmCameraCalibration {
    Transform pose;
    PolynomialDistortionCameraModel intrinsics;
};

/**
 * @brief Class representing a 6dof pose at a timestamp with a linear model for prediction.
 *
 * This class has both quaternion and rotation matrix to represent the 3D rotation.
 *
 */
struct Pose : public details::PosePred_<double> {
    static Pose Identity() {
        return Pose({0.,0,0},{1.,0,0,0,1,0,0,0,1});
    }
    Pose();
    /**
    * @brief Construct a pose with a translation, rotation, timestamps and confidence.
    */
    Pose(Vector3d const& translation, Matrix3d const& rotation,
          double hostTimestamp = std::numeric_limits<double>::infinity(), std::int64_t edgeTimestamp = (std::numeric_limits<std::int64_t>::min)(), double c=0.);
    /**
     * @brief Prediction of the pose based on angular and linear velocity and acceleration.
     * @param dt amount of prediction (in s)
     * @return The predicted (2nd order extrapolation) of the orientation.
     */
    Pose prediction(double dt) const;
};

/**
 * @brief Class representing a 6dof pose at a timestamp with a linear model for prediction.
 *
 * This class has both quaternion and rotation matrix to represent the 3D rotation.
 *
 */
struct PoseF : public details::PosePred_<float> {
    static Pose Identity() {
        return Pose({0.,0,0},{1.,0,0,0,1,0,0,0,1});
    }
    PoseF();
    /**
    * @brief Construct a pose with a translation, rotation, timestamps and confidence.
    */
    PoseF(Vector3f const& translation, Matrix3f const& rotation,
          double hostTimestamp = std::numeric_limits<double>::infinity(), std::int64_t edgeTimestamp = (std::numeric_limits<std::int64_t>::min)(), double c=0.);

    /**
     * @brief Prediction of the pose based on angular and linear velocity and acceleration.
     * @param dt amount of prediction (in s)
     * @return The predicted (2nd order extrapolation) of the orientation.
     */
    PoseF prediction(double dt) const;
};

/**
 * @brief Event.
 */
struct Event {
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    int type = 0; //!< Type of event
    int state = 0; //!< State of the event
};

/**
 * @brief Orientation only (3dof) of the pose.
 */
class Orientation {

    Matrix3d m_rotation;
    Vector4d m_quaternions; //!< [qx,qy,qz,qw]
    Vector3d m_angularVelocity = Vector3d{0.,0.,0.};
    Vector3d m_angularAcceleration = Vector3d{0.,0.,0.};

public:
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the plane (in second based on the `std::chrono::steady_clock`).
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the plane (in microsecond based on edge clock).

    Orientation();

    /**
    * @brief Construct an orientation (3dof) rotation and timestamps.
    */
    Orientation(Matrix3d const& rotation,
          double hostTimestamp = std::numeric_limits<double>::infinity(), std::int64_t edgeTimestamp = (std::numeric_limits<std::int64_t>::min)());

    /**
    * @brief Construct an orientation (3dof) rotation and timestamps.
    */
    Orientation(Vector4d const& quaternion,
          double hostTimestamp = std::numeric_limits<double>::infinity(), std::int64_t edgeTimestamp = (std::numeric_limits<std::int64_t>::min)());

    /**
    * @brief Get the quaternion of the rotation [qx,qy,qz,qw].
    */
    Vector4d const& quaternion() const;
    /**
    * @brief Set the quaternion of the rotation [qx,qy,qz,qw].
    */
    void setQuaternion(Vector4d const& v);
    /**
    * @brief Set the quaternion of the rotation using pointer of 4D array [qx,qy,qz,qw].
    */
    void setQuaternion(double const* v);

    /**
    * @brief @copybrief details::PoseRot_<F>::rotation()
    */
    Matrix3d const& rotation() const {return m_rotation;}

    /**
    * @brief @copybrief details::PoseRot_<F>::setRotation()
    */
    void setRotation(Matrix3d const& v);
    /**
    * @brief Set the quaternion of the rotation using pointer of 4D array.
    */
    void setRotation(double const* v);
    /**
     * @brief An estimation of instantaneous angular velocity of the pose.
     * @return [wx,wy,wz] angular velocity
     */
    Vector3d const& angularVelocity() const;

    /**
     * @brief Set an estimation of instantaneous angular velocity of the pose.
     * @param v [wx,wy,wz] angular velocity
     */
    void setAngularVelocity(Vector3d const& v);

    /**
     * @brief Set an estimation of instantaneous angular velocity of the pose.
     * @param v [wx,wy,wz] angular velocity
     */
    void setAngularVelocity(double const* v);

    /**
     * @brief Set an estimation of instantaneous angular acceleration of the pose.
     * @param v [ax,ay,az] angular acceleration (rad/s/s)
     */
    void setAngularAcceleration(Vector3d const& v);

    /**
     * @brief Set an estimation of instantaneous angular acceleration of the pose.
     * @param v [ax,ay,az] angular acceleration (rad/s/s)
     */
    void setAngularAcceleration(double const* v);

    /**
     * @brief Prediction of the orientation based on angular velocity and acceleration.
     * @param dt amount of prediction (in s)
     * @return The predicted (2nd order extrapolation) of the orientation.
     */
    Orientation prediction(double dt) const;

};

/**
 * @brief A 3D plane definition
 *
 * The plane is represented with normal equation x n[0] + y n[1] + z n[2] - d = 0
 */
struct Plane {
    /// @brief Plane unique identifier
    std::string id;

    /// @brief Unit vector normal to the plane
    Vector3d normal;

    /// @brief Signed distance to origin.
    /// Signed distance between the plane and the origin of the world. The distance is
    /// signed according to the direction of the normale.
    double d;

    /// @brief Points lying at the border of the plane.
    /// Array of 3D points lying on the plane that describes the
    /// polygon that borders the actually detected area.
    std::vector<Vector3d> points;

    /// @brief Flat, 3D, triangle mesh describing the detailed plane geometry extents.
    /// More convenient than the border points.
    std::vector<Vector3d> vertices;
    std::vector<std::array<uint32_t,3>> triangles;
};


/**
 * @brief A sparse SLAM map with 3D points.
 */
struct SlamMap
{
  std::vector<Vector3d> vertices;
};

/**
 * @brief Generic camera model
 */
class CameraModel {
public:
    virtual std::int32_t width() const {return 0; }
    virtual std::int32_t height() const {return 0; }
    virtual bool project(double const* /*p3d*/, double* /*p2d*/) const {return false;};
    virtual bool raytrace(double const* /*p2d*/, double* /*p3d*/) const {return false;};
};

/**
 * @brief Calibration (extrinsics and intrinsics).
 */
struct Calibration {
    Transform pose; //! pose of the sensor(camera and display) in the IMU frame coordinates.
    std::vector<UnifiedCameraModel> ucm; //! Deprecated, better to use camerasModel; List of Unified Camera Model parameters for differents camera resolutions (see UnifiedCameraModel#w and UnifiedCameraModel#h to find the corresponding resolution of the parameter set).
    std::vector<PolynomialDistortionCameraModel> pdcm; //! Deprecated, better to use camerasModel; List of Polynomial Distortion Camera Model parameters for differents camera resolutions (see UnifiedCameraModel#w and UnifiedCameraModel#h to find the corresponding resolution of the parameter set).
    std::vector<std::shared_ptr<CameraModel>> camerasModel; //! Can be ucm, pdcm or any other calibration model loaded from the device
};

/**
 * @brief Data from IMU sensor of the XVisio device.
 *
 * Contains temperature, 3-axis gyrometer, 3-axis accelerometer and 3-axis magnetometer measures.
 */
struct Imu {
    Vector3d gyro; //!< 3-axis gyrometer values (in rad/s)
    Vector3d accel; //!< 3-axis accelerometer values (in m/s²)
    Vector3b accelSaturation; //!< 3-axis accel saturation status (true if saturating)
    Vector3d magneto; //!< 3-axis magnetometer values
    double temperature; //!< sensor temperature (in K)
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
};

/**
 * @brief A grayscale image that is usually an image from a camera used for visual SLAM
 */
struct GrayScaleImage {
    std::size_t width; //!< width of the image (in pixel)
    std::size_t height; //!< height of the image (in pixel)
    std::shared_ptr<const std::uint8_t> data; //! image data (in row-major). Data size = width*height
};

/**
 * @brief Images coming from #xv::FisheyeCameras sensor system used for visual SLAM
 */
struct FisheyeImages {
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`), need to activate IMU stream (with SLAM or IMU callback) to have this value.
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    std::vector<GrayScaleImage> images; //! List of images (typically 2, first is left and second image is the right image)
    std::int64_t id;//! Unique id given by the edge to this instance
};

/**
 * @brief A color image in RGB format
 */
struct RgbImage {
    std::size_t width = 0; //!< width of the image (in pixel)
    std::size_t height = 0; //!< height of the image (in pixel)
    std::shared_ptr<const std::uint8_t> data = nullptr; //! image data (in row-major) : RGB RGB RGB ....  Data size = width*height*3
    RgbImage(std::size_t _width, std::size_t _height, std::shared_ptr<const std::uint8_t> _data) : width(_width), height(_height),data(_data) {}
};

/**
 * @brief A color image given by #xv::ColorCamera
 */
struct ColorImage {
    enum class Codec { YUYV = 0, YUV420p, JPEG, NV12, BITSTREAM};
    Codec codec = Codec::YUYV;
    std::size_t width = 0; //!< width of the image (in pixel)
    std::size_t height = 0; //!< height of the image (in pixel)
    std::shared_ptr<const std::uint8_t> data = nullptr; //! image data
    unsigned int dataSize = 0;
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    /**
     * @brief Convert to a #xv::RgbImage
     */
    RgbImage toRgb() const;
};

/**
 * @brief SGBM CONFIG STRUCT
*/
struct sgbm_config
    {
        int32_t enable_dewarp;
        float dewarp_zoom_factor;
        int32_t enable_disparity;
        int32_t enable_depth;
        int32_t enable_point_cloud;
        float baseline;
        float fov;
        uint8_t disparity_confidence_threshold;
        float homography[9];
        int32_t enable_gamma;
        float gamma_value;
        int32_t enable_gaussian;
        uint8_t mode;//standard 0 /Default:LRcheck 1 /Extended 2 /Subpixel 3
        uint16_t max_distance;//mm
        uint16_t min_distance;//mm

        inline bool operator==(const sgbm_config &cmp) const
        {
            return (enable_dewarp == cmp.enable_dewarp &&
                    dewarp_zoom_factor == cmp.dewarp_zoom_factor &&
                    enable_disparity == cmp.enable_disparity &&
                    enable_depth == cmp.enable_depth &&
                    enable_point_cloud == cmp.enable_point_cloud &&
                    baseline == cmp.baseline &&
                    fov == cmp.fov &&
                    disparity_confidence_threshold == cmp.disparity_confidence_threshold);
        }
        inline bool operator!=(const sgbm_config &cmp) const
        {
            return !(*this == cmp);
        }
    };

/**
 * @brief An image provided by a TOF camera.
 * @note  There are two manufacturers of TOF camera, Pmd and sony.
 *        Pmd TOF depth type is Depth_32，sony TOF depth type is Depth_16.
 *        Cloud type just use for Pmd point cloud,the coordinate system of the point cloud is the camera coordinate system, and the data unit is meters.
 *        Length, width and depth are in meters use Pmd TOF.
 *        Length, width and depth are in metmillimeterers use sony TOF. 
 */
struct DepthImage {
    enum class Type { Depth_16 = 0, Depth_32, IR, Cloud, Raw, Eeprom, IQ };
    Type type = Type::Depth_32;
    std::size_t width = 0; //!< width of the image (in pixel)
    std::size_t height = 0; //!< height of the image (in pixel)
    double  confidence = 0.0; //!< confidence of depth [0.0,1.0]
    std::shared_ptr<const std::uint8_t> data = nullptr; //! image of depth
    unsigned int dataSize = 0;
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    /**
     * @brief Convert to a #xv::RgbImage
     */
    RgbImage toRgb() const;
};

struct DepthColorImage {
    std::size_t width = 0; //!< width of the image (in pixel)
    std::size_t height = 0; //!< height of the image (in pixel)
    std::shared_ptr<const std::uint8_t> data = nullptr; //! image data of RGB-D pixels : RGB (3 bytes) D (float 4bytes)
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
};

/**
 * @brief A point cloud of 3D points.
 */
struct PointCloud {
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of ? (in second based on the `std::chrono::steady_clock`).
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of ? (in microsecond based on edge clock).
    std::vector<Vector3f> points;
};

/**
 * @brief Object detection bounding box.
 */
struct Object {
    enum class Shape { BoundingBox = 0, Human, HandSkeleton };
    struct keypoint {
        double x = -1, y = -1, z = -1;
    };

    Shape shape = Shape::BoundingBox;

    int typeID = -1;
    std::string type = "";
    double x = 0;
    double y = 0;
    double width = 0;
    double height = 0;
    double confidence = 0.0f;

    std::vector<keypoint> keypoints;
};


struct CnnRawWrapper{
    std::shared_ptr<float> raw_data = nullptr;
    unsigned int raw_data_length;
};




struct ObjectDescriptor{
    std::string type;
    std::vector<std::string> classes;
    double threshold = 0.5;
    bool flipStereo = false;
    bool flipRgb = false;
    bool flipTof = false;
    bool enable_3dbbox = false;
    std::vector<double> prior_bbox_z;   
};

/**
 * @brief SGBM data.
 * @note Length, width and depth are in millimeters.
 */
struct SgbmImage {
    enum class Type { Disparity = 0, Depth, PointCloud};
    const Type type;
    explicit SgbmImage(Type t) : type(t) {}
    std::size_t width = 0; //!< width of the image (in pixel)
    std::size_t height = 0; //!< height of the image (in pixel)
    std::shared_ptr<const std::uint8_t> data = nullptr; //! data of SGBM
    unsigned int dataSize = 0;
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    /**
     * @brief Convert to a #xv::RgbImage
     */
    RgbImage toRgb() const;
};

/**
 * @brief A color image given by #xv::ThermalCamera
 */
struct ThermalImage {
    enum class Codec {UYVY};
    const Codec codec;
    explicit ThermalImage(Codec t) : codec(t) {}
    std::size_t width = 0; //!< width of the image (in pixel)
    std::size_t height = 0; //!< height of the image (in pixel)
    std::shared_ptr<const std::uint8_t> data = nullptr; //! image data
    unsigned int dataSize = 0;
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    /**
     * @brief Convert to a #xv::RgbImage
     */
    RgbImage toRgb() const;
};

/**
 * @brief A color image given by #xv::EyetrackingCamera
 */
struct EyetrackingImage {
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`), need to activate IMU stream (with SLAM or IMU callback) to have this value.
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    std::vector<GrayScaleImage> images; //! List of images (typically 2, first is left and second image is the right image)
};

struct MicData {
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    std::shared_ptr<const std::uint8_t> data = nullptr; //! image data
    unsigned int dataSize = 0;
};

/**
 * @brief Gesture key point
 */
struct keypoint {
    float x = -1;
    float y = -1;
    float z = -1;
};

/**
 * @brief Gesture data
 */
struct GestureData {
    int index[2] = {-1,-1};//!<Index array for hands gesture, max size is two, default is -1 means invalid.
    keypoint position[2];//!<Position array for hand gesture,  max size is two, 2D points, z isn't used by default.
    keypoint slamPosition[2];//!<Convert rgb points into slam points, Position array for hand gesture,  max size is two.
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    float distance;//!< reserved, dynamic gesture movement distance.
    float confidence;//!<reserved, gesture confidence.
};

enum class ResolutionMode{
    R_VGA,
    R_720P
};

/**
* @union xv_ETPoint2D
*/
typedef union XV_ET_POINT_2D
{
	struct {
		float x, y;
	};
	float seq[2];
}xv_ETPoint2D;

/**
* @union xv_ETPoint3D
*/
typedef union XV_ET_POINT_3D
{
    struct {
        float x, y, z;
    };
    float seq[3];
}xv_ETPoint3D;

/**
* @enum xv_ETEyeType
*/
enum XV_ET_EYE_TYPE {
    L_EYE = 1,//!<left eye
    R_EYE = 2//!<right eye
};
typedef XV_ET_EYE_TYPE xv_ETEyeType;

/**
* @enum xv_ETMode
*/
enum XV_ET_MODE {
	track = 3,//!<eye track mode
	iris = 5//!<iris identify mode    	        
};
typedef XV_ET_MODE xv_ETMode;

/**
* @struct xv_ETInitParam
*/
struct XV_ET_INIT_PARAM {
	xv_ETMode mode;//!< sdk mode, refer to xv_ETMode.
	char configPath[260];//!<config file path.
};
typedef XV_ET_INIT_PARAM xv_ETInitParam;

/**
* @struct xv_ETCoefficient
*/
typedef struct XV_ET_COEFFICIENT {
	unsigned char buf[1024];//!<calibration factor.
}xv_ETCoefficient;


enum XV_EyeGazeExDataValidity {
    EYE_GAZE_EXDATA_SCORE = 0,//!<gaze score.
};

/**
* @struct
* gaze struct
*/
struct XV_ET_GAZE_POINT
{
    unsigned int gazeBitMask;//!<gaze bit mask, identify the six data below are valid or invalid.    
    xv_ETPoint3D gazePoint;//!<gaze point, x and y are valid, z default value is 0, x and y scope are related to the input calibration point, not fixed.    
    xv_ETPoint3D rawPoint;//!<gaze point before smooth, x and y are valid, z default value is 0, x and y scope are as above.
    xv_ETPoint3D smoothPoint;//!<gaze point after smooth, x and y are valid, z default value is 0, x and y scope are as above.
    xv_ETPoint3D gazeOrigin;//!<origin gaze center coordinate.
    xv_ETPoint3D gazeDirection;//!<gaze direction.
    float re;//!<gaze re value, confidence level.
    unsigned int exDataBitMask;//!<reserved data.
    float exData[32];//!<reserved data.
};

/**
* @struct
* pupil struct
*/
struct XV_ET_PUPIL_INFO
{
    unsigned int pupilBitMask;//!<pupil bit mask, identify the six data below are valid or invalid.
    xv_ETPoint2D pupilCenter;//!<pupil center(0-1), the coordinate value of pupil center in the image, normalization value, image height and width is 1.
    float pupilDistance;//!<the distance between pupil and camera(mm)
    float pupilDiameter;//!<pupil diameter, pupil long axis value(0-1), the ratio of the pixel value of the long axis size of the pupil ellipse to the image width, normalization value.
    float pupilDiameterMM;//!<pupil diameter, pupil long axis value(mm).
    float pupilMinorAxis;//!<pupil diameter, pupil minor axis value(0-1), the ratio of the pixel value of the minor axis size of the pupil ellipse to the image width, normalization value.
    float pupilMinorAxisMM;//!<pupil diameter, pupil minor axis value(mm).
};

/**
* @struct
* eye extend data struct
*/
struct XV_ET_EYE_EXDATA
{
    unsigned int eyeDataExBitMask;//!<eye extend data bit mask, identify the four data below are valid or invalid.
    int blink;//!<blink data, 0-no blink, 1-start blinking, 2-closing process, 3-close eyes, 4-opening process, 5-finish blinking.
    float openness;//!<eye openness(0-100), 0-cloing, 100-opening normally, >100-opening on purpose.
    float eyelidUp;//!<up eyelid data(0-1), up eyelid's vertical position in the image, normalization value, image height is 1.
    float eyelidDown;//!<down eyelid data(0-1), down eyelid's vertical position in the image, normalization value, image height is 1.
};

/**
* @struct 
* eye data struct
*/
struct XV_ET_EYE_DATA_EX
{
    unsigned long long timestamp;//!<timestamp.
    int recommend;//!<whether if there has the recommend point. 0-no recommend point, 1-use left eye as recommend point, 2-use right eye as recomment point.                      
    XV_ET_GAZE_POINT recomGaze;//!<recommend gaze data
    XV_ET_GAZE_POINT leftGaze;//!<left eye gaze data
    XV_ET_GAZE_POINT rightGaze;//!<right eye gaze data

    XV_ET_PUPIL_INFO leftPupil;//!<left eye pupil data
    XV_ET_PUPIL_INFO rightPupil;//!<right eye pupil data

    XV_ET_EYE_EXDATA leftExData;//!<left eye extend data(include blink and eyelid data)
    XV_ET_EYE_EXDATA rightExData;//!<right eye extend data(include blink and eyelid data)

    float ipd;//!<The estimated interpupilary distance (IPD)
};

/** Status codes returned by the API. */
typedef enum GazeStatus {
    /** No error. */
    GAZE_STATUS_OK,

    /** Undefined error. */
    GAZE_STATUS_ERROR,

    /** Failed to initialize the API. */
    GAZE_STATUS_INITIALIZE_FAILED,

    /** Failed to terminate the API. */
    GAZE_STATUS_TERMINATE_FAILED,

    /** An invalid parameter was given. */
    GAZE_STATUS_INVALID_PARAMETER,

    /** The operation was invalid. */
    GAZE_STATUS_INVALID_OPERATION,

    /** The device is unavailable. */
    GAZE_STATUS_DEVICE_NOT_AVAILABLE,

    /** The operation timed out. */
    GAZE_STATUS_TIMED_OUT,

    /** Failed to allocate memory. */
    GAZE_STATUS_MEM_ALLOCATION_FAILED
}
GazeStatus;


typedef enum CalibrationApiStatus {
    /**
     * Special status for CalibrationSetup API.
     *   Indicates that this operation already performed 'automatically' inside the algorithm core.
     */
    CALIBRATION_API_STATUS_COMPLETE_AUTOMATICALLY,

    /** Indicates that this operation already completed by a previous explict call. */
    CALIBRATION_API_STATUS_COMPLETE_MANUALLY,

    /**
     * Indicate that the API is ready for calling.
     *   Or failing on a CalibrationSetup explicit call (special definition)
     */
    CALIBRATION_API_STATUS_ACCEPT_CALLING,

    /** Indicate that the API is NOT ready for calling, could lead to undefined behavior if calling the not-ready-API */
    CALIBRATION_API_STATUS_DO_NOT_CALL,

    /** Special status for CalibrationSetup API, since its internal computation may take longer time than the other APIs */
    CALIBRATION_API_STATUS_BUSY
}
CalibrationApiStatus;

/** A struct to represent the current calibration routine API status */
typedef struct CalibrationStatus {
    /** The status of API CalibrationEnter */
    CalibrationApiStatus enter_status;

    /** The status of API CalibrationCollect */
    CalibrationApiStatus collect_status;

    /** The status of API CalibrationSetup */
    CalibrationApiStatus setup_status;

    /** The status of API CalibrationComputeApply */
    CalibrationApiStatus compute_apply_status;

    /** The status of API CalibrationLeave */
    CalibrationApiStatus leave_status;

    /** The status of API CalibrationReset */
    CalibrationApiStatus reset_status;
}
CalibrationStatus;

/** A structure represents the calibration data used by the eye tracker. */
typedef struct GazeCalibrationData {
    /** The calibration data used by the eye tracker. */
    void* data = nullptr;

    /** The size of the calibration data used by the eye tracker. */
    size_t size;

    /** Default Destructor */
    ~GazeCalibrationData()
    {
        if (data != nullptr)
        {
            free(data);
            data = nullptr;
        }
    };
}
GazeCalibrationData;

struct XV_IRIS_DATA
{
    std::vector<char> name;
    std::vector<unsigned char> feature;
    int size;
    int error;
};

/**
* @struct 
* UTC time struct
*/
struct DateTime {
  int Y, M, D;//!<Year, month, and day
  int h, m;//!<Hour and minutes
  int s;//!<Seconds
};

/**
* @struct 
* GPS distance data struct
*/
struct GPSDistanceData {
  int           distance;//!<Distance
  int           signal;//!<Signal Intensity
  unsigned char sum;//!<Check sum
};

struct Triple
{
  Vector2<unsigned short> p2d;
  size_t i3d;
  Triple(Vector2<unsigned short> const& p2, size_t i3): p2d(p2), i3d(i3) {}
};
struct PointMatches
{
    FisheyeImages fisheyeImages;
    std::vector<std::vector<Triple>> matches;
};

}

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/include/xvsdk/xv-sdk.h`:

```h
#pragma once

#include <map>
#include <functional>
#if defined( __ANDROID__ )
#include "jni.h" 
#endif
#include "xv-types.h"

namespace xv {

class Device;


/**
 * @brief Stream interface.
 */
template <typename T>
class Stream {
public:

    using Data = T;

    /**
     * @brief start streaming.
     */
    virtual bool start() = 0;
    /**
     * @brief stop streaming.
     */
    virtual bool stop() = 0;

    /**
     * @brief Register callback to receive data.
     */
    virtual int registerCallback(std::function<void (T)>) = 0;
    /**
     * @brief Unregister callback.
     */
    virtual bool unregisterCallback(int callbackId) = 0;

};

/**
 * @brief Camera interface
 */
class Camera {
public:

    /**
     * @brief Get the camera calibration.
     *
     * The frames coordinates are defined according to the IMU frame coordinates. If 2 fisheyes cameras the first is left and second is right camera.
     */
    virtual const std::vector<Calibration>& calibration();

    // TODO add more settings like AWB in later release
    virtual bool setResolution( int resolution );
    virtual bool setFramerate( float framerate );
    // aecMode 0:auto 1:manual
    /**
     * @brief Exposure setting.
     * @param[in] aecMode 0:auto exposure 1:manual exposure
     * @param[in] exposureGain Only valid in manual exposure mode, [0,255]
     * @param[in] exposureTimeMs Only valid in manual exposure mode, in milliseconds
     */
    virtual bool setExposure( int aecMode = 0,  int exposureGain = 0,  float exposureTimeMs = 0.0 );

    /**
     * @brief Set output image brightness. Only valid in auto exposure mode
     * @param[in] brightness brightness of image, [0,255]
     */
    virtual bool setBrightness( int brightness );

    virtual ~Camera(){}
};

/**
 * @brief The class to give access to data provided by the IMU sensor.
 */
class ImuSensor : virtual public Stream<Imu const &> {
public:

    virtual ~ImuSensor(){}
};

/**
 * @brief A class to handle callbacks of events.
 *
 * DataStructure Data structure of PSensor status data:
 * 
 * - Header:
 * 
 *      Bytes index: 0
 * 
 *      Bytes length: 2
 * 
 *      Const value: 0x01, 0xbd
 * 
 * - Sensor type:
 * 
 *      Bytes index: 2
 * 
 *      Bytes length: 1
 * 
 *      Const value: 0xdf
 * 
 *  - PSensor Data:
 *      
 *      Bytes index: 60
 * 
 *      Bytes length: 2
 * 
 *      value: 0x02, 0x00-far away from psensor, 0x02, 0x01-near psensor.
 * @code 
 example:
  
    device->eventStream()->registerCallback( [](xv::Event const & event){

        if(event.type == 0x02)
        {
        }

    });
 @endcode
 * 
 */
class EventStream : virtual public Stream<Event const &> {
public:

    virtual ~EventStream(){}
};

/**
 * @brief The class to give access to 3dof data which converted from raw IMU data.
 */
class OrientationStream : virtual public Stream<Orientation const &> {
public:

    /**
     * @brief Get the current orientation of the device.
     *
     * The orientation (3dof) is the rotation of the IMU frame coordinates based on the world frame coordinates. The world frame coordinates coorespond the IMU coordinates when #startOrientation().
     *
     * @param[out] result orientation corresponding to the timestamp "now" + "prediction"
     * @param[in] prediction (in s) amount of prediction to use to get the orientation corresponding to the future
     * @return true if ok, false else.
     */
    virtual bool get(Orientation& pose, double prediction = 0.) = 0;

    /**
     * @brief Get the orientation of the device at a given timestamp.
     *
     * The orientation (3dof) is the rotation of the IMU frame coordinates based on the world frame coordinates. The world frame coordinates coorespond the IMU coordinates when #startOrientation().
     *
     * @param[out] result orientation corresponding to the timestamp, need to be not too in the pass or too in the future
     * @param[in] timestamp of the requested orientation, in s based on the host clock `std::chrono::steady_clock()`
     * @return true if the orientation can be returned, false else. If timestamp is too in the past or too in the future, return false.
     */
    virtual bool getAt(Orientation& pose, double timestamp) = 0;
    virtual ~OrientationStream(){}
};

/**
 * @brief The class to handle callbacks of the multi cameras for the visual SLAM.
 *
 * FisheyeCameras will get 2 or 4 #Calibration parameters. If 2 fisheyes cameras the first is left and second is right camera. For fisheye, per #Calibration only have one #UnifiedCameraModel and one #PolynomialDistortionCameraModel.
 */
class FisheyeCameras : virtual public Stream<FisheyeImages const &>, virtual public Camera {
public:
    virtual int registerAntiDistortionCallback(std::function<void (FisheyeImages const &)> cb ) = 0;
    virtual bool unregisterAntiDistortionCallback( int callbackID ) = 0;

    virtual bool checkAntiDistortionSupport() = 0;

    virtual ~FisheyeCameras(){}
};

/**
 * @brief The class to handle informations about the display (if device can display like a HMD)
 */
class Display {
public:

    /**
     * @brief Get calibrations.
     *
     * Calibration parameters of the multiple parts of the dispaly (for HMD, first display is usually for left eye and second is for right eye).
     */
    virtual const std::vector<Calibration>& calibration() = 0;

    /**
     * @brief Turn on the display
     */
    virtual bool open() = 0;

    /**
     * @brief Turn off the display
     */
    virtual bool close() = 0;

    /**
     * @brief Set brightness level.
     *
     * @param[in] level display brightness level
     */
    virtual bool setBrightnessLevel( int level ) = 0;
};

/**
 * @brief A class to handle callbacks of the color image.
 */
class ColorCamera : virtual public Stream<ColorImage const &>, virtual public Camera {
public:
    enum class Resolution {
        RGB_1920x1080 = 0,  ///< RGB 1080p
        RGB_1280x720  = 1,  ///< RGB 720p
        RGB_640x480   = 2,  ///< RGB 480p
        RGB_320x240   = 3,  ///< RGB QVGA (not supported now)
        RGB_2560x1920 = 4,  ///< RGB 5m (not supported now)
        RGB_3840x2160 = 5,
    };

    enum class Mode {
        AF,
        MF,
        Unknown
    };

    virtual bool setCompensation(int compensation) = 0;
    virtual bool setAwb(int awb) = 0;
    virtual bool setResolution( const Resolution &resolution ) = 0;
    virtual bool isSupportAFRGB() = 0;
    virtual bool setRGBMode(const Mode &mode) = 0;
    virtual bool setRGBFocalDistance(unsigned char distance) = 0;
    virtual bool startCameras(int camIndex = 1) = 0;
    virtual bool stopCameras(int camIndex = 1) = 0;
    virtual bool setCamsResolution(const xv::ColorCamera::Resolution & resolution, int camIndex = 1) = 0;
    virtual bool setCamsFramerate(float framerate, int camIndex = 1) = 0;
    virtual int registerCam2Callback(std::function<void (ColorImage const &)> c ) = 0;
    virtual bool unregisterCam2Callback( int callbackID ) = 0;

    virtual ~ColorCamera(){}
};

/**
 * @brief A class to handle callbacks of the ToF camera
 */
class TofCamera : virtual public Stream<DepthImage const &>, virtual public Camera {
public:
    enum class StreamMode { DepthOnly = 0, CloudOnly, DepthAndCloud, None, CloudOnLeftHandSlam};
    enum class DistanceMode { Short = 0, Middle, Long };
    enum class SonyTofLibMode { IQMIX_DF ,IQMIX_SF, LABELIZE_DF ,LABELIZE_SF ,M2MIX_DF ,M2MIX_SF };
    enum class Framerate{ FPS_5 ,FPS_10 ,FPS_15 ,FPS_20 ,FPS_25 ,FPS_30 };
    enum class Resolution{ Unknown = -1,VGA = 0 ,QVGA ,HQVGA};
    enum class Manufacturer {Unknown = -1, Pmd = 0, Sony};

    /**
     * @brief Gives access to composed image with RBG color on depth images.
     */
    virtual int registerColorDepthImageCallback(std::function<void(const DepthColorImage&)>) = 0;
    virtual bool unregisterColorDepthImageCallback(int callbackId) = 0;

    /**
     * @brief Convert a depth image to point cloud for sony TOF.
     * @return  The point cloud of valid depth image pixels in ToF frame coordinates, nullptr if something went wrong.
     * @note The coordinate system of the point cloud is the camera coordinate system, and the data unit is millimeters.
     */
    virtual std::shared_ptr<PointCloud> depthImageToPointCloud(DepthImage const &) const = 0;

    /**
     * @brief format a depth image to point cloud for pmd TOF(Cloud Only).
     * @return  The point cloud of valid depth image pixels in ToF frame coordinates, nullptr if something went wrong.
     */
    virtual std::shared_ptr<PointCloud> formatPmdCloudToPointCloudStruct(DepthImage const &) const = 0;

    /**
     * @brief Set which stream will be reported. Not work with sony TOF.
     */
    virtual bool setStreamMode(StreamMode mode) = 0;

    /**
     * @brief Set distance mode.
     *
     * Midlle=Short for 010/009 TOF.
     * 
     * SonyTof:
     *  Short   LABELIZE_SF_VGA_30FPS
     *  Middle  M2_DF_VGA_30FPS
     *  Long    IQ_DF_VGA_30FPS
     */
    virtual bool setDistanceMode(DistanceMode mode) = 0;

    /**
     * @brief Get current resolution
     * @return Resolution{ Unknown = -1,VGA = 0 ,QVGA ,HQVGA};
     */
    virtual Resolution getResolution() = 0;

    /**
     * @brief Get tof Manufacturer
     * @return Manufacturer {Unknown = -1, Pmd = 0, Sony};
     */
    virtual Manufacturer getManufacturer() = 0;

    /**
     * @brief Set lib mode.
     *
     * 1. IQMIX_DF
     * 2. IQMIX_SF
     * 3. LABELIZE_DF
     * 4. LABELIZE_SF
     * 5. M2MIX_DF
     * 6. M2MIX_SF
     */
    virtual bool setLibWorkMode(SonyTofLibMode mode) = 0;

    /**
     * @brief Enable tof ir.
     */
    virtual bool enableTofIr(bool enable) = 0;

    /**
     * @brief Set work mode.
     */
    virtual bool setMode(int mode) = 0;

    /**
     * @brief SonyTof Settings
    */
    virtual bool setSonyTofSetting(SonyTofLibMode mode, Resolution resolution, Framerate frameRate) = 0;

    virtual void setTofIrGamma(double gamma) = 0;

    /**
     * @brief set SonyTof filter file
    */
    virtual void setFilterFile(std::string filePath) = 0;

    virtual ~TofCamera() {}
};

/**
 * @brief The class to represent the component doing the 6dof tracking with SLAM algorithm on host.
 *
 * For Mixed mode, callback will get the last computed SLAM pose. Callback is call on each IMU recieved because SLAM pose also use IMU for update.
 */
class Slam : virtual public Stream<Pose const&> {

public:
    enum class Mode { Edge = 0, Mixed, EdgeFusionOnHost };

    /**
     * @brief Get #Mode
     * @return return slam mode of this object, mode is const.
     */
    virtual Mode mode() const = 0;

    /**
     * @brief Start slam of current mode.
     */
    virtual bool start() override = 0;

    /**
     * @brief Start slam of specific mode.
     *
     * Stop old mode slam(if running), switch to new mode, and then start slam of new mode.
     */
    virtual bool start(Mode mode) = 0;

    /**
     * @brief Reset the 6dof tracker (SLAM)
     * @return return true if well reset, else if something went wrong.
     */
    virtual bool reset() = 0;

    /**
     * @brief Pause the 6dof tracker (SLAM)
     * @return return true if well pause, else if something went wrong.
     */
    virtual bool pause() = 0;

    /**
     * @brief Resume the 6dof tracker (SLAM)
     * @return return true if well resume, else if something went wrong.
     */
    virtual bool resume() = 0;

    /**
     * @brief Get the current 6dof pose of the device.
     *
     * The 6dof pose is the rotation and translation of the IMU frame coordinates based on the world frame coordinates. The world frame coordinates coorespond to the VLSAM map and the device
     * coordinates is based on the IMU coordinates.
     *
     * @param[out] pose corresponding to the timestamp "now" + "prediction"
     * @param[in] prediction (in s) amount of prediction to use to get a pose corresponding to the future
     * @return true if ok, false else.
     */
    virtual bool getPose(Pose& pose, double prediction = 0.) = 0;

    /**
     * @brief Get the 6dof pose of the device at a given timestamp.
     *
     * The 6dof pose is the rotation and translation of the IMU frame coordinates based on the world frame coordinates. The world frame coordinates coorespond to the VLSAM map and the device
     * coordinates is based on the IMU coordinates.
     *
     * @param[out] pose result pose corresponding to the timestamp, need to be not too in the pass or too in the future
     * @param[in] timestamp of the wanted pose, in s based on the host clock `std::chrono::steady_clock()`
     * @return true if the pose can be returned, false else. If timestamp is too in the past or too in the future, return false.
     */
    virtual bool getPoseAt(Pose& pose, double timestamp) = 0;

    /**
     * @brief Register a callback called when visual SLAM compute a new unfiltered pose.
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerVisualPoseCallback(std::function<void (const Pose&)> lostCallback) = 0;
    virtual bool unregisterVisualPoseCallback(int callbackId) = 0;
    #define XVSDK_HAS_SLAM_VISUAL_POSE_CALLBACK

    /**
     * @brief Register a callback called when SLAM is lost.
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerLostCallback(std::function<void ()> lostCallback) = 0;
    virtual bool unregisterLostCallback(int callbackId) = 0;

    /**
     * @brief Callback to get the detected planes using stereo cameras and SLAM
     *
     * The vector contains planes with current planes and each plane has an ID. Between mutiple calls new planes can be added, previous planes updated or merged. If a plane disappear from the vector, it means it was merged with other.
     *
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerStereoPlanesCallback(std::function<void (std::shared_ptr<const std::vector<Plane>>)> planeCallback) = 0;
    virtual bool unregisterStereoPlanesCallback(int callbackId) = 0;
    virtual bool clearStereoPlanes() = 0;

    /**
     * @brief Callback to get the detected planes using ToF camera and SLAM
     *
     * The vector contains the current planes with plane ID as key.
     * Between mutiple calls new planes can be added, previous planes updated or merged.
     * If a plane disappears from the vector, it means it was merged with another.
     *
     * If the planes IDs start with "SRP", it means the update comes from the surface reconstruction mapping.
     * If the planes IDs start with "SRIP", it means the update comes from the instantaneous plane detection algorithm.
     * Otherwise the planes comes from the legacy plane detection algorithm.
     *
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerTofPlanesCallback(std::function<void (std::shared_ptr<const std::vector<Plane>>)> planeCallback) = 0;
    virtual bool unregisterTofPlanesCallback(int callbackId) = 0;
    virtual bool clearTofPlanes() = 0;


    /**
     * @brief Callback to get the SLAM map updates.
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerMapCallback(std::function<void (std::shared_ptr<const xv::SlamMap>)> mapCallback) = 0;
    virtual bool unregisterMapCallback(int callbackId) = 0;

    /**
     * @brief Load a SLAM map and use it as an immutable reference map.
     *
     * @param mapStream the input map stream for loading the map
     * @param done_callback When the switch is done the callback will be called. The input of the callback is the quality result (0-100) of the reference map.
     * @param localized_on_reference_map Call the callback if the SLAM uses a reference map and is localized on the reference map. The input parameter is the threshold for the percentage [0.f,1.f]
     * of reference map usage according to whole map (reference map and dynamic map).
     * If SLAM is not currently using enough the reference map and the usage is below this threshold, then the callback is called with the current percentage [0.f,1.f] of
     * 3D points from the reference map.
     */
    virtual bool loadMapAndSwitchToCslam(std::streambuf& mapStream, std::function<void(int /* status of load map */)> done_callback, std::function<void(float)> localized_on_reference_map={}) = 0;
    /**
     * @brief Save a SLAM map and use it as an immutable reference map.
     *
     * @param mapStream the output map stream to for writing the map
     * @param done_callback When the switch is done the callback will be called. The input of the callback is the quality result (0-100) of the reference map.
     * @param localized_on_reference_map Call the callback if the SLAM uses a reference map and is localized on the reference map. The input parameter is the threshold for the percentage [0.f,1.f]
     * of reference map usage according to whole map (reference map and dynamic map).
     * If SLAM is not currently using enough the reference map and the usage is below this threshold, then the callback is called with the current percentage [0.f,1.f] of
     * 3D points from the reference map.
     */
    virtual bool saveMapAndSwitchToCslam(std::streambuf& mapStream, std::function<void(int /* status of save map */, int /* map quality */)> done_callback, std::function<void(float)> localized_on_reference_map={}) = 0;

    /**
     * @brief slam pose scale calibration.
     *
     * @param raw data
     * @return pose recalibrated data
     */
    virtual Pose poseScaleCalibration(const Pose& pose) = 0;

    virtual ~Slam() {}
};

/**
 * @brief A class to handle callbacks of the object detector (CNN)
 */
class ObjectDetector : virtual public Stream<std::vector<Object> const&> {
public:
    enum class Source { LEFT = 0, RIGHT, RGB, TOF };

    virtual bool setDescriptor( const std::string &filepath ) = 0;
    virtual bool setModel( const std::string &filepath ) = 0;
    virtual bool setSource( const Source &source ) = 0;
    virtual xv::ObjectDetector::Source getSource() const = 0;
    virtual xv::ObjectDescriptor getDescriptor() const = 0;

    virtual int registerCnnRawCallback(std::function<void (std::shared_ptr<CnnRawWrapper> const&)> poseCallback) = 0;
    virtual bool unregisterCnnRawCallback(int callbackId) = 0;

    virtual ~ObjectDetector() {}
};

/**
 * @brief A class to handle callbacks of the SGBM.
 */
class SgbmCamera : virtual public Stream<SgbmImage const &>, virtual public Camera {
public:
    enum class Resolution {
        SGBM_640x480   = 0,  ///< SGBM 480p
        SGBM_1280x720  = 1,  ///< SGBM 720p
    };
    enum class Mode { Hardware = 0, Software };
    /**
     * @brief Must be called befor start.
     */
    virtual Mode mode() const = 0;

    virtual bool start(const std::string &sgbmConfig) = 0;
    virtual bool start(const sgbm_config &sgbmConfig) = 0;
    virtual bool setConfig(const std::string &sgbmConfig) = 0;
    virtual bool setSgbmResolution(const xv::SgbmCamera::Resolution & resolution) = 0;
    virtual xv::SgbmCamera::Resolution getSgbmResolution() = 0;
    /**
     * @brief convert DepthImage to pointCloud .
     * 
     * @param sgbmImage 
     * @return std::shared_ptr<PointCloud> 
     * @note The coordinate system of the point cloud is the camera coordinate system, with the left camera as the origin.
     * @note The x, y, depth of the point cloud are in millimeters.
     */
    virtual std::shared_ptr<PointCloud> depthImageToPointCloud(SgbmImage const &sgbmImage) const = 0;
    virtual ~SgbmCamera() {}
};

/**
 * @brief A class to handle callbacks of the thermal camera.
 */
class ThermalCamera : virtual public Stream<ThermalImage const &>, virtual public Camera {
public:

    enum class Mode { TEMPERATURE = 0, TEMPERTURE = 0, GREY };
    virtual bool setMode( Mode mode ) = 0;
    virtual ~ThermalCamera() {}
};

/**
 * @brief A class to handle callbacks of the eyetracking camera.
 */
class EyetrackingCamera : virtual public Stream<EyetrackingImage const &>, virtual public Camera {
public:

    /**
     * @brief Set eyetracking exposure
     *
     * @param[in] leftGain Left eye exposure gain, [0, 255]
     * @param[in] leftTimeMs Left eye exposure time, in milliseconds
     * @param[in] rightGain Right eye exposure gain, [0, 255]
     * @param[in] rightTimeMs Right eye exposure time, in milliseconds
     */
    virtual bool setExposure( int leftGain, float leftTimeMs, int rightGain, float rightTimeMs ) = 0;

    /**
     * @brief Set eyetracking led brightness (in s)
     *
     * @param[in] eye 0:left, 1:right, 2:both
     * @param[in] led [0,7]:led index, 8:all
     * @param[in] brightness [0,255], 0 is off
     */
    virtual bool setLedBrighness( int eye, int led, int brightness ) = 0;

    virtual ~EyetrackingCamera() {}
};

/**
 * @brief A class to handle callbacks of the gaze data.
 */
class GazeStream : virtual public Stream<XV_ET_EYE_DATA_EX const &>{
public:

    /**
     * @brief Set coe configuration file path.
     *
     * @param[in] config string value, end with "/"
     */
    virtual void setConfigPath(std::string config) = 0;

    virtual ~GazeStream() {}
};

/**
 * @brief A class to handle callbacks of the Iris data.
 */
class IrisStream : virtual public Stream<XV_IRIS_DATA const &>{
public:

#if defined( __ANDROID__ )
    virtual bool start(JNIEnv* env, jobject thiz, std::string offlineS) = 0;

    virtual const char* onlineActive(JNIEnv *env, jobject context, const char* initLicense, const char* userId, const char* secret, int &activeResult) = 0;
#endif
    /**
     * @brief Set user name.
     *
     * @param[in] name string value
     */
    virtual void setUserName(std::string name) = 0;

    /**
     * @brief Callback to get the enroll information.
     *
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerEnrollCallback(std::function<void (XV_IRIS_DATA const &)>) = 0;
    virtual bool UnregisterEnrollCallback(int callbackID) = 0;

    /**
     * @brief Callback to get the identify information.
     *
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerIdentifyCallback(std::function<void (XV_IRIS_DATA const &)>) = 0;
    virtual bool UnregisterIdentifyCallback(int callbackID) = 0;

    /**
     * @brief Load the iris features the identify information.
     *
     * @return result of the method, true is successful, false is failure.
     */
    virtual bool loadIrisInfo(unsigned char* iris_features, int size) = 0;

    /**
     * @brief Set coe configuration file path.
     *
     * @param[in] config string value, end with "/"
     */
    virtual void setConfigPath(std::string config) = 0;

    virtual ~IrisStream() {}
};

/**
 * @brief A class to handle gusture.
 */
class GestureStream : virtual public Stream<GestureData const &> {
public:

    /**
     * @brief Callback to get the dynamic gesture information.
     *
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerDynamicGestureCallback(std::function<void (GestureData const &)>) = 0;
    virtual bool UnregisterDynamicGestureCallback(int callbackID) = 0;

    /**
     * @brief Callback to get the keypoints 21Dof information.
     *
     * The vector contains gesture keypoints 21Dof, size 21 means one hand, vector size 42 means two hands,  2D points, z isn't used by default.
     *
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerKeypointsCallback(std::function<void (std::shared_ptr<const std::vector<keypoint>>)> callback) = 0;
    virtual bool unregisterKeypointsCallback(int callbackId) = 0;

    /**
     * @brief Callback to get the keypoints 21Dof information based on slam position.
     *
     * The vector contains gesture keypoints 21Dof based on slam position,  size 21 means one hand, vector size 42 means two hands,  3D points with depth value.
     *
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerSlamKeypointsCallback(std::function<void (std::shared_ptr<const std::vector<Pose>>)> callback) = 0;
    virtual bool unregisterSlamKeypointsCallback(int callbackId) = 0;

    virtual ~GestureStream() {}
};

/**
 * @brief A class to handle callbacks of the GPS data.
 */
class GPSStream : virtual public Stream<std::vector<unsigned char> const &>{
public:
    virtual ~GPSStream() {}
};

/**
 * @brief A class to handle callbacks of the gaze data.
 */
class GPSDistanceStream : virtual public Stream<GPSDistanceData const &>{
public:

    virtual ~GPSDistanceStream() {}
};

/**
 * @brief A class to handle MIC. Adjust volumn through source.
 */
class MicStream : virtual public Stream<MicData const &> {
public:

    virtual ~MicStream() {}
};

/**
 * @brief A class to handle speaker. Adjust the sound source(PCM) volume to adjust the volume.
 */
class Speaker {
public:

    virtual bool enable() = 0;
    virtual bool disable() = 0;
    /**
     * @brief Send a small time slice of sound data.
     */
    virtual int  send(const std::uint8_t *data, int len) = 0;

    /**
     * @brief Async play sound file in new thread.
     */
    virtual bool play(const std::string &path) = 0;
    /**
     * @brief Async play buffer in new thread.
     */
    virtual bool play(const std::uint8_t *data, int len) = 0;
    /**
     * @brief If async playing.
     */
    virtual bool isPlaying() = 0;
    /**
     * @brief Rigster a callback for async play end.
     */
    virtual int registerPlayEndCallback( std::function<void ()> ) = 0;
    virtual bool unregisterPlayEndCallback( int callbackId ) = 0;

    virtual ~Speaker() {}
};

/**
 * @brief A class to handle device status event stream.
 * 
 * DataStructure Data structure of device status data:
 * 
 * - Header:
 * 
 *      Bytes index: 0
 * 
 *      Bytes length: 2
 * 
 *      Const value: 0x01, 0xaf
 * 
 * - Sensor type:
 * 
 *      Bytes index: 2
 * 
 *      Bytes length: 1
 * 
 *      Const value: 0x62
 * 
 * - Timestamp:
 * 
 *      Bytes index: 3
 * 
 *      Bytes length: 8
 * 
 * - Sensor tempreture:
 * 
 *      Bytes index: 11
 * 
 *      Bytes length: 6
 * 
 *      Comments: Two bytes for each sensor, factor is 0.0625, unit is centigrade. For example: if the value is 0x02c0, the tempreture value will be 704*0.0625 = 44.0 centigrade
 * 
 * - CPU tempreture:
 * 
 *      Bytes index: 17
 * 
 *      Bytes length: 1
 * 
 *      Comments: factor is 1, unit is centigrade.
 * 
 * - Current fan speed:
 * 
 *      Bytes index: 18
 * 
 *      Bytes length: 2
 * 
 * - Average fan speed:
 * 
 *      Bytes index: 20
 * 
 *      Bytes length: 2
 * 
 * - Fan speed change:
 * 
 *      Bytes index: 22
 * 
 *      Bytes length: 2
 * 
 * - Previous fan speed:
 * 
 *      Bytes index: 24
 * 
 *      Bytes length: 2
 * 
 * - Curent fan duty cycle:
 * 
 *      Bytes index: 26
 * 
 *      Bytes length: 1
 * 
 * - Fan duty cycle change:
 * 
 *      Bytes index: 27
 * 
 *      Bytes length: 1
 * 
 * - Previous fan duty cycle:
 * 
 *      Bytes index: 28
 * 
 *      Bytes length: 1
 * 
 * - Soft reset:
 * 
 *      Bytes index: 29
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x80-soft reset unsupported, 0x81-soft reset supported
 * 
 * - Frequency:
 * 
 *      Bytes index: 30
 * 
 *      Bytes length: 4
 * 
 * - RGB switch:
 * 
 *      Bytes index: 34
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x0-offline, 0x01-online
 * 
 * - Fisheye switch:
 * 
 *      Bytes index: 35
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x00-offline, 0x01-online. The upper four bits are status of four cameras, bit7-sen_right2, bit6-sen_left2, bit5-sen_right, bit4-sen_left
 * 
 * - TOF switch:
 * 
 *      Bytes index: 36
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x00-offline, 0x01-online
 * 
 * - UAC speaker switch:
 * 
 *      Bytes index: 37
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x00-offline, 0x01-online with normal streaming, 0x02-online with interrupted streaming
 * 
 * - UAC mic switch:
 * 
 *      Bytes index: 38
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x00-offline, 0x01-online with normal streaming, 0x02-online with interrupted streaming
 * 
 * - Audio speaker switch:
 * 
 *      Bytes index: 39
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x00-offline, 0x01-online
 * 
 * - Audio mic switch:
 * 
 *      Bytes index: 40
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x00-offline, 0x01-online
 * 
 * - DP switch:
 * 
 *      Bytes index: 41
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x00-offline, 0x01-online
 * 
 * - Panel switch:
 * 
 *      Bytes index: 42
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x00-offline, 0x01-online
 * 
 * - USB status:
 * 
 *      Bytes index: 43
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x00-interface up, 0x01-interface down, 0x02-suspend, 0x03-resume, 0x04-reset
 * 
 * - USB reset flag:
 * 
 *      Bytes index: 44
 * 
 *      Bytes length: 1
 * 
 *      Comment: 0x00-normal, 0x01-reset
 * 
 * DataStructure of handle controller data:
 * 
 * - Header:
 * 
 *      Bytes index: 0
 * 
 *      Bytes length: 2
 * 
 *      Const value: 0x01, 0xaf
 * 
 * - Sensor type:
 * 
 *      Bytes index: 2
 * 
 *      Bytes length: 1
 * 
 *      Const value: 0x71
 * 
 * - Handle controller type:
 * 
 *      Bytes index: 3
 * 
 *      Bytes length: 1
 * 
 *      Const value: 0x22-left handle controller, 0x23-right handle controller
 * 
 * - Timestamp:
 * 
 *      Bytes index: 4
 * 
 *      Bytes length: 2
 * 
 *      Const value: 0x22-left handle controller, 0x23-right handle controller
 * 
 * - Acc x:
 * 
 *      Bytes index: 4
 * 
 *      Bytes length: 2
 * 
 * - Acc y:
 * 
 *      Bytes index: 4
 * 
 *      Bytes length: 2
 * 
 * - Acc z:
 * 
 *      Bytes index: 4
 * 
 *      Bytes length: 2
 * 
 * - Gyro x:
 * 
 *      Bytes index: 4
 * 
 *      Bytes length: 2
 * 
 * - Gyro y:
 * 
 *      Bytes index: 4
 * 
 *      Bytes length: 2
 * 
 * - Gyro z:
 * 
 *      Bytes index: 4
 * 
 *      Bytes length: 2
 * 
 * DataStructure of handle controller key eventdata:
 * 
 * - Header:
 * 
 *      Bytes index: 0
 * 
 *      Bytes length: 2
 * 
 *      Const value: 0x01, 0xaf
 * 
 * - Sensor type:
 * 
 *      Bytes index: 2
 * 
 *      Bytes length: 1
 * 
 *      Const value: 0x71
 * 
 * - Handle controller type:
 * 
 *      Bytes index: 3
 * 
 *      Bytes length: 1
 * 
 *      Const value: 0x32-left handle controller, 0x33-right handle controller
 * 
 * - Trigger:
 * 
 *      Bytes index: 4
 * 
 *      Bytes length: 1
 * 
 * - Side trigger:
 * 
 *      Bytes index: 5
 * 
 *      Bytes length: 1
 * 
 * - Rocker x:
 * 
 *      Bytes index: 6
 * 
 *      Bytes length: 2
 * 
 * - Rocker y:
 * 
 *      Bytes index: 8
 * 
 *      Bytes length: 2
 * 
 * - Key:
 * 
 *      Bytes index: 10
 * 
 *      Bytes length: 1
 * 
 * - Battery:
 * 
 *      Bytes index: 11
 * 
 *      Bytes length: 2
 * @code 
 example:
  
    device->deviceStatus()->registerCallback( [](const std::vector<unsigned char>& deviceStatus){
       
    });
 @endcode
 */
class DeviceStatusStream : virtual public Stream<std::vector<unsigned char> const &> {
public:

    virtual ~DeviceStatusStream(){}
};


/**
 * @brief Class to get tracking results and raw outputs with a connected device.
 *
 * This class is the main entry point of the API, it gives access to the device and algorithms. See xv::getDevices() to have an instance corresponding to a device.
 *
 * A device can have multiple components, accessible with member functions :
 * - #xv::Device::slam() : 6dof tracker doing the SLAM algorithm on host based on informations coming from device (stereo camera, IMU sensor, ..)
 * - #xv::Device::imuSensor() : sensor with at least 3-axis accelerometer and 3-axis gyrometer
 * - #xv::Device::fisheyeCameras(): typically 2 fisheye cameras used for Visual SLAM
 * - #xv::Device::tofCamera(): a depth camera sensor
 * - #xv::Device::edge(): used to run some algorithm directly on embedded device (typically Visual SLAM) when it is possible to choose between host and edge processing
 * - #xv::Device::display(): used to handle informations about the display (if device can display like a HMD)
 * - #xv::Device::objectDetector(): used to run and get results of the CNN object detector
 *
 * If a device does not support a component or doesn't have the component (for example ToF), the accessor function will return `null_ptr`.
 * The data streams and processings under a component are activated only if at least one callback is registerd to the component. If all the callbacks are unregistered then steams can be deactivated.
 */
class Device {

public:


    /**
     * @brief Get informations (Serial Number, version ...) about the device.
     * @return A map with key and values of the informations.
     */
    virtual std::map<std::string, std::string> info() = 0;

    /**
     * @brief Get the SLAM component.
     */
    virtual std::shared_ptr<Slam> slam() = 0;

    /**
     * @brief Get the IMU sensor of the device.
     */
    virtual std::shared_ptr<ImuSensor> imuSensor() = 0;

    /**
     * @brief Get the event component.
     */
    virtual std::shared_ptr<EventStream> eventStream() = 0;

    /**
     * @brief Get the 3dof component.
     */
    virtual std::shared_ptr<OrientationStream> orientationStream() = 0;

    /**
     * @brief Get the stereo cameras component of the device.
     */
    virtual std::shared_ptr<FisheyeCameras> fisheyeCameras() = 0;

    /**
     * @brief Get the color camera component of the device.
     */
    virtual std::shared_ptr<ColorCamera> colorCamera() = 0;

    /**
     * @brief Get the ToF component of the device.
     */
    virtual std::shared_ptr<TofCamera> tofCamera() = 0;

    /**
     * @brief Get the SGBM component of the device.
     */
    virtual std::shared_ptr<SgbmCamera> sgbmCamera() = 0;

    /**
     * @brief Get the thermal component of the device.
     */
    virtual std::shared_ptr<ThermalCamera> thermalCamera() = 0;

    /**
     * @brief Get the eyetracking component of the device.
     */
    virtual std::shared_ptr<EyetrackingCamera> eyetracking() = 0;

    /**
     * @brief Get the gaze data of the device.
     */
    virtual std::shared_ptr<GazeStream> gaze() = 0;

    /**
     * @brief Get the iris data of the device.
     */
    virtual std::shared_ptr<IrisStream> iris() = 0;

   /**
     * @brief Get the gesture component.
     */
    virtual std::shared_ptr<GestureStream> gesture() = 0;

    /**
     * @brief Get the GPS data of the device.
     */
    virtual std::shared_ptr<GPSStream> gpsModule() = 0;

    /**
     * @brief Get the GPS distance data of the device.
     */
    virtual std::shared_ptr<GPSDistanceStream> gpsDistanceModule() = 0;

    /**
     * @brief Get the MIC component of the device.
     */
    virtual std::shared_ptr<MicStream> mic() = 0;

    /**
     * @brief Get the speaker component of the device.
     */
    virtual std::shared_ptr<Speaker> speaker() = 0;

    /**
     * @brief Get the display component.
     */
    virtual std::shared_ptr<Display> display() = 0;

    /**
     * @brief Get the object detection component.
     */
    virtual std::shared_ptr<ObjectDetector> objectDetector() = 0;

    /**
     * @brief Get the device status component.
     */
    virtual std::shared_ptr<DeviceStatusStream> deviceStatus() = 0;

    /**
     * @brief Let device sleep.
     */
    virtual bool sleep(int level = 0) = 0;
    /**
     * @brief Wake up device.
     */
    virtual bool wakeup() = 0;

    /**
     * @brief Control device.
     */
    virtual bool control(const DeviceSetting &setting) = 0;

    /**
     * @brief Write HID control command and read result. HID command list:
     * 
     * - Refresh rate Setting command: 
     * 
     *      Header: 0x02, 0xfe, 0x20, 0x03, 0x01, 0x09
     * 
     *      Data: 0x02-72hz, 0x03-90hz
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xfe, 0x20, 0x03, 0x01, 0x09, 0x03}, result);
     @endcode
     *
     * - Breathing lamp chip standby setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x01
     * 
     *      Data: 0x01
     * 
     *      Comment: If you want to wake up the sensor, send mode set command to the needed mode.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x01, 0x01}, result);
     @endcode
     * 
     * - Monochrome breathing lamp cycle setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x02,0x01
     * 
     *      Data: 0x01-FF0000 color cycle, 0x02-FFF300 color cycle, 0x03-36FF48 color cycle, 0x04-62F1FF color cycle, 0x05-000CFF color cycle, 0x06-8000FF color cycle.
     * 
     *      Comment: The default rise time is 1.04s, storage time is 0.004s, fall time is 1.04s, closing time is 0.04s, and other times are 0. To change time, refer to breathing speed setting command.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x02, 0x01, 0x01}, result);
     @endcode
     * 
     * - 4-color breathing lamp cycle setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x02, 0x02
     * 
     *      Data: 0x04
     * 
     *      Comment: The default rise time is 1.04s, storage time is 0.004s, fall time is 1.04s, closing time is 0.04s, and other times are 0. To change time, refer to breathing speed setting command. 4-color breathing cycle: 00F5A9->00CBF5->0C61F5->D700EF.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x02, 0x02, 0x04}, result);
     @endcode
     * 
     * - Breathing lamp speed setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x05, 0x30-rise time and storage time setting, 0x05, 0x31-fall time and closing time setting.
     * 
     *      Data: The upper four bits are rise time or fall time, the lower four bits are storage time or closing time.
     * 
     *      Comment: Rise time or fall time: 0000-0s, 0001-0.13s, 0010-0.26s, 0011-0.38s, 0100-0.51s, 0101-0.77s, 0110-1.04s, 0111-1.6s, 1000-2.1s, 1001-2.6s, 1010-3.1s, 1011-4.2s, 1100-5.2s, 1101-6.2s, 1110-7.3s, 1111-8.3s. Storage time or closing time: 0000-0.04s, 0001-0.13s, 0010-0.26s, 0011-0.38s, 0100-0.51s, 0101-0.77s, 0110-1.04s, 0111-1.6s, 1000-2.1s, 1001-2.6s, 1010-3.1s, 1011-4.2s, 1100-5.2s, 1101-6.2s, 1110-7.3s, 1111-8.3s.
     * @code 
     example:

        std::vector<unsigned char> result;

        //set rise time to 1.04s
        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x05, 0x30, 0x60}, result);
     @endcode
     * 
     * - Constant breathing lamp light cycle switch setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x02, 0x03
     * 
     *      Data: 0x08-8 color constant light cycle, 0x14-20 color constant light cycle, 0x60-96 color constant light cycle
     * 
     *      Comment: The default change time is 0.1s. To change time, refer to constant breathing lamp light cycle switch speed setting command. 8 colors cycle: FF18FF->FF1010->FF8000->EFFF00 ->00FF00->00FFFF ->1858FF ->8A00FF. 20 colors cycle: E000FF->E80093->FF000D->E82400->FF5300->E87500->FFA400->E8B200->FFE100->E8E800->97FF00->2DE800->00FF2B->00EB7C->00FFE5->00B0EB->0069FF->0012EB->4000FF->8500EB. 96 colors cycle: FF00FE->F000FF->E000FF->CF00FE->C001FF->B000FF->A000FF->8F00FF->7F00FF->700FF->6000FF->5000FF->3F00FF->2F00FE->2001FF->1000FF->0000FE->0110FF->0020FF->0030FF->0140FF->0050FF->0060FF->0071FE->0080FF->0090FF->00A0FE->00AFFE->00C0FF->00D0FF->01E0FF->00F0FF->01FFFF->00FFF1->00FFE1->00FFD0->01FFC1->00FFB1->00FFA1->01FE91->00FE81->00FF71->00FF61->01FF51->00FF41->00FF31->00FE20->01FF11->00FF01->10FF01->1FFF00->30FF00->40FF01->50FF00->5FFF00->6FFF00->80FF00->90FF00->A0FF01->AFFF00->C0FF00->D0FF00->E0FF01->EFFF00->FFFF01->FFF001->FFE001->FED000->FFC000->FFB001->FF9F00->FF9000->FF7F00->FF7000->FF6100->FF5001->FF4001->FE3000->FF2000->FF1001->FE0000->FF0010->FF0020->FF0030->FF0140->FF0050->FF0060->FE0070->FF0080->FF0090->FF01A1->FE00B0->FF00C0->FF00D0->FF00E0->FF00F0
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x02, 0x03, 0x08}, result);
     @endcode
     *
     * - Constant breathing lamp light cycle switch speed setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x07, 0x03
     * 
     *      Data: 0~255
     * 
     *      Comment: The unit is 100ms, set single color light time, only effect in constant light cycle switch.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x07, 0x03, 0x00}, result);
     @endcode
     * 
     * - Breathing lamp real color mode setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x02
     * 
     *      Data: 0x04
     * 
     *      Comment: Composed of red, green and blue lights. The real color can be formed if the on-off time of the three lights is inconsistent.The default red color rise time is 1.04s, red color storage time is 2.1s, red color fall time is 1.04s, red color closing time is 2.6s, red color delay time is 0s. The default green color rise time is 1.04s, green color storage time is 2.1s, green color fall time is 1.04s, green color closing time is 1.6s, green color delay time is 1.04s. The default blue color rise time is 1.04s, blue color storage time is 2.1s, blue color fall time is 1.04s, blue color closing time is 0.004s, blue color delay time is 3.1s. To change time, refer to breathing lamp real color mode speed setting command.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x02, 0x04}, result);
     @endcode
     * 
     * - Breathing lamp real color mode speed setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x05, 0x30-red color rise time and storage time setting, 0x05, 0x31-red color fall time and closing time setting, 0x05, 0x32-red color delay time. 0x05, 0x35-blue color rise time and storage time setting, 0x05, 0x36-blue color fall time and closing time setting, 0x05, 0x37-blue color delay time. 0x05, 0x3a-green color rise time and storage time setting, 0x05, 0x3b-green color fall time and closing time setting, 0x05, 0x3c-green color delay time.
     * 
     *      Data: The upper four bits are rise time or fall time, the lower four bits are storage time, closing time or delay time.
     * 
     *      Comment: Rise time or fall time: 0000-0s, 0001-0.13s, 0010-0.26s, 0011-0.38s, 0100-0.51s, 0101-0.77s, 0110-1.04s, 0111-1.6s, 1000-2.1s, 1001-2.6s, 1010-3.1s, 1011-4.2s, 1100-5.2s, 1101-6.2s, 1110-7.3s, 1111-8.3s. Storage time or closing time: 0000-0.04s, 0001-0.13s, 0010-0.26s, 0011-0.38s, 0100-0.51s, 0101-0.77s, 0110-1.04s, 0111-1.6s, 1000-2.1s, 1001-2.6s, 1010-3.1s, 1011-4.2s, 1100-5.2s, 1101-6.2s, 1110-7.3s, 1111-8.3s. Delay time: 0000-0s, 0001-0.13s, 0010-0.26s, 0011-0.38s, 0100-0.51s, 0101-0.77s, 0110-1.04s, 0111-1.6s, 1000-2.1s, 1001-2.6s, 1010-3.1s, 1011-4.2s, 1100-5.2s, 1101-6.2s, 1110-7.3s, 1111-8.3s.
     * @code 
     example:

        std::vector<unsigned char> result;

        //set red color delay time to 1.04s
        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x05, 0x30, 0x06}, result);
     @endcode
     * 
     * - Breathing lamp status display setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x03-green, 0x04-red
     * 
     *      Data: 0x01
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x03, 0x01}, result);
     @endcode
     * 
     * - Breathing lamp brightness setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x05, 0x03
     * 
     *      Data: 0x00-3.1875 Ma, 0x01-6.375 Ma, 0x10-12.75Ma, 0x11-25.5 Ma
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x05, 0x03, 0x00}, result);
     @endcode
     * 
     * - Breathing lamp register write command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x05
     * 
     *      Data: value0-register address, value1-register value.
     *      
     *      Comment: For detailed information, please refer to AW2026 chip register manual.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x05, 0x00, 0x00}, result);
     @endcode
     * 
     * - Breathing lamp register read command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x06
     * 
     *      Data: value0-register address, value1-register value.
     * 
     *      Comment: The data will be saved in result data. For detailed information, please refer to AW2026 chip register manual.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x06, 0x00, 0x00}, result);
     @endcode
     *
     * - Breathing lamp speed setting command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x08
     * 
     *      Data: 0x01-fast, 0x02-middle, 0x03-slow.
     * 
     *      Comment: Set breathing lamp speed.
     * 
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x08, 0x01}, result);
     @endcode
     * 
     * - Breathing lamp close command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x06
     * 
     *      Data: 0x06-close.
     * 
     *      Comment: Close breathing lamp.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x02, 0x06}, result);
     @endcode
     *
     * - Breathing lamp param reading command: 
     * 
     *      Header: 0x02, 0xbe, 0x9a
     * 
     *      Type: 0x09
     * 
     *      Comment: The data will be returned in result value, value1-breathing lamp mode, value2-color index, value3-breathing lamp speed. For detailed information, please refer to AW2026 chip register manual.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xbe, 0x9a, 0x09}, result);
     @endcode
     *
     * - Save display panel 2/3D mode command: 
     * 
     *      Header: 0x02, 0xfe, 0x20
     * 
     *      Type: 0x16
     * 
     *      Data: 0x02-72hz, 0x03-90hz.
     * 
     *      Comment: The data will be saved in flash.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xfe, 0x20, 0x16, 0x02}, result);
     @endcode
     *
     * - Read display panel 2/3D mode command: 
     * 
     *      Header: 0x02, 0xfe, 0x20
     * 
     *      Type: 0x17
     * 
     *      Comment: The data will be returned in result value, value1-Hz in flash, value2-Hz of real display.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xfe, 0x20, 0x17}, result);
     @endcode
     *
     * - Set display panel brightness command: 
     * 
     *      Header: 0x02, 0xfe, 0x20
     * 
     *      Type: 0x02
     * 
     *      Data: Range: 0x01-0x20, 32 levels. 1 is darkest, 32 is brightest.
     * 
     *      Comment: The data will be saved in flash.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xfe, 0x20, 0x02, 0x01}, result);
     @endcode
     *
     * - Read display panel brightness command: 
     * 
     *      Header: 0x02, 0xfe, 0x20
     * 
     *      Type: 0x02
     * 
     *      Comment: The data will be returned in result value, value1-display brightness.
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xfe, 0x20, 0x02}, result);
     @endcode
     *
     * - Set auto display pane brightness command: 
     * 
     *      Header: 0x02, 0xfe, 0x20
     * 
     *      Type: 0x04
     * 
     *      Data: 0x00-close, 0x01-open.
     * 
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xfe, 0x20, 0x04, 0x00}, result);
     @endcode
     * 
     * - Set display pane brightness level command: 
     * 
     *      Header: 0x02, 0xfe, 0x20
     * 
     *      Type: 0x02 or 0x07
     * 
     *      Data: Range: 0x01-0x07, 0x01-darkest, 0x07 brightest.
     * 
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xfe, 0x20, 0x02, 0x01}, result);
     @endcode
     * 
     * - Get display pane brightness setting command: 
     * 
     *      Header: 0x02, 0xfe, 0x20
     * 
     *      Type: 0x02 or 0x07
     * 
     *      Comment: The result except the header shows auto mode and brightness level. value1-auto mode, value2 brightness level.
     * 
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xfe, 0x20, 0x02}, result);
     @endcode
     * 
     * - Save display pane brightness mode command: 
     * 
     *      Header: 0x02, 0xfe, 0x20
     * 
     *      Type: 0x0f
     * 
     *      Data: 0x00-close, 0x01-open.
     * 
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xfe, 0x20, 0x0f}, result);
     @endcode
     * 
     * - Save display pane brightness level command: 
     * 
     *      Header: 0x02, 0xfe, 0x20
     * 
     *      Type: 0x10
     * 
     *      Data: Range: 0x01-0x07, 0x01-darkest, 0x07 brightest.
     * 
     * @code 
     example:

        std::vector<unsigned char> result;

        bool bOK = hidWriteAndRead({0x02, 0xfe, 0x20, 0x10}, result);
     @endcode
     *
     */
    virtual bool hidWriteAndRead(const std::vector<unsigned char> &command, std::vector<unsigned char> &result) = 0;
    /**
     * @brief Write UVC control command and read result.
     */
    virtual bool uvcWriteAndRead(const std::vector<unsigned char> &command, std::vector<unsigned char> &result) = 0;
    /**
     * @brief Write VSC control command and read result.
     */
    virtual bool vscWriteAndRead(const std::vector<unsigned char> &command, std::vector<unsigned char> &result) = 0;

    /**
     * @brief set enable camera synchronize.
    */
    virtual bool enableSync(bool isEnable) = 0;

    /**
     * @brief Return the serial number of the device.
     */
    virtual std::string id() const = 0;

    virtual ~Device(){}

};


/**
 * \defgroup xv_functions Global functions
 * @{
 */

/**
 * @brief Get xvsdk version.
 */
Version version();

/**
 * @brief Retrieve all the detected XVisio devices.
 * If no device is found after the timeout is reached, the result will be empty.
 * @param timeOut : wait until the timeout is reached or find at least one device.
 * @param stopWaiting : stop scanning when become true.
 * @param desc : Load device according to feature in desc(json string). SDK can choose device feature from desc accoring SN or hardware version. The desc also contains default values of slam algorithm(old SDK is INI file).
 * @return A map with key corresponding to device ID and the value is a #Device.
 */
std::map<std::string,std::shared_ptr<Device>> getDevices(double timeOut = 0., const std::string& desc = "", bool* stopWaiting = nullptr, xv::SlamStartMode slamStartMode = xv::SlamStartMode::Normal, xv::DeviceSupport deviceSupport = xv::DeviceSupport ::ONLYUSB);

/**
 * @brief Change the log level.
 */
void setLogLevel(LogLevel l);

/**
 * @brief Register the callback for hotplug.
 * @return Id of the callback (used to unregister the callback).
 */
int registerPlugEventCallback(const std::function<void (std::shared_ptr<Device> device, PlugEventType type)> &Callback, const std::string& desc = "");
/**
 * @brief Unregister a plug callback.
 */
bool unregisterHotplugCallback( int callbackID );

/**
 * @}
 */


/**
 * \defgroup xv_android_functions Functions for Android
 * @{
 */
/**
 * @brief Retrieve #Device by given descriptor. Only for Android.
 * @param fd : file descriptor opened by android USBManager.
 * @return A #Device.
 */
std::shared_ptr<Device> getDevice(int fd);
/**
 * @brief Retrieve #Device by given descriptor. Only for Android.
 * @param fd : file descriptor opened by android USBManager.
 * @param desc : load device according to feature in desc. SDK can choose device feature from desc accoring SN or version.
 * @return A #Device.
 */
std::shared_ptr<Device> getDevice(int fd, std::string const& desc, xv::SlamStartMode slamStartMode = xv::SlamStartMode::Normal);

/**
 * @brief Tell sdk device has disconnected. Only for Android.
 */
bool detachDevice(int fd );

/**
 * @brief Retrieve default device description.
 */
std::string getDefaultDescription();

/**
 * @brief Get UTC time.
 */
void getUTCTIme(DateTime* utc);

/**
 * @}
 */

/**
 * @brief save pcd image file, now only support Windows and Unix
 * @note The parameter path specifies the path to save the file,
 *       parameter point is the PointCloud data,
 *       parameter precision saves the precision of the data (the default is 8),
 *       parameter removeInvalidPoints specifies whether to remove invalid points (the default is true).
*/
bool savePointCloudToPcd(const std::string &path, std::shared_ptr<PointCloud> const & point, int precision = 8, bool removeInvalidPoints = true);


}

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/share/ros-wrapper/xv_sdk/include/xv_sdk/xv-sdk-ex.h`:

```h
#pragma once

#include <xv-sdk.h>

#include <mutex>
#include <unordered_map>

namespace x {
    class AprilTagDetector;
}

namespace xv {

/**
 * @brief Perspective Camera Model
 */
struct PerspectiveCameraModel {
    /**
     * @brief Image width (in pixel)
     */
    std::uint16_t w;
    /**
     * @brief Image height (in pixel)
     */
    std::uint16_t h;
    /**
     * @brief Focal length in width direction (in pixel)
     */
    double fx;
    /**
     * @brief Focal length in height direction (in pixel)
     */
    double fy;
    /**
     * @brief Optical axis intersection in width direction (in pixel)
     */
    double u0;
    /**
     * @brief Optical axis intersection in height direction (in pixel)
     */
    double v0;
};

/**
 * @brief Special Extended Unified Camera Model
 */
struct SpecialUnifiedCameraModel
{
  /**
   * @brief Image width (in pixel)
   */
  int w;
  /**
   * @brief Image height (in pixel)
   */
  int h;
  /**
   * @brief Focal length in width direction (in pixel)
   */
  double fx;
  /**
   * @brief Focal length in height direction (in pixel)
   */
  double fy;
  /**
   * @brief Optical axis intersection in width direction (in pixel)
   */
  double u0;
  /**
   * @brief Optical axis intersection in height direction (in pixel)
   */
  double v0;
  /**
   * @brief Optical center of distortion in width direction (in pixel)
   */
  double eu;
  /**
   * @brief Optical center of distortion in width direction (in pixel)
   */
  double ev;
  /**
   * @brief Optical center of distortion in height direction (in pixel)
   */
  double alpha;
  /**
   * @brief alpha
   */
  double beta;
  /**
   * @brief beta
   */
};


struct CalibrationEx : public Calibration {
/*
    std::vector<UnifiedCameraModel> ucm; //! List of Unified Camera Model parameters for differents camera resolutions (see UnifiedCameraModel#w and UnifiedCameraModel#h to find the corresponding resolution of the parameter set).
    std::vector<PolynomialDistortionCameraModel> pdcm; //! List of Polynomial Distortion Camera Model parameters for differents camera resolutions (see UnifiedCameraModel#w and UnifiedCameraModel#h to find the corresponding resolution of the parameter set).
*/
    std::vector<SpecialUnifiedCameraModel> seucm; //! List of Polynomial Distortion Camera Model parameters for differents camera resolutions (see UnifiedCameraModel#w and UnifiedCameraModel#h to find the corresponding resolution of the parameter set).
};


/**
 * Features detections and descriptors
 * @tparam F number of Fisheyes
 * @tparam DESC_SIZE number of bytes for a descriptor
 */
template <std::size_t F, std::size_t DESC_SIZE>
struct FisheyeKeyPoints {
    std::int64_t version; //! Field use to help parsing and future evolutions of this struct
    std::int64_t id; //! Unique id given by the edge to this instance
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`), need to activate IMU stream (with SLAM or IMU callback) to have this value.
    std::int64_t edgeTimestampUs = std::numeric_limits<std::int64_t>::min(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    struct Features {
       std::size_t size; //! number of feature detected
       std::shared_ptr<const float> keypoints; //! list of detected keypoints (xy) in image (size x 2 float elements)
       std::shared_ptr<const unsigned char> descriptors; //! list of (DESC_SIZE x size) keypoints descriptors in image
    };
    std::array<Features,F> descriptors; //! usually first is left and second is right, it is the same order fisheyes calibrations

    /**
     * @brief Size of a feature descriptor
     */
    std::size_t descriptorSize() const { return DESC_SIZE; }
};

#if 0
struct FisheyeKeyPointsDescriptor {
    std::int64_t version; //! Field use to help parsing and future evolutions of this struct
    std::int64_t id; //! Unique id given by the edge to this instance
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`), need to activate IMU stream (with SLAM or IMU callback) to have this value.
    std::int64_t edgeTimestampUs = std::numeric_limits<std::int64_t>::min(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    struct Features {
       std::size_t size; //! number of feature detected
       std::shared_ptr<const float> keypoints; //! list of detected keypoints (xy) in image (size x 2 float elements)
       std::shared_ptr<const unsigned char> descriptors; //! list of (DESC_SIZE x size) keypoints descriptors in image
    };
    std::vector<Features> descriptors; //! usually first is left and second is right, it is the same order fisheyes calibrations

    /**
     * @brief Size of a feature descriptor
     */
    std::size_t descriptorSize() const { return 32; }
};
#endif

enum class StereoMode {
    IMAGES = 0, ///< Images only
    IMAGES_AND_DATA, ///< Images and data
    DATA,  ///< Data only
    NO_STREAMS, ///< No stream
};


/**
 * @brief Data from IMU sensor for the XVisio handle.
 */
struct HandleImu {
    std::array<unsigned char,32> raw;

    //enum class Position {Head = 0, Left, Right};
    //enum class DataType {Init = 0, Work};

    //Position position; //!< 3-axis gyrometer values of left handle (in rad/s)
    //DataType dataType; //!< 3-axis gyrometer values of left handle (in rad/s)
    //int status;
    //Vector3d lgyro; //!< 3-axis gyrometer values of left handle (in rad/s)
    //Vector3d rgyro; //!< 3-axis gyrometer values of right handle (in rad/s)
    //Vector3d laccel; //!< 3-axis accelerometer values of left handle (in m/s²)
    //Vector3d raccel; //!< 3-axis accelerometer values of right handle (in m/s²)
    //double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
    //std::uint32_t edgeTimestampUs = std::numeric_limits<std::uint32_t>::min(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
};


/**
 * @brief The class to give access to data provided by the IMU sensor.
 */
class HandleImuSensor : virtual public Stream<HandleImu const &> {
public:

    virtual ~HandleImuSensor(){}
};


/**
 * @brief For senior developer.
 */
class DeviceEx : virtual public Device {

public:

    static std::shared_ptr<DeviceEx> instance; /// FIXME not compatible with multi-device

    /**
     * @brief Get the secondary SLAM component to make edge and mixed slam work at sametime. Default in edge mode.
     */
    virtual std::shared_ptr<Slam> slam2() = 0;

    virtual std::shared_ptr<HandleImuSensor> handleImuSensor() = 0;
    virtual std::shared_ptr<FisheyeCameras> handleFisheyeCameras() = 0;

    /**
     * @brief Start use external IMU.
     */
    virtual void enableImuInput() = 0;
    /**
     * @brief Stop use external IMU.
     */
    virtual void disableImuInput() = 0;
    /**
     * @brief Send external IMU to SDK.
     */
    virtual void pushImu(Imu const& imu, bool with_bias_correction=false, bool with_timestamp_correction=false) = 0;

    enum class StereoInputType {ImageOnly, KeyPointsOnly, Both, None};

    /**
     * @brief Start use external fisheye image and/or key points.
     *
     * only image,only key points,both
     */
    virtual bool enableStereoInput(StereoInputType type) = 0;
    /**
     * @brief Stop use external fisheye image and/or key points.
     */
    virtual bool disableStereoInput() = 0;
    /**
     * @brief Send external fisheye image and/or key points to SDK.
     */
    virtual void pushStereo(FisheyeKeyPoints<2,32> const& stereo) = 0;

    virtual bool setDisplayCalibration(const std::vector<CalibrationEx>&) = 0;
    virtual bool setRgbCalibration(const std::vector<CalibrationEx>&) = 0;
    virtual bool setTofCalibration(const std::vector<CalibrationEx>&) = 0;
    virtual bool getFisheyeCalibration(std::vector<CalibrationEx>&, double& imuFisheyeTimestampOffset) = 0;
    virtual bool setFisheyeCalibration(const std::vector<CalibrationEx>&, double imuFisheyeTimestampOffset) = 0;
    virtual bool setEyetrackingCalibration(const std::vector<CalibrationEx>&) = 0;

    virtual bool setImuOffset( int offset ) = 0;
    virtual bool setImuMode( int mode ) = 0;


    virtual bool setFeMode( StereoMode mode ) = 0;

    virtual bool setEdgePrediction( int prediction ) = 0;

    /**
     * @brief Get the third SLAM component to make edge and mixed slam work at sametime. Default is edge with fusion on host mode.
     */
    virtual std::shared_ptr<Slam> slam3() = 0;

    /**
     * @brief Return the customize flash array.
     */
    virtual std::string getCustomizeFlash48BytesArray1() = 0;

    /**
     * @brief Set the customize flash array.
     */
    virtual bool setCustomizeFlash48BytesArray1(std::string flashArray) = 0;

    virtual void setDeviceOffsetStatus() = 0;


    virtual ~DeviceEx() { }
};

/**
 * @deprecated
 */
struct FisheyeImageKeyPoints {
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`), need to activate IMU stream (with SLAM or IMU callback) to have this value.
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    struct Detections {
        std::vector<Vector2<std::uint16_t>> keypoints; //! list of detected keypoints in image
        std::vector<std::array<std::int16_t,16>> descriptors; //! list of keypoints descriptors in image
        GrayScaleImage image; //! Fisheye image
    };
    std::vector<Detections> detections; //! List of image detections (typically 2, first is left and second image is the right image)
    std::int64_t id;//! Unique id given by the edge to this instance
};

struct TagInfo {
  std::string family;
  int id;
  double size;
};

struct TagPoseInfo : public TagInfo {
  xv::Transform transform;
};

struct TagPose {
    int tagId;
    xv::Transform transform;
    double confidence; // in [0,1]
};

struct TagDetection {
    int tagId;
    std::array<std::array<double,2>,4> corners;
    double confidence; // in [0,1]
};

class AprilTagDetector {
public:
    /**
     * @brief Construct an AprilTag detector
     * @param c multi camera calibration
     * @param f name of the AprilTag family to use (support: "36h11" "25h9" "16h5" and "14h12")
     */
    explicit AprilTagDetector(std::vector<xv::CalibrationEx> const& c, std::string const& f="36h11", bool subpixelic=false);

    /**
     * @brief Construct an AprilTag detector on single view
     * @param c camera intrinsics
     * @param camerPose camera pose
     * @param f name of the AprilTag family to use (support: "36h11" "25h9" "16h5" and "14h12")
     */
    explicit AprilTagDetector(xv::PolynomialDistortionCameraModel const& c, xv::Transform const& camerPose, std::string const& f="36h11", bool subpixelic=false);

    /**
     * @brief Construct an AprilTag detector on single view
     * @param c camera calibration
     * @param camerPose camera pose
     * @param f name of the AprilTag family to use (support: "36h11" "25h9" "16h5" and "14h12")
     */
    explicit AprilTagDetector(xv::UnifiedCameraModel const& c, xv::Transform const& camerPose, std::string const& f="36h11", bool subpixelic=false);
    
    /**
     * @brief Construct an AprilTag detector on single view
     * @param c camera calibration
     * @param camerPose camera pose
     * @param f name of the AprilTag family to use (support: "36h11" "25h9" "16h5" and "14h12")
     */
    explicit AprilTagDetector(xv::SpecialUnifiedCameraModel const& c, xv::Transform const& camerPose, std::string const& f="36h11", bool subpixelic=false);

    /**
     * @brief Construct an AprilTag detector without camera calibration (only 2D detections are available)
     * @param c multi camera calibration
     * @param f name of the AprilTag family to use (support: "36h11" "25h9" "16h5" and "14h12")
     */
    explicit AprilTagDetector(std::string const& f="36h11", bool subpixelic=false);

    /**
     * @brief Detect AprilTags in Fisheye Images and return the poses of the tags
     * @param tagSize : of the side of the april tag to detect (in m)
     * @return vector of pairs with tag id and pose of the tag
     */
    std::vector<TagPose> detect(xv::FisheyeImages const& fe, double tagSize) const;

    /**
     * @brief Detect AprilTags in a grayscale image and return the poses of the tags
     * @param tagSize : of the side of the april tag to detect (in m)
     * @return vector of pairs with tag id and pose of the tag
     */
    std::vector<TagPose> detect(xv::GrayScaleImage const& im, double tagSize) const;

    /**
     * @brief Detect AprilTags in a grayscale image and return the 4 corners of each detected tag
     * @return vector of pairs with tag id and 4 corners of the tag (in pxl)
     */
    std::vector<TagDetection> detect(const xv::GrayScaleImage& fe);

    /**
     * @brief Compute the poses of the tags detections
     * @param detections: tag detections
     * @param tagSize: size of the tag (in m)
     * @return the tag poses
     */
    std::vector<TagPose> detectionsToPoses(const std::vector<TagDetection>& detections, double tagSize);

    /**
     * @brief Detect AprilTags in a multiple cameras system and return the 4 corners of each detected tag
     * @return map of detections by tag id, each vector of tag detection has the size of the number of images
     */
    std::map<int, std::vector<TagDetection> > detect(const xv::FisheyeImages &fe);
    /**
     * @brief Compute the poses of the tags detections for multiple cameras case
     * @param detections: tag detections grouped by tagId, each vector of tag detection has the size of the number of images
     * @param tagSize: size of the tag (in m)
     * @return the tag poses
     */
    std::vector<TagPose> detectionsToPoses(const std::map<int, std::vector<TagDetection> > &detectionsByTagId, double tagSize);

    /**
     * @brief Start a tag detectors
     * @param slam : SLAM to use for localisation of the tag. The detected tags will be in world frame coordinates as defined by the SLAM.
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @param refreshRate : the refresh rate used for the detection (in Hz)
     * @return Id of the started detector.
     */
    static std::string startTagDetector(std::shared_ptr<xv::FisheyeCameras>, std::shared_ptr<Slam> slam, std::string const& tagFamily, double size, double refreshRate);
    /**
     * @brief Start a tag detectors
     * @param slam : SLAM to use for localisation of the tag. The detected tags will be in world frame coordinates as defined by the SLAM.
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @param refreshRate : the refresh rate used for the detection (in Hz)
     * @return Id of the started detector.
     */
    static std::string startTagDetector(std::shared_ptr<xv::ColorCamera>, std::shared_ptr<Slam> slam, std::string const& tagFamily, double size, double refreshRate);

    /**
     * @brief Stop a tag detector.
     * @param detectorId : detector id (see output of #startTagDetector)
     * @return True if succeeded to stop the detector.
     */
    static bool stopTagDetector(std::string const& detectorId);
    /**
     * @brief Get the current localized tag detections in SLAM world frame coordinates.
     *
     * @param detectorId : detector id
     * @return Poses of all the detected tags, even if the tag is not visible, if it was once detected it remains in this output map.
     */
    static std::map<int,xv::Pose> getTagDetections(std::string const& detectorId);


private:
    std::shared_ptr <x::AprilTagDetector> pimpl;
};


class ColorCameraEx : public ColorCamera, public std::enable_shared_from_this<ColorCameraEx> {

public:
    std::shared_ptr<ColorCameraEx> getThis();

    /**
     * @brief Start a tag detectors
     * @param slam : SLAM to use for localisation of the tag. The detected tags will be in world frame coordinates as defined by the SLAM.
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @param refreshRate : the refresh rate used for the detection (in Hz)
     * @return Id of the started detector.
     */
    std::string startTagDetector(std::shared_ptr<Slam> slam, std::string const& tagFamily, double size, double refreshRate);
    /**
     * @brief Stop a tag detector.
     * @param detectorId : detector id (see output of #startTagDetector)
     * @return True if succeeded to stop the detector.
     */
    bool stopTagDetector(std::string const& detectorId);
    /**
     * @brief Get the current localized tag detections in SLAM world frame coordinates.
     *
     * @param detectorId : detector id
     * @return Poses of all the detected tags, even if the tag is not visible, if it was once detected it remains in this output map.
     */
    std::map<int,xv::Pose> getTagDetections(std::string const& detectorId);
};

/**
 * @brief For senior developer.
 */
class FisheyeCamerasEx : public FisheyeCameras, public std::enable_shared_from_this<FisheyeCamerasEx>
{

    std::mutex m_aprilTagDetectorsMtx;
    std::unordered_map<std::string, std::shared_ptr<AprilTagDetector>> m_aprilTagDetectors;
    std::shared_ptr<AprilTagDetector> getDetector(std::string const& tagFamily);

    std::mutex m_lastFisheyeImageMtx;
    xv::FisheyeImages m_lastFisheyeImage;
    int m_lastFisheyeImageCbId = -1;

    class TagDetectorThread;

    std::shared_ptr<FisheyeCamerasEx> getThis();

public:

    virtual int registerKeyPointsCallback(std::function<void (const FisheyeKeyPoints<2,32>&)>) = 0;
    virtual int registerKeyPointsCallback(std::function<void (const FisheyeKeyPoints<4,32>&)>) = 0;
    virtual bool unregisterKeyPointsCallback(int callbackId) = 0;
    virtual bool unregisterKeyPoints4Callback(int callbackId) = 0;

    virtual const std::vector<CalibrationEx>& calibrationEx() = 0;

    virtual xv::FisheyeImages lastImages() = 0;

    /**
     * @brief The ResolutionMode enum
     */
    enum class ResolutionMode {
        LOW = 1, ///< Low resolution (typically QVGA)
        MEDIUM = 2, ///< Medium resolution (typically VGA)
        HIGH = 3 ///< High resolution (typically HD 720)
    };
    virtual bool setResolutionMode(ResolutionMode mode) = 0;

    /**
     * @brief Detect tags in a fisheye image
     * @param img : fisheye image to used for detection
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @return Vector of tag detections, each tag detection is a pair containing the tag id (AprilTag id) and four corners coordinates (in pixel) of the detected tags.
     */
    std::vector<std::pair<int,std::array<Vector2d,4>>> detectTags(xv::GrayScaleImage const& img, std::string const& tagFamily);

    /**
     * @brief Detect tags in the last fisheye images
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @return Vector of tag detections, each tag detection is a pair containing the tag id (AprilTag id) and the 6dof poses (in Fisheyes frame coordinates).
     * The confidence of the pose reflects the confidence of the tag detection.
     */
    std::vector<std::pair<int,xv::Pose>> detectTags(std::string const& tagFamily, double size);
    /**
     * @brief Detect tags in fisheye images and return poses of the tags
     * @param fe : fisheye images to use for detection
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @return Vector of tag detections, each tag detection is a pair containing the tag id (AprilTag id) and the 6dof poses (in Fisheyes frame coordinates).
     * The confidence of the pose reflects the confidence of the tag detection.
     */
    std::vector<std::pair<int,xv::Pose>> detectTags(xv::FisheyeImages const& fe, std::string const& tagFamily, double size);
    /**
     * @brief Detect tags in the last fisheye images and return poses of the tags in SLAM world frame coordinates
     * @param slam : the slam to use for localization of detection in SLAM world frame coordinates
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @return Vector of tag detections, each tag detection is a pair containing the tag id (AprilTag id) and the 6dof poses (in Fisheyes frame coordinates).
     * The confidence of the pose reflects the confidence of the tag detection.
     */
    std::vector<std::pair<int,xv::Pose>> detectTags(std::shared_ptr<Slam> slam, std::string const& tagFamily, double size);

    /**
     * @brief Start a tag detectors
     * @param slam : SLAM to use for localisation of the tag. The detected tags will be in world frame coordinates as defined by the SLAM.
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @param refreshRate : the refresh rate used for the detection (in Hz)
     * @return Id of the started detector.
     */
    std::string startTagDetector(std::shared_ptr<Slam> slam, std::string const& tagFamily, double size, double refreshRate);
    /**
     * @brief Stop a tag detector.
     * @param detectorId : detector id (see output of #startTagDetector)
     * @return True if succeeded to stop the detector.
     */
    bool stopTagDetector(std::string const& detectorId);
    /**
     * @brief Get the current localized tag detections in SLAM world frame coordinates.
     *
     * @param detectorId : detector id
     * @return Poses of all the detected tags, even if the tag is not visible, if it was once detected it remains in this output map.
     */
    std::map<int,xv::Pose> getTagDetections(std::string const& detectorId);

    virtual ~FisheyeCamerasEx() { }

};

/**
 * @brief Compute the pixel shift to go from tracker pose p0 to tracker pose p1
 *
 * If p0 and p1 are at the same translation (rotation onlty) this function can be used for ATW and the parameter d is useless. If p0 and p1 are with different translation the pixel shift correspond
 * to a virtual object at d meter from the display.
 *
 * @param p0 : tracker pose
 * @param p1 : tracker pose (need to be in same frame coordinate as p)1
 * @param displayExtrinsics : display pose in tracker pose
 * @param displayCalib : display intrinsics
 * @param d : distance of the virtual object to use to estimate the pixel shift
 * @return (x,y) pixel shift corresponding to the motion from p0 to p1
 */
xv::Vector2d getPixelShift(xv::Pose const& p0, xv::Pose const& p1, xv::Transform const& displayExtrinsics, xv::CameraModel const& displayCalib, double d=1.);

class CameraEx : public Camera {
public:
    virtual const std::vector<CalibrationEx>& calibrationEx() = 0;
    virtual std::shared_ptr<CameraModel> cameraModel() { return nullptr; }
    virtual std::vector<std::shared_ptr<CameraModel>> camerasModel() {
        if (cameraModel())
            return {cameraModel()};
        else
            return {};
    }
};

class DisplayEx : public Display {
public:
    virtual const std::vector<CalibrationEx>& calibrationEx() = 0;
    virtual std::vector<std::shared_ptr<CameraModel>> camerasModel() {
            return {};
    }
};

namespace ex {

struct PointCloud
{
  std::int32_t version;
  std::uint64_t id;
  std::uint32_t xyznSize;
  std::shared_ptr<const std::array<float,6>> xyzn; //!< oriented point cloud in sensor frame
  Transform sensorPose;                            //!< sensor pose in world
};

struct PointClouds {
    std::map<std::uint64_t, xv::ex::PointCloud> pointClouds;
};

struct Surface
{
  std::int32_t version;
  std::uint64_t id;

  std::uint32_t verticesSize;
  std::shared_ptr<const std::array<float,3>> vertices;
  std::shared_ptr<const std::array<float,3>> vertexNormals;

  std::uint32_t trianglesSize;
  std::shared_ptr<const std::array<std::uint32_t,3>> triangles;

  std::shared_ptr<const std::array<float,2>> textureCoordinates; //!< one per vertex
  std::uint32_t textureWidth;
  std::uint32_t textureHeight;
  std::shared_ptr<const std::uint8_t> textureRgba;    //!< row major
};

struct Surfaces {
    std::map<std::uint64_t, xv::ex::Surface> surfaces;
};

}

class SlamEx : public Slam {

protected:

    bool m_enableOnlineLoopClosure = false;
    bool m_enableSurface = false;
    bool m_enableSurfaceTexturing = false;
    bool m_enableSurfaceMultiResolutionMesh = false;
    bool m_enableSurfaceMobileObjects = false;
    bool m_enableSurfacePlanes = false;
    bool m_surfaceUseFisheyes = false; //!< instead of Tof as depth source
    bool m_surfaceUseFisheyeTexturing = true; //!< fisheye texturing instead of RGB texturing
    double m_surfaceMinVoxelSize = 0.1;

public:

    virtual void setEnableOnlineLoopClosure(bool enable) { m_enableOnlineLoopClosure = enable; }
    virtual void setEnableSurface(bool enable) { m_enableSurface = enable; }
    virtual void setEnableSurfaceTexturing(bool enable) { m_enableSurfaceTexturing = enable; }
    virtual void setEnableSurfaceMultiResolutionMesh(bool enable) { m_enableSurfaceMultiResolutionMesh = enable;}
    virtual void setEnableSurfaceMobileObjects(bool enable) { m_enableSurfaceMobileObjects = enable;}
    virtual void setEnableSurfacePlanes(bool enable) { m_enableSurfacePlanes = enable;}
    virtual void setSurfaceUseFisheyes(bool use) { m_surfaceUseFisheyes = use;}
    virtual void setSurfaceUseFisheyeTexturing(bool use) { m_surfaceUseFisheyeTexturing = use;}
    virtual void setSurfaceMinVoxelSize(double size) { m_surfaceMinVoxelSize = size; }
    virtual int registerLocal3dPointsCallback(std::function<void (std::shared_ptr<const std::vector<std::array<double,3>>>)>) = 0;
    virtual bool unregister3dPointsCallback(int callbackId) = 0;
    virtual bool getLastVSlamPose(Pose &) { return false; }

    virtual bool startSurfaceReconstruction() { return false; }
    virtual bool stopSurfaceReconstruction() { return false; }
    virtual bool startPlaneDetection() { return false; }
    virtual bool stopPlaneDetection() { return false; }

    /**
     * @brief Define a map of tags poses
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param tagSize : size in m of the real tag side
     * @param tagIds : AprilTag id of each tag
     * @param poses : poses of the AprilTag in a world frame coordinate. This world frame coordinates will then be used
     */
    virtual void setTagsMap(std::string const& tagFamily, double tagSize, std::vector<int> tagIds, std::vector<xv::Transform> poses) = 0;

    /**
     * @brief Get the current 6dof pose of the device in tags map
     *
     * Same as #getPose but return true even if it can localize on the map defined by #setTagsMap. The output pose is relative to this tags map thanks to the tags detection.
     *
     * @param[out] pose corresponding to the timestamp "now" + "prediction"
     * @param[in] prediction (in s) amount of prediction to use to get a pose corresponding to the future
     * @return true if localized on tag map, false else.
     */
    virtual bool getPoseInTagsMap(Pose& pose, double prediction = 0.) = 0;

    /**
    * @brief Callback to get the reconstructed pointcloud (is computed faster than surface reconstruction but is not a mesh).
    * @return Id of the callback (used to unregister the callback).
    */
    virtual int registerPointCloudCallback(std::function<void (std::shared_ptr<const ex::PointClouds>)>) = 0;
    virtual bool unregisterPointCloudCallback(int callbackId) = 0;
    virtual bool getPointCloud(std::shared_ptr<const ex::PointClouds>&) = 0;

    /**
     * @brief Callback to get the reconstructed surface updates.
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerSurfaceCallback(std::function<void (std::shared_ptr<const xv::ex::Surfaces>)>) = 0;
    virtual bool unregisterSurfaceCallback(int callbackId) = 0;
    virtual bool getSurface(std::shared_ptr<const xv::ex::Surfaces>&) = 0;

    /**
     * @brief Set the tag tos use in SLAM; must be done before calling slam->start()
     * @param v : list of tags
     * @return success
     */
    virtual bool addTags(std::vector<TagInfo> const& v) = 0;

    /**
     * @brief Get the tags used in the SLAM
     * @param tagId : unique tag identifier
     * @param pose : pose of the tag in the SLAM coordinate frame
     * @param tagSize : size of the tag
     * @return success
     */
    virtual bool onTagUpdate(std::function<void(std::string const& tagId, xv::Transform const& pose, double const& tagSize)>) = 0;
};

/**
 * @brief To compute the 3D position of 2D pixel point of RGB image.
 *
 * It uses raytrace of RGB pixel and ToF image to get the depth. If SLAM is running, then the output 3D position is in World frame coordinate of the SLAM,
 * else it is relative to the IMU frame coordinate.
 */
class RgbPixelPoseWithTof {

public:
    RgbPixelPoseWithTof(std::shared_ptr<xv::Device> d);

    /**
     * @brief Get the position of the pointed area in RGB image, it uses ToF to determine the depth.
     *
     * If SLAM is running, then the output pointerPose is in World frame coordinate of the SLAM, else it is relative to the IMU frame coordinate.
     *
     * @param pointerPose: 3D position in World frame coordinate of the point selected in RGB
     * @param hostTimestamp: timestamp corresponding to the pointing
     * @param rgbPixelPoint: xy position of the point in color image (in pixel)
     * @param radius: size of the area to select for ToF 3D points selection
     * @return true if succes, false else
     */
    bool getRgbPixel3dPoseAt(xv::Vector3d& pointerPose, double hostTimestamp, xv::Vector2d const& rgbPixelPoint, double radius);

private:
    class Impl;
    std::unique_ptr<Impl> pImpl;

public:
    ~RgbPixelPoseWithTof();
};

/**
* eye data function pointer
*/
typedef void (* xv_ET_point_process_callback)(int index, int percent, void* context);//!<Callback function used to receive calibration progress of calibration point.
typedef void (* xv_ET_point_finish_callback)(int index, int error, void* context);//!<Callback function used to receive the completion status of calibration point.

/**
 * @brief A class to handle interfaces of the gaze calibration operations.
 */
class GazeCalibration{
public:

    /**
     * @brief Start calibration
     *
     * @param[in] points Total number of calibration points.
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
	 * -212 or -7001 Parameter error
     */
    int StartCalibration(int points);

    /**
     * @brief Start calibrating a point
     *
     * @param[in] eye Calibrate the left and right eyes, 1-left eye and 2-right eye. Note that the left and right eyes are not calibrated together and must be calibrated separately.
     * @param[in] index Calibration point index.
     * @param[in] point Datum coordinates of calibration points (normalized values are used for X and y).
     * @param[in] cb1 The callback when this point is calibrated.
     * @param[in] context1 The callback corresponding to CB1 is used to pass the context of the caller, can be empty.
     * @param[in] cb2 The callback when eyeball information of each image is recalled.
     * @param[in] context2 The callback corresponding to CB2 is used to pass the context of the caller, can be empty.
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
	 * -221 Eye is not set.
     * -221222 Parameter error, start calibration index point error.
     */
    int StartCalibrationPoints(int eye, int index, const xv::XV_ET_POINT_2D* point, 
        xv::xv_ET_point_process_callback cb1, void* context1, 
        xv::xv_ET_point_finish_callback cb2, void* context2);

    /**
     * @brief Cancel calibration
     *
     * @param[in] eye Calibrate the left and right eyes, 1-left eye and 2-right eye. Note that the left and right eyes are not calibrated together and must be calibrated separately.
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
	 * -241 Eye is not set.
     */
    int CancelCalibration(int eye);

    /**
     * @brief Compute calibration
     *
     * @param[in] eye Calibrate the left and right eyes, 1-left eye and 2-right eye. Note that the left and right eyes are not calibrated together and must be calibrated separately.
     * @param[out] out_coe Calibration coefficient.
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
     * -251 Eye is not set.
     * -253 Calibration coefficient parameter is null.
	 * -251252 Parameter error
     */
    int ComputeCalibration(int eye, xv::XV_ET_COEFFICIENT* out_coe);

    /**
     * @brief Complete calibration
     *
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
     */
    int CompleteCalibration();

    /**
     * @brief Set the calibration range and default calibration factor.
     *
     * @param[in] eye Calibrate the left and right eyes, 1-left eye and 2-right eye. Note that the left and right eyes are not calibrated together and must be calibrated separately.
     * @param[in] minX Minimum value of X coordinate system.
     * @param[in] maxX Maximum value of X coordinate system.
     * @param[in] minY Minimum value of Y coordinate system.
     * @param[in] maxY Maximum value of Y coordinate system.
     * @param[in] coe Default coefficient, can be null.
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
     * -3 Wrong sdk version.
     * -271 Eye is not set.
	 * -272 Parameter error
     */
    int SetDefaultCalibration(int eye, float minX, float maxX, float minY, float maxY, const xv::XV_ET_COEFFICIENT* coe);

    /**
     * @brief Input camera image
     *
     * @param[in] image Input image.
     * @param[in] size Image size.
     * @param[in] width Image width.
     * @param[in] height Image height.
     * @param[in] timestamp Image timestamp.
     * @return int 
	 *  0 success
     */
    int InputCameraImage(const unsigned char* image, int size, int width, int height, long long timestamp) ;
};

/**
 * @brief Obtain a virtual #Device, and input sensor data to the sdk from caller.
 * @return A #Device.
 */
std::shared_ptr<Device> getVirtualDevice();

}

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/share/ros-wrapper/xv_sdk/include/xv_sdk/xv-sdk-private.h`:

```h
#pragma once

#include "xv-sdk-ex.h"
#include <stdexcept>

#ifdef WIN32
#ifdef DLL_EXPORTS
#define DLL_API __declspec(dllexport)
#else
#define DLL_API __declspec(dllimport)
#endif
#else
#define DLL_API
#endif

namespace xv {

/**
 * \defgroup xv_rotation_utils functions to help manipulation of rotations
 */
/**
 * @brief Linear extrapolation of a rotation matrix.
 * @param input The rotation to extrapolate in the future
 * @param angularVelocity The angular velocity (rad/s) for the linear prediction.
 * @param prediction Amount (in s) of prediction.
 * @return The extrapolated (predicted) rotation.
 */
Matrix3f rotPrediction(Matrix3f const& input, Vector3f const& angularVelocity, double prediction);

/**
 * @brief Linear extrapolation of a rotation matrix.
 * @param input The rotation to extrapolate in the future
 * @param angularVelocity The angular velocity (rad/s) for the linear prediction.
 * @param prediction Amount (in s) of prediction.
 * @return The extrapolated (predicted) rotation.
 */
Matrix3d rotPrediction(Matrix3d const& input, Vector3d const& angularVelocity, double prediction);

/**
 * @brief 2nd order extrapolation of a rotation matrix.
 * @param input The rotation to extrapolate in the future
 * @param angularVelocity The angular velocity (rad/s) for the extrapolation.
 * @param angularAcceleration The angular acceleration (rad/s/s) for the extrapolation.
 * @param prediction Amount (in s) of prediction.
 * @return The extrapolated (predicted) rotation.
 */
Matrix3d rotPrediction(Matrix3f const& input, Vector3f const& angularVelocity, Vector3f const& angularAcceleration, double prediction);

/**
 * @brief 2nd order extrapolation of a rotation matrix.
 * @param input The rotation to extrapolate in the future
 * @param angularVelocity The angular velocity (rad/s) for the extrapolation.
 * @param angularAcceleration The angular acceleration (rad/s/s) for the extrapolation.
 * @param prediction Amount (in s) of prediction.
 * @return The extrapolated (predicted) rotation.
 */
Matrix3d rotPrediction(Matrix3d const& input, Vector3d const& angularVelocity, Vector3d const& angularAcceleration, double prediction);

/**
 * @brief Return the linear interpolation of two rotations matrices.
 * @param a The first rotation matrix (t=0)
 * @param b The second rotation matrix (t=1)
 * @param t Ratio to select where is the interpolation between a (t=0) and b (t=1). t=0.5 means the average between a and b.
 * @return The interpolated rotation matrix.
 */
Matrix3d rotLinearInterpolation(const Matrix3d &a, const Matrix3d &b, double t);

/**
 * @brief Return the linear interpolation of two rotations matrices.
 * @param a The first rotation matrix (t=0)
 * @param b The second rotation matrix (t=1)
 * @param t Ratio to select where is the interpolation between a (t=0) and b (t=1). t=0.5 means the average between a and b.
 * @return The interpolated rotation matrix.
 */
Matrix3f rotLinearInterpolation(const Matrix3f &a, const Matrix3f &b, double t);

/** @}
 *
*/

class DeviceImpl;
class DeviceDriver;
class TimeServer;
struct CallbackMaps;



/**
 * @brief Base class of slam implemnetations (edge, mixed, edge fusion on host ...)
 * Not in public API because public API use modes to switch between implementations. Implements almost all of xv::Slam except features related to modes.
 */
class SlamBase {

    std::unique_ptr<CallbackMaps> m_callbackMaps;

protected:
    const std::shared_ptr<DeviceDriver> m_deviceDriver;
    std::shared_ptr<TimeServer> m_timeServer;

public:

    explicit SlamBase(std::shared_ptr<DeviceDriver> d);

    CallbackMaps& callbackMaps();


    virtual bool start() {throw std::runtime_error("Invalid call, not implemented.");}
    virtual bool stop() {throw std::runtime_error("Invalid call, not implemented.");}
    virtual bool reset() {throw std::runtime_error("Invalid call, not implemented.");}
    virtual bool getPose(Pose &, double );
    virtual bool getPoseAt(Pose &, double );


    virtual int registerCallback(std::function<void (const Pose &)> cb);
    virtual bool unregisterCallback(int cb);
    virtual int registerVisualPoseCallback(std::function<void (const Pose &)> cb);
    virtual bool unregisterVisualPoseCallback(int cb);
    virtual int registerLostCallback(std::function<void ()>);
    virtual bool unregisterLostCallback(int );

    virtual int registerStereoPlanesCallback(std::function<void (std::shared_ptr<const std::vector<Plane> >)> );
    virtual bool unregisterStereoPlanesCallback(int );
    virtual int registerTofPlanesCallback(std::function<void (std::shared_ptr<const std::vector<Plane> >)> );
    virtual bool unregisterTofPlanesCallback(int );
    virtual bool clearStereoPlanes();
    virtual bool clearTofPlanes();

    virtual int registerMapCallback(std::function<void (std::shared_ptr<const SlamMap>)> );
    virtual bool unregisterMapCallback(int );

    virtual int registerLocal3dPointsCallback(std::function<void (std::shared_ptr<const std::vector<std::array<double,3>>>)>);
    virtual bool unregister3dPointsCallback(int);

    virtual bool startCslam(std::streambuf &, std::function<void (float)> ) {throw std::runtime_error("Invalid call, not implemented.");}
    virtual bool loadMapAndSwitchToCslam(std::streambuf &, std::function<void (int)> , std::function<void (float)> ) {throw std::runtime_error("Invalid call, not implemented.");}
    virtual bool saveMapAndSwitchToCslam(std::streambuf &, std::function<void (int, int)> , std::function<void (float)> ) {throw std::runtime_error("Invalid call, not implemented.");}
    virtual bool switchToCSlam(std::function<void (int)> , std::function<void (float)> ) {throw std::runtime_error("Invalid call, not implemented.");}
    virtual bool saveMap(std::streambuf &, bool ) {throw std::runtime_error("Invalid call, not implemented.");}

    virtual bool getLastVSlamPose(Pose &) {throw std::runtime_error("Invalid call, not implemented.");}

    virtual bool getPointCloud(std::shared_ptr<const ex::PointClouds>&) {throw std::runtime_error("Invalid call, not implemented.");}
    virtual int registerPointCloudCallback(std::function<void (std::shared_ptr<const ex::PointClouds>)> );
    virtual bool unregisterPointCloudCallback(int );

    virtual bool getSurface(std::shared_ptr<const ex::Surfaces>&) {throw std::runtime_error("Invalid call, not implemented.");}
    virtual int registerSurfaceCallback(std::function<void (std::shared_ptr<const ex::Surfaces>)> );
    virtual bool unregisterSurfaceCallback(int );

    virtual bool startSurfaceReconstruction();
    virtual bool stopSurfaceReconstruction();
    virtual bool startPlaneDetection();
    virtual bool stopPlaneDetection();

    virtual bool addTags(std::vector<TagInfo> const& v);
    virtual bool onTagUpdate(std::function<void(std::string const& tagId, xv::Transform const& pose, double const& tagSize)>);

    virtual ~SlamBase();

};

class ImuSensorCalibration {

    // gyro corrected = m_gyroScaleInv * (gyro - gyroBias)
    // accel corrected = m_accelScaleInv * (gyro - accelBias)

    double m_temperature; // Temperature of the calibration parameters (in K)
    xv::Vector3d m_gyroBias; // gyrometer bias (in bits or rad/s)
    xv::Vector3d m_accelBias; // accelerometer bias (in bits or g)
    xv::Matrix3d m_gyroScaleInv;
    xv::Matrix3d m_accelScaleInv;

private:

    void apply(xv::Vector3d &x, xv::Vector3d bias, xv::Matrix3d scaleInv) {
        Vector3d xx = x;
        for (std::size_t i=0; i<3; ++i)
            xx[i] -= bias[i];
        for (std::size_t i=0; i<3; ++i)
            x[i] = scaleInv[0+i*3]*xx[0] + scaleInv[1+i*3]*xx[1] + scaleInv[2+i*3]*xx[2];
    }

public:

    ImuSensorCalibration() {
        setGyroScale({1.,1.,1.});
        setAccelScale({1.,1.,1.});
    }
    ImuSensorCalibration(double temperature, xv::Vector3d const& gyroBias, xv::Vector3d const& accelBias, xv::Vector3d const& gyroScale={1.,1,1}, xv::Vector3d const& accelScale={1.,1,1})
        : m_temperature(temperature), m_gyroBias(gyroBias), m_accelBias(accelBias) {
        setGyroScale(gyroScale);
        setAccelScale(accelScale);
    }

    double temperature() const {return m_temperature;}
    void setTemperature(double t) { m_temperature = t; }

    xv::Vector3d const& gyroBias() const { return m_gyroBias; }
    void setGyroBias(xv::Vector3d const& b) { m_gyroBias = b; }
    xv::Matrix3d const& gyroScaleInv() const { return m_gyroScaleInv; }
    void setGyroScale(double scale) {
        setGyroScale({scale,scale,scale});
    }
    void setGyroScale(xv::Vector3d scale) {
        m_gyroScaleInv = {
            1./scale[0], 0, 0,
            0, 1./scale[1], 0,
            0, 0, 1./scale[2]
        };
    }
    void setGyroScaleInv(xv::Matrix3d scaleInv) {
        m_gyroScaleInv = scaleInv;
    }

    xv::Vector3d const& accelBias() const { return m_accelBias; }
    void setAccelBias(xv::Vector3d const& b) { m_accelBias = b; }
    xv::Matrix3d const& accelScaleInv() const { return m_accelScaleInv; }
    void setAccelScale(double scale) {
        setAccelScale({scale,scale,scale});
    }
    void setAccelScale(xv::Vector3d scale) {
        m_accelScaleInv = {
            1./scale[0], 0, 0,
            0, 1./scale[1], 0,
            0, 0, 1./scale[2]
        };
    }
    void setAccelScaleInv(xv::Matrix3d scaleInv) {
        m_accelScaleInv = scaleInv;
    }

    void apply(Imu& imu) {
        apply(imu.gyro, m_gyroBias, m_gyroScaleInv);
        apply(imu.accel, m_accelBias, m_accelScaleInv);
    }

};

class DevicePrivate : virtual public DeviceEx {

private :

   std::shared_ptr<SlamBase> m_slamVo;
   std::shared_ptr<SlamBase> m_slamHostOnly;
   std::shared_ptr<SlamBase> m_slamEdgeLocHostMap;

public:

   static bool s_slamVisionOnlyEnabled;
   static bool s_slamHostOnlyEnabled;
   static bool s_slamEnableEdgeLocHostMapping; // if enabled, the EdgeFusionOnHost will do the loc on edge and mapping on host,

   struct SlamVisionOnlyParams {
       bool m_useSmoothFilter;
       double m_cutSpeedSmoothFilter;
       double m_cutAngularSpeedSmoothFilter;
       SlamVisionOnlyParams() : m_useSmoothFilter(true), m_cutSpeedSmoothFilter(0.5),m_cutAngularSpeedSmoothFilter(0.5)  {}
   };

   bool initSlamVisionOnly(std::shared_ptr<DeviceImpl> d, bool enableOnlineLoopClosure=false);
   void setSlamVisionOnlyParams(SlamVisionOnlyParams const&p);

   // setXXXCalibration : only set the current calibration
   // writeXXXCalibration : set the current calibration and write the calibration on device

   virtual bool getImuCalibration(ImuSensorCalibration&) {return false; }
   virtual bool setImuCalibration(const ImuSensorCalibration&) {return false;}
   virtual bool writeImuCalibration(const ImuSensorCalibration&) {return false;}

   virtual bool writeDisplayCalibration(const std::vector<CalibrationEx>&) { return false; }
   
   virtual bool writeRgbCalibration(const std::vector<CalibrationEx>&) { return false; }
   virtual bool writeTofCalibration(const std::vector<CalibrationEx>&) { return false; }

   virtual bool setTofIrEnabled(bool enabled) {return false; }

   virtual bool writeFisheyeCalibration(const std::vector<CalibrationEx>&, double imuFisheyeTimestampOffset) { return false; }
   virtual double getImuFisheyeTimestampOffset() { return 0.; }

   virtual bool writeEyetrackingCalibration(const std::vector<Calibration>&) { return false; }

   /**
    * @brief Provide a SLAM without IMU data.
    */
   std::shared_ptr<SlamBase> slamVisionOnly();

   bool initSlamHostOnly(std::shared_ptr<DeviceImpl> d, bool enableOnlineLoopClosure=false, bool enableSurface=false, bool enableSurfaceTexturing=false, bool surfaceMultiResolutionMesh=false, bool surfaceMobileObjects=false, bool enableSurfacePlanes=false, bool surfaceUseFisheyes=false, bool surfaceUseFisheyeTexturing=false);

   /**
    * @brief Provide a SLAM only on host (only use IMU and fisheye images from the device).
    */
   std::shared_ptr<SlamBase> slamHostOnly();

   /**
    * @brief Provide a SLAM with localization on edge and mapping on host.
    */
   std::shared_ptr<SlamBase> slamEdgeLocHostMap();


   virtual ~DevicePrivate();
};

/**
 * @brief A mesh to find the new position of pixel of a Warp used for undistortion or rectification for stereo alignement
 */
class ImageWarpMesh {
    std::uint16_t w;
    std::uint16_t h;
    std::vector<xv::Vector2f> map;
    std::vector<std::array<std::int32_t,4>> bilinearInterpolateCache;
    std::vector<std::array<float,4>> bilinearInterpolateFloatCache;
public:
    ImageWarpMesh();
    ImageWarpMesh(std::uint16_t w, std::uint16_t h);
    xv::Vector2f const& pixel(int x, int y) const;
    xv::Vector2f& pixel(int x, int y);
    std::uint16_t width() const;
    std::uint16_t height() const;
    void initBilinearInterpolate(std::uint16_t w, std::uint16_t h);
    /**
     * @brief This unwarp function is optimize for full image unwarp and need to call #initBilinearInterpolate before
     * @param inputImage the image to unwarp
     * @return the unwarped image
     */
    GrayScaleImage unwarp(const GrayScaleImage &inputImage) const;
    /**
     * @brief This unwarp function is optimize for full image unwarp and need to call #initBilinearInterpolate before
     * @param inputImage the image to unwarp
     * @return the unwarped image
     */
    RgbImage unwarp(const RgbImage &inputImage) const;
};

/**
 * @brief Create an undistor image warp from camera calibrations and image size.
 *
 * The new camera model corresponding to the warp is camera perspective with u0,v0,fx,fy from original parameters
 *
 * @param cameraCalibrations : calibration of the camera that gives the input image of the warp
 * @param w : width of the input images
 * @param h : height of the input image
 * @return The image warp that undistor the images
 */
std::pair<ImageWarpMesh,PerspectiveCameraModel> createImageWarpMeshWithPdcm(std::vector<Calibration> cameraCalibrations, uint16_t w, uint16_t h, double zoomFactor=1.);

/**
 * @brief Create an undistor image warp from camera calibrations and image size.
 *
 * The new camera model corresponding to the warp is camera perspective with u0,v0,fx,fy from original parameters
 *
 * @param cameraCalibrations : calibration of the camera that gives the input image of the warp
 * @param w : width of the input images
 * @param h : height of the input image
 * @return The image warp that undistor the images
 */
std::pair<ImageWarpMesh,PerspectiveCameraModel> createImageWarpMeshWithUcm(std::vector<Calibration> cameraCalibrations, uint16_t w, uint16_t h, double zoomFactor=1.);

/**
 * @brief A class to compute the rectification or undistortion mesh to help undistor stereo images.
 */
class StereoRectificationMesh {
    ImageWarpMesh left;
    ImageWarpMesh right;

public:
    StereoRectificationMesh(std::vector<xv::Calibration> const& calib, bool useUcm=true);
    StereoRectificationMesh(std::vector<xv::Calibration> const& calib, double focal, double baseline);
    StereoRectificationMesh(std::vector<xv::Calibration> const& calib, std::vector<xv::Calibration> const& displayCalib);

    std::pair<xv::GrayScaleImage, xv::GrayScaleImage> rectify(const xv::GrayScaleImage &img_l, const xv::GrayScaleImage &img_r) const;

    double disparityToDepth(double disparity) const;

    static std::pair<xv::GrayScaleImage,xv::GrayScaleImage> applyMesh(xv::GrayScaleImage const& img_l, xv::GrayScaleImage const& img_r, ImageWarpMesh const& leftMesh, ImageWarpMesh const& rightMesh);
    static std::pair<ImageWarpMesh, ImageWarpMesh> initInverse(const std::vector<xv::Calibration> &calib);

    double focal() const;
    double baseline() const;

    xv::Transform const& leftVirtualPose() const;
    xv::Transform const& rightVirtualPose() const;

    ImageWarpMesh const& leftWarp() const;
    ImageWarpMesh const& rightWarp() const;

private:
    double m_baseline = -1e9;
    double m_focal = -1e9;
    xv::Transform m_P1;
    xv::Transform m_P2;
    void init(std::vector<xv::Calibration> const& calib);
    void init(const std::vector<xv::Calibration> &calib, const std::vector<xv::Calibration> &displayCalib);
    void initPdcm(std::vector<xv::Calibration> const& calib);
};

}

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/share/ros2-wrapper/xv_sdk_ros2/include/xv_ros2_node.h`:

```h
#ifndef __XV_ROS2_NODE_H__
#define __XV_ROS2_NODE_H__
#include <chrono>
#include <fstream>
#include <functional>
#include <memory>
#include <sstream>
#include <string>
#include <thread>
#include <xv-sdk.h>

#include "rclcpp/rclcpp.hpp"
#include "sensor_msgs/msg/imu.hpp"
#include "std_msgs/msg/string.hpp"
#include "std_srvs/srv/trigger.hpp"
#include "nav_msgs/msg/path.hpp"
#include <tf2/LinearMath/Quaternion.h>
#include <tf2_ros/transform_broadcaster.h>
#include <tf2_ros/static_transform_broadcaster.h>
#include "xv_ros2_msgs/msg/orientation_stamped.hpp"
#include "xv_ros2_msgs/srv/get_orientation.hpp"
#include "xv_ros2_msgs/srv/get_orientation_at.hpp"
#include "xv_ros2_msgs/srv/get_pose.hpp"
#include "xv_ros2_msgs/srv/get_pose_at.hpp"
#include <image_transport/image_transport.h>
#include "xv_dev_wrapper.h"
using namespace xv;
using rosImage = sensor_msgs::msg::Image;
using rosCamInfo = sensor_msgs::msg::CameraInfo;
class xvision_ros2_node : public rclcpp::Node {
public:
    xvision_ros2_node(void);
    void init(void);
    void initTopicAndServer(void);
    void initFrameIDParameter(void);
    void get_frameId_parameters(void);
    void get_device_config_parameters(void);
    void printInfoMsg(const std::string msgString) const;
    void printErrorMsg(const std::string msgString) const;
    void publishImu(const sensor_msgs::msg::Imu &imuMsg);
    void publisheOrientation(const xv_ros2_msgs::msg::OrientationStamped& orientationMsg);
    void publishFEImage(const rosImage& image, const rosCamInfo& cameraInfo, xv_dev_wrapper::FE_IMAGE_TYPE imageType);
    void publishFEAntiDistortionImage(const rosImage& image, const rosCamInfo& cameraInfo, xv_dev_wrapper::FE_IMAGE_TYPE imageType);
    void publishSlamPose(const geometry_msgs::msg::PoseStamped& pose);
    void publishSlamTrajectory(const nav_msgs::msg::Path& path);
    void publishTofCameraImage(const sensor_msgs::msg::Image& image,const sensor_msgs::msg::CameraInfo& cameraInfo);
    void publishRGBCameraImage(const sensor_msgs::msg::Image& image,const sensor_msgs::msg::CameraInfo& cameraInfo);
    void publishSGBMImage(const sensor_msgs::msg::Image& image,const sensor_msgs::msg::CameraInfo& cameraInfo);
    void publishSGBMRawImage(const sensor_msgs::msg::Image& image,const sensor_msgs::msg::CameraInfo& cameraInfo);
    std::string getFrameID(const std::string &defaultId);
    bool getConfig(std::string configName);
    void broadcasterTfTransform(const geometry_msgs::msg::TransformStamped & transform);

private:
    void watchDevices(void);

private:
    // imu publisher and service.
    rclcpp::Publisher<sensor_msgs::msg::Imu>::SharedPtr m_imuPublisher;
    
    // orientationStream publisher and service.
    rclcpp::Publisher<xv_ros2_msgs::msg::OrientationStamped>::SharedPtr m_orientationPublisher;
    rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr m_service_imuSensor_startOri;
    rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr m_service_imuSensor_stopOri;
    rclcpp::Service<xv_ros2_msgs::srv::GetOrientation>::SharedPtr m_service_imuSensor_getOri;
    rclcpp::Service<xv_ros2_msgs::srv::GetOrientationAt>::SharedPtr m_service_imuSensor_getOriAt;
    
    // FE publisher and service.
    image_transport::Publisher  m_fisheyeImagePublisher[2];
    image_transport::Publisher  m_fisheyeAntiDistortionImagePublisher[2];
    //rclcpp::Publisher<sensor_msgs::msg::Image>::SharedPtr m_fisheyeImagePublisher[2];
    rclcpp::Publisher<sensor_msgs::msg::CameraInfo>::SharedPtr m_fisheyeCamInfoPublisher[2];
    rclcpp::Publisher<sensor_msgs::msg::CameraInfo>::SharedPtr m_fisheyeAntiDistortionCamInfoPublisher[2];

    //slam publisher and service.
    rclcpp::Publisher<geometry_msgs::msg::PoseStamped>::SharedPtr m_slam_pose_publisher;
    rclcpp::Publisher<nav_msgs::msg::Path>::SharedPtr m_slam_path_publisher;
    rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr m_service_slam_start;
    rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr m_service_slam_stop;
    rclcpp::Service<xv_ros2_msgs::srv::GetPose>::SharedPtr m_service_slam_get_pose;
    rclcpp::Service<xv_ros2_msgs::srv::GetPoseAt>::SharedPtr m_service_slam_get_pose_at;

    //tf2 static transform broadcaster
    std::shared_ptr<tf2_ros::StaticTransformBroadcaster> m_static_tf_broadcaster;
    
    //tof publisher and service.
    rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr m_service_tof_start;
    rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr m_service_tof_stop;
    image_transport::Publisher m_tof_camera_publisher;
    rclcpp::Publisher<sensor_msgs::msg::CameraInfo>::SharedPtr m_tof_cameraInfo_pub;

    rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr m_service_rgb_start;
    rclcpp::Service<std_srvs::srv::Trigger>::SharedPtr m_service_rgb_stop;
    image_transport::Publisher m_rgb_camera_publisher;
    rclcpp::Publisher<sensor_msgs::msg::CameraInfo>::SharedPtr m_rgb_cameraInfo_pub;

    image_transport::Publisher m_sgbm_camera_publisher;
    rclcpp::Publisher<sensor_msgs::msg::CameraInfo>::SharedPtr m_sgbm_cameraInfo_pub;
    image_transport::Publisher m_sgbm_raw_camera_publisher;
    rclcpp::Publisher<sensor_msgs::msg::CameraInfo>::SharedPtr m_sgbm_raw_cameraInfo_pub;

    size_t count_;
    std::thread m_watchDevThread;
    bool m_stopWatchDevices;
    std::shared_ptr<xv_dev_wrapper> m_device;
    std::string m_devSerialNumber;
    std::map<std::string, std::shared_ptr<xv_dev_wrapper>> m_deviceMap;
    std::map<std::string, std::string> m_frameID;
    std::map<std::string, bool> m_deviceConfig;
};
#endif
```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/share/ros2-wrapper/xv_sdk_ros2/include/xv_dev_wrapper.h`:

```h
#ifndef __XV_DEV_WRAPPER_H__
#define __XV_DEV_WRAPPER_H__

#include <chrono>
#include <fstream>
#include <functional>
#include <memory>
#include <sstream>
#include <string>
#include <thread>
#include <xv-sdk.h>
#include <xv-sdk-ex.h>
#include "sensor_msgs/msg/imu.hpp"
#include "sensor_msgs/msg/camera_info.hpp"
#include "sensor_msgs/msg/image.hpp"
#include <sensor_msgs/image_encodings.hpp>
#include "geometry_msgs/msg/pose_stamped.hpp"
#include "geometry_msgs/msg/transform_stamped.hpp"
#include "builtin_interfaces/msg/duration.hpp"
#include "nav_msgs/msg/path.hpp"
#include "xv_ros2_msgs/msg/orientation_stamped.hpp"
#include "rclcpp/rclcpp.hpp"
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/imgproc/imgproc.hpp>

class xvision_ros2_node;

using namespace xv;
class xv_dev_wrapper
{
public:
    explicit xv_dev_wrapper(xvision_ros2_node* node, std::shared_ptr<xv::Device> device);
    void init(void);
    bool startImuOri(void);
    bool stopImuOri(void);
    bool getImuOri(xv_ros2_msgs::msg::OrientationStamped& oriStamped, const builtin_interfaces::msg::Duration& duration);
    bool getImuOriAt(xv_ros2_msgs::msg::OrientationStamped& oriStamped, const builtin_interfaces::msg::Time &time);
    bool start_slam(void);
    bool stop_slam(void);
    bool slam_get_pose(geometry_msgs::msg::PoseStamped& poseSteamped, const builtin_interfaces::msg::Duration& prediction);
    bool slam_get_pose_at(geometry_msgs::msg::PoseStamped& poseSteamped, const builtin_interfaces::msg::Time& time);
    bool start_tof(void);
    bool stop_tof(void);
    bool start_rgb(void);
    bool stop_rgb(void);

public:
    enum FE_IMAGE_TYPE
    {
        LEFT_IMAGE = 0,
        RIGHT_IMAGE = 1
    };

private:
    void initImu(void);
    void initOrientationStream(void);
    void initFisheyeCameras(void);
    void initSlam(void);
    void initTofCamera();
    void initColorCamera();
    void formatImuTopicMsg(sensor_msgs::msg::Imu &rosImu, const xv::Imu &xvImu);
    void formatXvOriToRosOriStamped(xv_ros2_msgs::msg::OrientationStamped &rosOrientation,
                                    const xv::Orientation& xvOrientation,
                                    const std::string& frameId);
    rclcpp::Time getHeaderStamp(double hostTimesStamp);
    sensor_msgs::msg::CameraInfo toRosCameraInfo(const xv::UnifiedCameraModel* const ucm,
                                                 const xv::PolynomialDistortionCameraModel* const pdcm);
    sensor_msgs::msg::CameraInfo toRosCameraInfo(const xv::SpecialUnifiedCameraModel* const seucm);
    bool registerFECallbackFunc(void);
    bool registerFEAntiDistortionCallbackFunc(void);
    bool registerSGBMCallbackFunc(void);
    sensor_msgs::msg::Image changeFEGrayScaleImage2RosImage(const GrayScaleImage& xvGrayImage, double timestamp, const std::string& frame_id);
    void getFECalibration();
    double get_sec(const builtin_interfaces::msg::Duration& prediction)const;
    double get_sec(const builtin_interfaces::msg::Time& timestamp)const;
    geometry_msgs::msg::PoseStamped to_ros_poseStamped(const Pose& xvPose, const std::string& frame_id);
    builtin_interfaces::msg::Time get_stamp_from_sec(double sec) const;
    double steady_clock_now();
    nav_msgs::msg::Path toRosPoseStampedRetNavmsgs(const Pose& xvPose, const std::string& frame_id, nav_msgs::msg::Path& path);
    geometry_msgs::msg::TransformStamped toRosTransformStamped(const Pose& pose,
                                                      const std::string& parent_frame_id,
                                                      const std::string& frame_id);
    sensor_msgs::msg::Image toRosImage(const DepthImage& xvDepthImage, const std::string& frame_id);
    sensor_msgs::msg::Image toRosImage(const ColorImage& xvColorImage, const std::string& frame_id);
    sensor_msgs::msg::Image toRosImage(const SgbmImage& xvSgbmDepthImage, const std::string& frame_id);
    sensor_msgs::msg::Image sgbmRawDepthtoRosImage(const SgbmImage& xvSgbmDepthImage, const std::string& frame_id);
    cv::Mat toCvMatRGB(const ColorImage& xvColorImage);
    void toRosOrientationStamped(xv_ros2_msgs::msg::OrientationStamped& orientation, Orientation const& xvOrientation, const std::string& frame_id);

private:
    xvision_ros2_node* m_node;
    std::shared_ptr<xv::Device> m_device;
    std::vector<xv::Calibration> m_xvFisheyesCalibs;
    std::vector<std::map<int /*height*/, sensor_msgs::msg::CameraInfo>> m_fisheyeCameraInfos;
    bool m_slam_pose_enable;
    bool m_slam_path_enable;
    bool m_fisheye_enable;
    sensor_msgs::msg::CameraInfo m_tofCameraInfo;
    sensor_msgs::msg::CameraInfo m_rgbCameraInfo;
    xv::Calibration m_xvTofCalib;
    xv::Calibration m_xvRGBCalib;
    xv::CalibrationEx m_xvSgbmCalib;
    sensor_msgs::msg::CameraInfo m_sgbmCamInfo;
};
#endif

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/share/xvsdk/all_stream/colors.h`:

```h
#ifndef COLORS_H
#define COLORS_H

#include <vector>

static std::vector<std::vector<unsigned char>> colors = {{0,0,0},
{255,4,0},
{255,8,0},
{255,12,0},
{255,17,0},
{255,21,0},
{255,25,0},
{255,29,0},
{255,34,0},
{255,38,0},
{255,42,0},
{255,46,0},
{255,51,0},
{255,55,0},
{255,59,0},
{255,64,0},
{255,68,0},
{255,72,0},
{255,76,0},
{255,81,0},
{255,85,0},
{255,89,0},
{255,93,0},
{255,98,0},
{255,102,0},
{255,106,0},
{255,110,0},
{255,115,0},
{255,119,0},
{255,123,0},
{255,128,0},
{255,132,0},
{255,136,0},
{255,140,0},
{255,145,0},
{255,149,0},
{255,153,0},
{255,157,0},
{255,162,0},
{255,166,0},
{255,170,0},
{255,174,0},
{255,179,0},
{255,183,0},
{255,187,0},
{255,191,0},
{255,196,0},
{255,200,0},
{255,204,0},
{255,209,0},
{255,213,0},
{255,217,0},
{255,221,0},
{255,226,0},
{255,230,0},
{255,234,0},
{255,238,0},
{255,243,0},
{255,247,0},
{255,251,0},
{255,255,0},
{251,255,0},
{247,255,0},
{243,255,0},
{238,255,0},
{234,255,0},
{230,255,0},
{226,255,0},
{221,255,0},
{217,255,0},
{213,255,0},
{209,255,0},
{204,255,0},
{200,255,0},
{196,255,0},
{191,255,0},
{187,255,0},
{183,255,0},
{179,255,0},
{174,255,0},
{170,255,0},
{166,255,0},
{162,255,0},
{157,255,0},
{153,255,0},
{149,255,0},
{145,255,0},
{140,255,0},
{136,255,0},
{132,255,0},
{128,255,0},
{123,255,0},
{119,255,0},
{115,255,0},
{110,255,0},
{106,255,0},
{102,255,0},
{98,255,0},
{93,255,0},
{89,255,0},
{85,255,0},
{81,255,0},
{76,255,0},
{72,255,0},
{68,255,0},
{64,255,0},
{59,255,0},
{55,255,0},
{51,255,0},
{46,255,0},
{42,255,0},
{38,255,0},
{34,255,0},
{29,255,0},
{25,255,0},
{21,255,0},
{17,255,0},
{12,255,0},
{8,255,0},
{4,255,0},
{0,255,0},
{0,255,4},
{0,255,8},
{0,255,12},
{0,255,17},
{0,255,21},
{0,255,25},
{0,255,29},
{0,255,34},
{0,255,38},
{0,255,42},
{0,255,46},
{0,255,51},
{0,255,55},
{0,255,59},
{0,255,64},
{0,255,68},
{0,255,72},
{0,255,76},
{0,255,81},
{0,255,85},
{0,255,89},
{0,255,93},
{0,255,98},
{0,255,102},
{0,255,106},
{0,255,110},
{0,255,115},
{0,255,119},
{0,255,123},
{0,255,128},
{0,255,132},
{0,255,136},
{0,255,140},
{0,255,145},
{0,255,149},
{0,255,153},
{0,255,157},
{0,255,162},
{0,255,166},
{0,255,170},
{0,255,174},
{0,255,179},
{0,255,183},
{0,255,187},
{0,255,191},
{0,255,196},
{0,255,200},
{0,255,204},
{0,255,209},
{0,255,213},
{0,255,217},
{0,255,221},
{0,255,226},
{0,255,230},
{0,255,234},
{0,255,238},
{0,255,243},
{0,255,247},
{0,255,251},
{0,255,255},
{0,251,255},
{0,247,255},
{0,243,255},
{0,238,255},
{0,234,255},
{0,230,255},
{0,226,255},
{0,221,255},
{0,217,255},
{0,213,255},
{0,209,255},
{0,204,255},
{0,200,255},
{0,196,255},
{0,191,255},
{0,187,255},
{0,183,255},
{0,179,255},
{0,174,255},
{0,170,255},
{0,166,255},
{0,162,255},
{0,157,255},
{0,153,255},
{0,149,255},
{0,145,255},
{0,140,255},
{0,136,255},
{0,132,255},
{0,128,255},
{0,123,255},
{0,119,255},
{0,115,255},
{0,110,255},
{0,106,255},
{0,102,255},
{0,98,255},
{0,93,255},
{0,89,255},
{0,85,255},
{0,81,255},
{0,76,255},
{0,72,255},
{0,68,255},
{0,64,255},
{0,59,255},
{0,55,255},
{0,51,255},
{0,46,255},
{0,42,255},
{0,38,255},
{0,34,255},
{0,29,255},
{0,25,255},
{0,21,255},
{0,17,255},
{0,12,255},
{0,8,255},
{0,4,255},
{0,0,255},
{4,0,255},
{8,0,255},
{12,0,255},
{17,0,255},
{21,0,255},
{25,0,255},
{29,0,255},
{34,0,255},
{38,0,255},
{42,0,255},
{46,0,255},
{51,0,255},
{55,0,255},
{59,0,255},
{64,0,255}};

#endif // COLORS_H

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/share/xvsdk/demo-api/pipe_srv.h`:

```h
#ifndef __VSC_CLIENT_H_
#define __VSC_CLIENT_H_

#ifdef _WIN32

#else
#include <stdio.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <signal.h>


#define VSC_SRV_RECV_PIPE	"./vscSrvRfifo"   // client write,
#define VSC_SRV_SEND_PIPE	"./vscSrvWfifo"   // client read

#define SRV_CMD_FLAG1	('C')
#define SRV_CMD_FLAG2	('M')
#define SRV_CMD_FLAG3	('D')

#define IS_PIPE_SRV_CMD(cmd1, cmd2, cmd3) \
	((cmd1==SRV_CMD_FLAG1) && (cmd2==SRV_CMD_FLAG2) && (cmd3==SRV_CMD_FLAG3))

static int vsc_pipe_init(void)
{
	int ret;

	if (access(VSC_SRV_RECV_PIPE, F_OK) != 0) {
		ret = mkfifo(VSC_SRV_RECV_PIPE, 0666);
		if (0 != ret) {
			printf("mkfifo server recv pipe fail, ret: %d\n", ret);
			return -1;
		}
		printf("create server recv pipe\n");
	} else {
		printf("pipe server recv has already exist\n");
	}

	if (access(VSC_SRV_SEND_PIPE, F_OK) != 0) {
		ret = mkfifo(VSC_SRV_SEND_PIPE, 0666);
		if (0 != ret) {
			printf("mkfifo server send pipe fail, ret: %d\n", ret);
			return -1;
		}
		printf("create server send pipe\n");
	} else {
		printf("pipe server send has already exist\n");
	}

	return 0;
}

/*--------------  server API -------------------------------------- */
static int vsc_server_get_handle(int *sendfd, int *recvfd)
{
	int ret = 0;

	ret = open(VSC_SRV_SEND_PIPE, O_WRONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for server fail, %d\n", ret);
		return ret;
	}
	*sendfd = ret;
	printf("pipe server get send handle: %d\n", *sendfd);

	ret = open(VSC_SRV_RECV_PIPE, O_RDONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for server fail, %d\n", ret);
		return ret;
	}
	*recvfd = ret;
	printf("pipe server get recv handle: %d\n", *recvfd);

	return 0;
}

static void vsc_srv_release_handle(int *sendfd, int *recvfd)
{
	if (*recvfd > 0)
		close(*recvfd);

	if (*sendfd > 0)
		close(*sendfd);
	return;
}
/*------------------------------------------------------------------------- */
static int vsc_cmd_recv_fd = -1;
static int vsc_cmd_send_fd = -1;
static int vsc_pipe_srv_pid = -1;

static int vsc_client_pipe_init(void)
{
	int ret = 0;

	ret = vsc_pipe_init();
	if (ret != 0) return ret;

	ret = open(VSC_SRV_SEND_PIPE, O_RDONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for client recv fail, %d\n", ret);
		return ret;
	}
	vsc_cmd_recv_fd = ret;
	printf("client pipe get recv handle: %d\n", vsc_cmd_recv_fd);

	ret = open(VSC_SRV_RECV_PIPE, O_WRONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for client send fail, %d\n", ret);
		return ret;
	}
	vsc_cmd_send_fd = ret;
	printf("client pipe get send handle:%d\n", vsc_cmd_send_fd);

	printf("command pipe init for client successful\n");
	return 0;
}

static void vsc_client_pipe_deinit(void)
{
	if (vsc_cmd_send_fd > 0)
		close(vsc_cmd_send_fd);

	if (vsc_cmd_recv_fd > 0)
		close(vsc_cmd_recv_fd);

	return;
}
static int vsc_client_pipe_get_srv_pid()
{
	FILE *fp = popen("ps -e | grep \'pipe_srv\' | awk \'{print $1}\'", "r");
	char buffer[10] = {0};

    if (fp == NULL) {
		printf("Cannot find pipe service process\n");
        return -1;
    }

	while (NULL != fgets(buffer, 10, fp)) {
		vsc_pipe_srv_pid = atoi(buffer);
		printf("vsc pipe server pid: %d\n", vsc_pipe_srv_pid);
	}
	pclose(fp);
	return 0;
}

static int vsc_client_pipe_request_cmd(const char *tip_info, int size)
{
	int cmdbuf[16] = {0};
	int retval;

	retval = write(vsc_cmd_send_fd, tip_info, size);
	retval = read(vsc_cmd_recv_fd, cmdbuf, sizeof(cmdbuf));
	if (IS_PIPE_SRV_CMD(cmdbuf[0], cmdbuf[1], cmdbuf[2])) {
		printf("recv cmd %d\n", cmdbuf[3]);
		return cmdbuf[3];
	}
	return -1;
}

static int vsc_client_pipe_terminal_srv(void)
{
	if (vsc_pipe_srv_pid > 0)
		kill(vsc_pipe_srv_pid, SIGTERM);
	return 0;
}
#endif

#endif //__VSC_CLIENT_H_

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/share/xvsdk/pipe_srv/pipe_srv.h`:

```h
#ifndef __VSC_CLIENT_H_
#define __VSC_CLIENT_H_

#ifdef _WIN32

#else
#include <stdio.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <signal.h>


#define VSC_SRV_RECV_PIPE	"./vscSrvRfifo"   // client write,
#define VSC_SRV_SEND_PIPE	"./vscSrvWfifo"   // client read

#define SRV_CMD_FLAG1	('C')
#define SRV_CMD_FLAG2	('M')
#define SRV_CMD_FLAG3	('D')

#define IS_PIPE_SRV_CMD(cmd1, cmd2, cmd3) \
	((cmd1==SRV_CMD_FLAG1) && (cmd2==SRV_CMD_FLAG2) && (cmd3==SRV_CMD_FLAG3))

static int vsc_pipe_init(void)
{
	int ret;

	if (access(VSC_SRV_RECV_PIPE, F_OK) != 0) {
		ret = mkfifo(VSC_SRV_RECV_PIPE, 0666);
		if (0 != ret) {
			printf("mkfifo server recv pipe fail, ret: %d\n", ret);
			return -1;
		}
		printf("create server recv pipe\n");
	} else {
		printf("pipe server recv has already exist\n");
	}

	if (access(VSC_SRV_SEND_PIPE, F_OK) != 0) {
		ret = mkfifo(VSC_SRV_SEND_PIPE, 0666);
		if (0 != ret) {
			printf("mkfifo server send pipe fail, ret: %d\n", ret);
			return -1;
		}
		printf("create server send pipe\n");
	} else {
		printf("pipe server send has already exist\n");
	}

	return 0;
}

/*--------------  server API -------------------------------------- */
static int vsc_server_get_handle(int *sendfd, int *recvfd)
{
	int ret = 0;

	ret = open(VSC_SRV_SEND_PIPE, O_WRONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for server fail, %d\n", ret);
		return ret;
	}
	*sendfd = ret;
	printf("pipe server get send handle: %d\n", *sendfd);

	ret = open(VSC_SRV_RECV_PIPE, O_RDONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for server fail, %d\n", ret);
		return ret;
	}
	*recvfd = ret;
	printf("pipe server get recv handle: %d\n", *recvfd);

	return 0;
}

static void vsc_srv_release_handle(int *sendfd, int *recvfd)
{
	if (*recvfd > 0)
		close(*recvfd);

	if (*sendfd > 0)
		close(*sendfd);
	return;
}
/*------------------------------------------------------------------------- */
static int vsc_cmd_recv_fd = -1;
static int vsc_cmd_send_fd = -1;
static int vsc_pipe_srv_pid = -1;

static int vsc_client_pipe_init(void)
{
	int ret = 0;

	ret = vsc_pipe_init();
	if (ret != 0) return ret;

	ret = open(VSC_SRV_SEND_PIPE, O_RDONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for client recv fail, %d\n", ret);
		return ret;
	}
	vsc_cmd_recv_fd = ret;
	printf("client pipe get recv handle: %d\n", vsc_cmd_recv_fd);

	ret = open(VSC_SRV_RECV_PIPE, O_WRONLY);
	if (ret < 0) {
		printf("open vsc cmd pipe for client send fail, %d\n", ret);
		return ret;
	}
	vsc_cmd_send_fd = ret;
	printf("client pipe get send handle:%d\n", vsc_cmd_send_fd);

	printf("command pipe init for client successful\n");
	return 0;
}

static void vsc_client_pipe_deinit(void)
{
	if (vsc_cmd_send_fd > 0)
		close(vsc_cmd_send_fd);

	if (vsc_cmd_recv_fd > 0)
		close(vsc_cmd_recv_fd);

	return;
}
static int vsc_client_pipe_get_srv_pid()
{
	FILE *fp = popen("ps -e | grep \'pipe_srv\' | awk \'{print $1}\'", "r");
	char buffer[10] = {0};

    if (fp == NULL) {
		printf("Cannot find pipe service process\n");
        return -1;
    }

	while (NULL != fgets(buffer, 10, fp)) {
		vsc_pipe_srv_pid = atoi(buffer);
		printf("vsc pipe server pid: %d\n", vsc_pipe_srv_pid);
	}
	pclose(fp);
	return 0;
}

static int vsc_client_pipe_request_cmd(const char *tip_info, int size)
{
	int cmdbuf[16] = {0};
	int retval;

	retval = write(vsc_cmd_send_fd, tip_info, size);
	retval = read(vsc_cmd_recv_fd, cmdbuf, sizeof(cmdbuf));
	if (IS_PIPE_SRV_CMD(cmdbuf[0], cmdbuf[1], cmdbuf[2])) {
		printf("recv cmd %d\n", cmdbuf[3]);
		return cmdbuf[3];
	}
	return -1;
}

static int vsc_client_pipe_terminal_srv(void)
{
	if (vsc_pipe_srv_pid > 0)
		kill(vsc_pipe_srv_pid, SIGTERM);
	return 0;
}
#endif

#endif //__VSC_CLIENT_H_

```

`/nix/store/nf93hsf1bm2ck9zldz4yg6bg186hxji0-xvsdk-3.2.0-20230907/include2/xv-sdk-ex.h`:

```h
#pragma once

#include <xv-sdk.h>

#include <mutex>
#include <unordered_map>

namespace x {
    class AprilTagDetector;
}

namespace xv {

/**
 * @brief Perspective Camera Model
 */
struct PerspectiveCameraModel {
    /**
     * @brief Image width (in pixel)
     */
    std::uint16_t w;
    /**
     * @brief Image height (in pixel)
     */
    std::uint16_t h;
    /**
     * @brief Focal length in width direction (in pixel)
     */
    double fx;
    /**
     * @brief Focal length in height direction (in pixel)
     */
    double fy;
    /**
     * @brief Optical axis intersection in width direction (in pixel)
     */
    double u0;
    /**
     * @brief Optical axis intersection in height direction (in pixel)
     */
    double v0;
};

/**
 * @brief Special Extended Unified Camera Model
 */
struct SpecialUnifiedCameraModel
{
  /**
   * @brief Image width (in pixel)
   */
  int w;
  /**
   * @brief Image height (in pixel)
   */
  int h;
  /**
   * @brief Focal length in width direction (in pixel)
   */
  double fx;
  /**
   * @brief Focal length in height direction (in pixel)
   */
  double fy;
  /**
   * @brief Optical axis intersection in width direction (in pixel)
   */
  double u0;
  /**
   * @brief Optical axis intersection in height direction (in pixel)
   */
  double v0;
  /**
   * @brief Optical center of distortion in width direction (in pixel)
   */
  double eu;
  /**
   * @brief Optical center of distortion in height direction (in pixel)
   */
  double ev;
  /**
   * @brief alpha
   */
  double alpha;
  /**
   * @brief beta
   */
  double beta;
};


struct CalibrationEx : public Calibration {
/*
    std::vector<UnifiedCameraModel> ucm; //! List of Unified Camera Model parameters for differents camera resolutions (see UnifiedCameraModel#w and UnifiedCameraModel#h to find the corresponding resolution of the parameter set).
    std::vector<PolynomialDistortionCameraModel> pdcm; //! List of Polynomial Distortion Camera Model parameters for differents camera resolutions (see UnifiedCameraModel#w and UnifiedCameraModel#h to find the corresponding resolution of the parameter set).
*/
    std::vector<SpecialUnifiedCameraModel> seucm; //! List of Polynomial Distortion Camera Model parameters for differents camera resolutions (see UnifiedCameraModel#w and UnifiedCameraModel#h to find the corresponding resolution of the parameter set).
};


/**
 * Features detections and descriptors
 * @tparam F number of Fisheyes
 * @tparam DESC_SIZE number of bytes for a descriptor
 */
template <std::size_t F, std::size_t DESC_SIZE>
struct FisheyeKeyPoints {
    std::int64_t version; //! Field use to help parsing and future evolutions of this struct
    std::int64_t id; //! Unique id given by the edge to this instance
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`), need to activate IMU stream (with SLAM or IMU callback) to have this value.
    std::int64_t edgeTimestampUs = std::numeric_limits<std::int64_t>::min(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    struct Features {
       std::size_t size; //! number of feature detected
       std::shared_ptr<const float> keypoints; //! list of detected keypoints (xy) in image (size x 2 float elements)
       std::shared_ptr<const unsigned char> descriptors; //! list of (DESC_SIZE x size) keypoints descriptors in image
    };
    std::array<Features,F> descriptors; //! usually first is left and second is right, it is the same order fisheyes calibrations

    /**
     * @brief Size of a feature descriptor
     */
    std::size_t descriptorSize() const { return DESC_SIZE; }
};

#if 0
struct FisheyeKeyPointsDescriptor {
    std::int64_t version; //! Field use to help parsing and future evolutions of this struct
    std::int64_t id; //! Unique id given by the edge to this instance
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`), need to activate IMU stream (with SLAM or IMU callback) to have this value.
    std::int64_t edgeTimestampUs = std::numeric_limits<std::int64_t>::min(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    struct Features {
       std::size_t size; //! number of feature detected
       std::shared_ptr<const float> keypoints; //! list of detected keypoints (xy) in image (size x 2 float elements)
       std::shared_ptr<const unsigned char> descriptors; //! list of (DESC_SIZE x size) keypoints descriptors in image
    };
    std::vector<Features> descriptors; //! usually first is left and second is right, it is the same order fisheyes calibrations

    /**
     * @brief Size of a feature descriptor
     */
    std::size_t descriptorSize() const { return 32; }
};
#endif

enum class StereoMode {
    IMAGES = 0, ///< Images only
    IMAGES_AND_DATA, ///< Images and data
    DATA,  ///< Data only
    NO_STREAMS, ///< No stream
};


/**
 * @brief Data from IMU sensor for the XVisio handle.
 */
struct HandleImu {
    std::array<unsigned char,64> raw;

    //enum class Position {Head = 0, Left, Right};
    //enum class DataType {Init = 0, Work};

    //Position position; //!< 3-axis gyrometer values of left handle (in rad/s)
    //DataType dataType; //!< 3-axis gyrometer values of left handle (in rad/s)
    //int status;
    //Vector3d lgyro; //!< 3-axis gyrometer values of left handle (in rad/s)
    //Vector3d rgyro; //!< 3-axis gyrometer values of right handle (in rad/s)
    //Vector3d laccel; //!< 3-axis accelerometer values of left handle (in m/s²)
    //Vector3d raccel; //!< 3-axis accelerometer values of right handle (in m/s²)
    //double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`).
    //std::uint32_t edgeTimestampUs = std::numeric_limits<std::uint32_t>::min(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
};


/**
 * @brief The class to give access to data provided by the IMU sensor.
 */
class HandleImuSensor : virtual public Stream<HandleImu const &> {
public:

    virtual ~HandleImuSensor(){}
};


/**
 * @brief For senior developer.
 */
class DeviceEx : virtual public Device {

public:

    static std::shared_ptr<DeviceEx> instance; /// FIXME not compatible with multi-device

    /**
     * @brief Get the secondary SLAM component to make edge and mixed slam work at sametime. Default in edge mode.
     */
    virtual std::shared_ptr<Slam> slam2() = 0;

    virtual std::shared_ptr<HandleImuSensor> handleImuSensor() = 0;
    virtual std::shared_ptr<FisheyeCameras> handleFisheyeCameras() = 0;

    /**
     * @brief Start use external IMU.
     */
    virtual void enableImuInput() = 0;
    /**
     * @brief Stop use external IMU.
     */
    virtual void disableImuInput() = 0;
    /**
     * @brief Send external IMU to SDK.
     */
    virtual void pushImu(Imu const& imu, bool with_bias_correction=false, bool with_timestamp_correction=false) = 0;

    enum class StereoInputType {ImageOnly, KeyPointsOnly, Both, None};

    /**
     * @brief Start use external fisheye image and/or key points.
     *
     * only image,only key points,both
     */
    virtual bool enableStereoInput(StereoInputType type) = 0;
    /**
     * @brief Stop use external fisheye image and/or key points.
     */
    virtual bool disableStereoInput() = 0;
    /**
     * @brief Send external fisheye image and/or key points to SDK.
     */
    virtual void pushStereo(FisheyeKeyPoints<2,32> const& stereo) = 0;
    /**
     * @brief Send external fisheye image to SDK.
     */
    virtual void pushStereo(FisheyeImages const& stereo) = 0;
    /**
     * @brief Send external 4-cameras fisheye image and/or key points to SDK.
     */
    virtual void push4Cameras(FisheyeKeyPoints<4,32> const& stereo) = 0;

    virtual bool setDisplayCalibration(const std::vector<CalibrationEx>&) = 0;
    virtual bool setRgbCalibration(const std::vector<CalibrationEx>&) = 0;
    virtual bool setTofCalibration(const std::vector<CalibrationEx>&) = 0;
    virtual bool getFisheyeCalibration(std::vector<CalibrationEx>&, double& imuFisheyeTimestampOffset) = 0;
    virtual bool setFisheyeCalibration(const std::vector<CalibrationEx>&, double imuFisheyeTimestampOffset) = 0;
    virtual bool setEyetrackingCalibration(const std::vector<CalibrationEx>&) = 0;

    virtual bool setImuOffset( int offset ) = 0;
    virtual bool setImuMode( int mode ) = 0;


    virtual bool setFeMode( StereoMode mode ) = 0;

    virtual bool setEdgePrediction( int prediction ) = 0;

    /**
     * @brief Get the third SLAM component to make edge and mixed slam work at sametime. Default is edge with fusion on host mode.
     */
    virtual std::shared_ptr<Slam> slam3() = 0;

    /**
     * @brief Return the customize flash array.
     */
    virtual std::string getCustomizeFlash48BytesArray1() = 0;

    /**
     * @brief Set the customize flash array.
     */
    virtual bool setCustomizeFlash48BytesArray1(std::string flashArray) = 0;

    virtual void setDeviceOffsetStatus() = 0;


    virtual ~DeviceEx() { }
};

/**
 * @deprecated
 */
struct FisheyeImageKeyPoints {
    double hostTimestamp = std::numeric_limits<double>::infinity(); //!< host timestamp of the physical measurement (in second based on the `std::chrono::steady_clock`), need to activate IMU stream (with SLAM or IMU callback) to have this value.
    std::int64_t edgeTimestampUs = (std::numeric_limits<std::int64_t>::min)(); //!< timestamp of the physical measurement (in microsecond based on edge clock).
    struct Detections {
        std::vector<Vector2<std::uint16_t>> keypoints; //! list of detected keypoints in image
        std::vector<std::array<std::int16_t,16>> descriptors; //! list of keypoints descriptors in image
        GrayScaleImage image; //! Fisheye image
    };
    std::vector<Detections> detections; //! List of image detections (typically 2, first is left and second image is the right image)
    std::int64_t id;//! Unique id given by the edge to this instance
};

struct TagInfo {
  std::string family;
  int id;
  double size;
};

struct TagPoseInfo : public TagInfo {
  xv::Transform transform;
};

struct TagPose {
    int tagId;
    xv::Transform transform;
    double confidence; // in [0,1]
};

struct TagDetection {
    int tagId;
    std::array<std::array<double,2>,4> corners;
    double confidence; // in [0,1]
};

class AprilTagDetector {
public:
    /**
     * @brief Construct an AprilTag detector
     * @param c multi camera calibration
     * @param f name of the AprilTag family to use (support: "36h11" "25h9" "16h5" and "14h12")
     */
    explicit AprilTagDetector(std::vector<xv::CalibrationEx> const& c, std::string const& f="36h11", bool subpixelic=false);

    /**
     * @brief Construct an AprilTag detector on single view
     * @param c camera intrinsics
     * @param camerPose camera pose
     * @param f name of the AprilTag family to use (support: "36h11" "25h9" "16h5" and "14h12")
     */
    explicit AprilTagDetector(xv::PolynomialDistortionCameraModel const& c, xv::Transform const& camerPose, std::string const& f="36h11", bool subpixelic=false);

    /**
     * @brief Construct an AprilTag detector on single view
     * @param c camera calibration
     * @param camerPose camera pose
     * @param f name of the AprilTag family to use (support: "36h11" "25h9" "16h5" and "14h12")
     */
    explicit AprilTagDetector(xv::UnifiedCameraModel const& c, xv::Transform const& camerPose, std::string const& f="36h11", bool subpixelic=false);

    /**
     * @brief Construct an AprilTag detector on single view
     * @param c camera calibration
     * @param camerPose camera pose
     * @param f name of the AprilTag family to use (support: "36h11" "25h9" "16h5" and "14h12")
     */
    explicit AprilTagDetector(xv::SpecialUnifiedCameraModel const& c, xv::Transform const& camerPose, std::string const& f="36h11", bool subpixelic=false);

    /**
     * @brief Construct an AprilTag detector without camera calibration (only 2D detections are available)
     * @param c multi camera calibration
     * @param f name of the AprilTag family to use (support: "36h11" "25h9" "16h5" and "14h12")
     */
    explicit AprilTagDetector(std::string const& f="36h11", bool subpixelic=false);

    /**
     * @brief Detect AprilTags in Fisheye Images and return the poses of the tags
     * @param tagSize : of the side of the april tag to detect (in m)
     * @return vector of pairs with tag id and pose of the tag
     */
    std::vector<TagPose> detect(xv::FisheyeImages const& fe, double tagSize) const;

    /**
     * @brief Detect AprilTags in a grayscale image and return the poses of the tags
     * @param tagSize : of the side of the april tag to detect (in m)
     * @return vector of pairs with tag id and pose of the tag
     */
    std::vector<TagPose> detect(xv::GrayScaleImage const& im, double tagSize) const;

    /**
     * @brief Detect AprilTags in a grayscale image and return the 4 corners of each detected tag
     * @return vector of pairs with tag id and 4 corners of the tag (in pxl)
     */
    std::vector<TagDetection> detect(const xv::GrayScaleImage& fe);

    /**
     * @brief Compute the poses of the tags detections
     * @param detections: tag detections
     * @param tagSize: size of the tag (in m)
     * @return the tag poses
     */
    std::vector<TagPose> detectionsToPoses(const std::vector<TagDetection>& detections, double tagSize);

    /**
     * @brief Detect AprilTags in a multiple cameras system and return the 4 corners of each detected tag
     * @return map of detections by tag id, each vector of tag detection has the size of the number of images
     */
    std::map<int, std::vector<TagDetection> > detect(const xv::FisheyeImages &fe);
    /**
     * @brief Compute the poses of the tags detections for multiple cameras case
     * @param detections: tag detections grouped by tagId, each vector of tag detection has the size of the number of images
     * @param tagSize: size of the tag (in m)
     * @return the tag poses
     */
    std::vector<TagPose> detectionsToPoses(const std::map<int, std::vector<TagDetection> > &detectionsByTagId, double tagSize);

    /**
     * @brief Start a tag detectors
     * @param slam : SLAM to use for localisation of the tag. The detected tags will be in world frame coordinates as defined by the SLAM.
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @param refreshRate : the refresh rate used for the detection (in Hz)
     * @return Id of the started detector.
     */
    static std::string startTagDetector(std::shared_ptr<xv::FisheyeCameras>, std::shared_ptr<Slam> slam, std::string const& tagFamily, double size, double refreshRate);
    /**
     * @brief Start a tag detectors
     * @param slam : SLAM to use for localisation of the tag. The detected tags will be in world frame coordinates as defined by the SLAM.
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @param refreshRate : the refresh rate used for the detection (in Hz)
     * @return Id of the started detector.
     */
    static std::string startTagDetector(std::shared_ptr<xv::ColorCamera>, std::shared_ptr<Slam> slam, std::string const& tagFamily, double size, double refreshRate);

    /**
     * @brief Stop a tag detector.
     * @param detectorId : detector id (see output of #startTagDetector)
     * @return True if succeeded to stop the detector.
     */
    static bool stopTagDetector(std::string const& detectorId);
    /**
     * @brief Get the current localized tag detections in SLAM world frame coordinates.
     *
     * @param detectorId : detector id
     * @return Poses of all the detected tags, even if the tag is not visible, if it was once detected it remains in this output map.
     */
    static std::map<int,xv::Pose> getTagDetections(std::string const& detectorId);


private:
    std::shared_ptr <x::AprilTagDetector> pimpl;
};


class ColorCameraEx : public ColorCamera, public std::enable_shared_from_this<ColorCameraEx> {

public:
    std::shared_ptr<ColorCameraEx> getThis();

    /**
     * @brief Start a tag detectors
     * @param slam : SLAM to use for localisation of the tag. The detected tags will be in world frame coordinates as defined by the SLAM.
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @param refreshRate : the refresh rate used for the detection (in Hz)
     * @return Id of the started detector.
     */
    std::string startTagDetector(std::shared_ptr<Slam> slam, std::string const& tagFamily, double size, double refreshRate);
    /**
     * @brief Stop a tag detector.
     * @param detectorId : detector id (see output of #startTagDetector)
     * @return True if succeeded to stop the detector.
     */
    bool stopTagDetector(std::string const& detectorId);
    /**
     * @brief Get the current localized tag detections in SLAM world frame coordinates.
     *
     * @param detectorId : detector id
     * @return Poses of all the detected tags, even if the tag is not visible, if it was once detected it remains in this output map.
     */
    std::map<int,xv::Pose> getTagDetections(std::string const& detectorId);
};

/**
 * @brief For senior developer.
 */
class FisheyeCamerasEx : public FisheyeCameras, public std::enable_shared_from_this<FisheyeCamerasEx>
{

    std::mutex m_aprilTagDetectorsMtx;
    std::unordered_map<std::string, std::shared_ptr<AprilTagDetector>> m_aprilTagDetectors;
    std::shared_ptr<AprilTagDetector> getDetector(std::string const& tagFamily);

    std::mutex m_lastFisheyeImageMtx;
    xv::FisheyeImages m_lastFisheyeImage;
    int m_lastFisheyeImageCbId = -1;

    class TagDetectorThread;

    std::shared_ptr<FisheyeCamerasEx> getThis();

public:

    virtual int registerKeyPointsCallback(std::function<void (const FisheyeKeyPoints<2,32>&)>) = 0;
    virtual int registerKeyPointsCallback(std::function<void (const FisheyeKeyPoints<4,32>&)>) = 0;
    virtual bool unregisterKeyPointsCallback(int callbackId) = 0;
    virtual bool unregisterKeyPoints4Callback(int callbackId) = 0;

    virtual const std::vector<CalibrationEx>& calibrationEx() = 0;
    virtual const std::vector<CalibrationEx>& defaultcalibration() = 0;
    virtual xv::FisheyeImages lastImages() = 0;

    /**
     * @brief The ResolutionMode enum
     */
    enum class ResolutionMode {
        LOW = 1, ///< Low resolution (typically QVGA)
        MEDIUM = 2, ///< Medium resolution (typically VGA)
        HIGH = 3 ///< High resolution (typically HD 720)
    };
    virtual bool setResolutionMode(ResolutionMode mode) = 0;

    /**
     * @brief Detect tags in a fisheye image
     * @param img : fisheye image to used for detection
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @return Vector of tag detections, each tag detection is a pair containing the tag id (AprilTag id) and four corners coordinates (in pixel) of the detected tags.
     */
    std::vector<std::pair<int,std::array<Vector2d,4>>> detectTags(xv::GrayScaleImage const& img, std::string const& tagFamily);

    /**
     * @brief Detect tags in the last fisheye images
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @return Vector of tag detections, each tag detection is a pair containing the tag id (AprilTag id) and the 6dof poses (in Fisheyes frame coordinates).
     * The confidence of the pose reflects the confidence of the tag detection.
     */
    std::vector<std::pair<int,xv::Pose>> detectTags(std::string const& tagFamily, double size);
    /**
     * @brief Detect tags in fisheye images and return poses of the tags
     * @param fe : fisheye images to use for detection
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @return Vector of tag detections, each tag detection is a pair containing the tag id (AprilTag id) and the 6dof poses (in Fisheyes frame coordinates).
     * The confidence of the pose reflects the confidence of the tag detection.
     */
    std::vector<std::pair<int,xv::Pose>> detectTags(xv::FisheyeImages const& fe, std::string const& tagFamily, double size);
    /**
     * @brief Detect tags in the last fisheye images and return poses of the tags in SLAM world frame coordinates
     * @param slam : the slam to use for localization of detection in SLAM world frame coordinates
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @return Vector of tag detections, each tag detection is a pair containing the tag id (AprilTag id) and the 6dof poses (in Fisheyes frame coordinates).
     * The confidence of the pose reflects the confidence of the tag detection.
     */
    std::vector<std::pair<int,xv::Pose>> detectTags(std::shared_ptr<Slam> slam, std::string const& tagFamily, double size);

    /**
     * @brief Start a tag detectors
     * @param slam : SLAM to use for localisation of the tag. The detected tags will be in world frame coordinates as defined by the SLAM.
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param size : size in m of the real tag side
     * @param refreshRate : the refresh rate used for the detection (in Hz)
     * @return Id of the started detector.
     */
    std::string startTagDetector(std::shared_ptr<Slam> slam, std::string const& tagFamily, double size, double refreshRate);
    /**
     * @brief Stop a tag detector.
     * @param detectorId : detector id (see output of #startTagDetector)
     * @return True if succeeded to stop the detector.
     */
    bool stopTagDetector(std::string const& detectorId);
    /**
     * @brief Get the current localized tag detections in SLAM world frame coordinates.
     *
     * @param detectorId : detector id
     * @return Poses of all the detected tags, even if the tag is not visible, if it was once detected it remains in this output map.
     */
    std::map<int,xv::Pose> getTagDetections(std::string const& detectorId);

    virtual DeviceEx::StereoInputType externalStereoInputType() const = 0;
    virtual bool getAecParameters(IspAecSetting& params) = 0;
    virtual void setAecParameters(const IspAecSetting& params) = 0;

    virtual ~FisheyeCamerasEx() { }

};

/**
 * @brief Compute the pixel shift to go from tracker pose p0 to tracker pose p1
 *
 * If p0 and p1 are at the same translation (rotation onlty) this function can be used for ATW and the parameter d is useless. If p0 and p1 are with different translation the pixel shift correspond
 * to a virtual object at d meter from the display.
 *
 * @param p0 : tracker pose
 * @param p1 : tracker pose (need to be in same frame coordinate as p)1
 * @param displayExtrinsics : display pose in tracker pose
 * @param displayCalib : display intrinsics
 * @param d : distance of the virtual object to use to estimate the pixel shift
 * @return (x,y) pixel shift corresponding to the motion from p0 to p1
 */
xv::Vector2d getPixelShift(xv::Pose const& p0, xv::Pose const& p1, xv::Transform const& displayExtrinsics, xv::CameraModel const& displayCalib, double d=1.);

class CameraEx : public Camera {
public:
    virtual const std::vector<CalibrationEx>& calibrationEx() = 0;
    virtual std::shared_ptr<CameraModel> cameraModel() { return nullptr; }
    virtual std::vector<std::shared_ptr<CameraModel>> camerasModel() {
        if (cameraModel())
            return {cameraModel()};
        else
            return {};
    }
};

class DisplayEx : public Display {
public:
    virtual const std::vector<CalibrationEx>& calibrationEx() = 0;
    virtual std::vector<std::shared_ptr<CameraModel>> camerasModel() {
            return {};
    }
};

namespace ex {

struct PointCloud
{
  std::int32_t version;
  std::uint64_t id;
  std::uint32_t xyznSize;
  std::shared_ptr<const std::array<float,6>> xyzn; //!< oriented point cloud in sensor frame
  Transform sensorPose;                            //!< sensor pose in world
};

struct PointClouds {
    std::map<std::uint64_t, xv::ex::PointCloud> pointClouds;
};

struct Surface
{
  std::int32_t version;
  std::uint64_t id;

  std::uint32_t verticesSize;
  std::shared_ptr<const std::array<float,3>> vertices;
  std::shared_ptr<const std::array<float,3>> vertexNormals;

  std::uint32_t trianglesSize;
  std::shared_ptr<const std::array<std::uint32_t,3>> triangles;

  std::shared_ptr<const std::array<float,2>> textureCoordinates; //!< one per vertex
  std::uint32_t textureWidth;
  std::uint32_t textureHeight;
  std::shared_ptr<const std::uint8_t> textureRgba;    //!< row major
};

struct Surfaces {
    std::map<std::uint64_t, xv::ex::Surface> surfaces;
};

}

class SlamEx : public Slam {

protected:

    bool m_enableOnlineLoopClosure = false;
    bool m_enableSurfaceReconstruction = false;
    bool m_enableSurfacePlanes = false;
    bool m_enableSurfaceInstantReconstruction = false;
    bool m_enableSurfaceInstantPlanes = false;
    bool m_enableSurfaceTexturing = false;
    bool m_enableSurfaceMultiResolutionMesh = false;
    bool m_enableSurfaceMobileObjects = false;
    bool m_surfaceUseFisheyes = false; //!< instead of Tof as depth source
    bool m_surfaceUseFisheyeTexturing = true; //!< fisheye texturing instead of RGB texturing
    double m_surfaceMinVoxelSize = 0.1;
    bool m_useAccel = true; //!< use the IMU acceleration (true by default)

public:

    virtual void setEnableOnlineLoopClosure(bool enable) { m_enableOnlineLoopClosure = enable; }
    virtual void setEnableSurfaceReconstruction(bool enable) { m_enableSurfaceReconstruction = enable; }
    virtual void setEnableSurfacePlanes(bool enable) { m_enableSurfacePlanes = enable;}
    virtual void setEnableSurfaceInstantReconstruction(bool enable) { m_enableSurfaceInstantReconstruction = enable; }
    virtual void setEnableSurfaceInstantPlanes(bool enable) { m_enableSurfaceInstantPlanes = enable; }
    virtual void setEnableSurfaceTexturing(bool enable) { m_enableSurfaceTexturing = enable; }
    virtual void setEnableSurfaceMultiResolutionMesh(bool enable) { m_enableSurfaceMultiResolutionMesh = enable;}
    virtual void setEnableSurfaceMobileObjects(bool enable) { m_enableSurfaceMobileObjects = enable;}
    virtual void setSurfaceUseFisheyes(bool use) { m_surfaceUseFisheyes = use;}
    virtual void setSurfaceUseFisheyeTexturing(bool use) { m_surfaceUseFisheyeTexturing = use;}
    virtual void setSurfaceMinVoxelSize(double size) { m_surfaceMinVoxelSize = size; }

    virtual void setUseAccel(bool use) { m_useAccel = use; }

    virtual int registerLocal3dPointsCallback(std::function<void (std::shared_ptr<const std::vector<std::array<double,3>>>)>) = 0;
    virtual bool unregister3dPointsCallback(int callbackId) = 0;
    virtual bool getLastVSlamPose(Pose &) { return false; }

    virtual bool startSurfaceReconstruction() { return false; }
    virtual bool stopSurfaceReconstruction() { return false; }
    virtual bool startPlaneDetection() { return false; }
    virtual bool stopPlaneDetection() { return false; }

    /**
     * @brief Define a map of tags poses
     * @param tagFamily : can be "41h12" "36h11" "25h9" or "16h5" (AprilTag)
     * @param tagSize : size in m of the real tag side
     * @param tagIds : AprilTag id of each tag
     * @param poses : poses of the AprilTag in a world frame coordinate. This world frame coordinates will then be used
     */
    virtual void setTagsMap(std::string const& tagFamily, double tagSize, std::vector<int> tagIds, std::vector<xv::Transform> poses) = 0;

    /**
     * @brief Get the current 6dof pose of the device in tags map
     *
     * Same as #getPose but return true even if it can localize on the map defined by #setTagsMap. The output pose is relative to this tags map thanks to the tags detection.
     *
     * @param[out] pose corresponding to the timestamp "now" + "prediction"
     * @param[in] prediction (in s) amount of prediction to use to get a pose corresponding to the future
     * @return true if localized on tag map, false else.
     */
    virtual bool getPoseInTagsMap(Pose& pose, double prediction = 0.) = 0;

    /**
    * @brief Callback to get the reconstructed pointcloud (is computed faster than surface reconstruction but is not a mesh).
    * @return Id of the callback (used to unregister the callback).
    */
    virtual int registerPointCloudCallback(std::function<void (std::shared_ptr<const ex::PointClouds>)>) = 0;
    virtual bool unregisterPointCloudCallback(int callbackId) = 0;
    virtual bool getPointCloud(std::shared_ptr<const ex::PointClouds>&) = 0;

    /**
     * @brief Callback to get the reconstructed surface updates.
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerSurfaceCallback(std::function<void (std::shared_ptr<const xv::ex::Surfaces>)>) = 0;
    virtual bool unregisterSurfaceCallback(int callbackId) = 0;
    virtual bool getSurface(std::shared_ptr<const xv::ex::Surfaces>&) = 0;

    /**
     * @brief Callback to get the point matches updates.
     * @return Id of the callback (used to unregister the callback).
     */
    virtual int registerPointMatchesCallback(std::function<void (std::shared_ptr<const xv::PointMatches>)>) = 0;
    virtual bool unregisterPointMatchesCallback(int callbackId) = 0;
    /**
     * @brief Set the tag tos use in SLAM; must be done before calling slam->start()
     * @param v : list of tags
     * @return success
     */
    virtual bool addTags(std::vector<TagInfo> const& v) = 0;

    /**
     * @brief Get the tags used in the SLAM
     * @param tagId : unique tag identifier
     * @param pose : pose of the tag in the SLAM coordinate frame
     * @param tagSize : size of the tag
     * @return success
     */
    virtual bool onTagUpdate(std::function<void(std::string const& tagId, xv::Transform const& pose, double const& tagSize)>) = 0;
};

/**
 * @brief To compute the 3D position of 2D pixel point of RGB image.
 *
 * It uses raytrace of RGB pixel and ToF image to get the depth. If SLAM is running, then the output 3D position is in World frame coordinate of the SLAM,
 * else it is relative to the IMU frame coordinate.
 */
class RgbPixelPoseWithTof {

public:
    RgbPixelPoseWithTof(std::shared_ptr<xv::Device> d);

    /**
     * @brief Get the position of the pointed area in RGB image, it uses ToF to determine the depth.
     *
     * If SLAM is running, then the output pointerPose is in World frame coordinate of the SLAM, else it is relative to the IMU frame coordinate.
     *
     * @param pointerPose: 3D position in World frame coordinate of the point selected in RGB
     * @param hostTimestamp: timestamp corresponding to the pointing, if no SLAM is running, set this value to -1
     * @param rgbPixelPoint: xy position of the point in color image (in pixel)
     * @param radius: size of the area to select for ToF 3D points selection
     * @return true if succes, false else
     */
    bool getRgbPixel3dPoseAt(xv::Vector3d& pointerPose, double hostTimestamp, xv::Vector2d const& rgbPixelPoint, double radius);

private:
    class Impl;
    std::unique_ptr<Impl> pImpl;

public:
    ~RgbPixelPoseWithTof();
};

/**
* eye data function pointer
*/
typedef void (* xv_ET_point_process_callback)(int index, int percent, void* context);//!<Callback function used to receive calibration progress of calibration point.
typedef void (* xv_ET_point_finish_callback)(int index, int error, void* context);//!<Callback function used to receive the completion status of calibration point.

/**
 * @brief A class to handle interfaces of the gaze calibration operations.
 */
class GazeCalibration{
public:

    /**
     * @brief Start calibration
     *
     * @param[in] points Total number of calibration points.
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
	 * -212 or -7001 Parameter error
     */
    int StartCalibration(int points);

    /**
     * @brief Start calibrating a point
     *
     * @param[in] eye Calibrate the left and right eyes, 1-left eye and 2-right eye. Note that the left and right eyes are not calibrated together and must be calibrated separately.
     * @param[in] index Calibration point index.
     * @param[in] point Datum coordinates of calibration points (normalized values are used for X and y).
     * @param[in] cb1 The callback when this point is calibrated.
     * @param[in] context1 The callback corresponding to CB1 is used to pass the context of the caller, can be empty.
     * @param[in] cb2 The callback when eyeball information of each image is recalled.
     * @param[in] context2 The callback corresponding to CB2 is used to pass the context of the caller, can be empty.
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
	 * -221 Eye is not set.
     * -221222 Parameter error, start calibration index point error.
     */
    int StartCalibrationPoints(int eye, int index, const xv::XV_ET_POINT_2D* point, 
        xv::xv_ET_point_process_callback cb1, void* context1, 
        xv::xv_ET_point_finish_callback cb2, void* context2);

    /**
     * @brief Cancel calibration
     *
     * @param[in] eye Calibrate the left and right eyes, 1-left eye and 2-right eye. Note that the left and right eyes are not calibrated together and must be calibrated separately.
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
	 * -241 Eye is not set.
     */
    int CancelCalibration(int eye);

    /**
     * @brief Compute calibration
     *
     * @param[in] eye Calibrate the left and right eyes, 1-left eye and 2-right eye. Note that the left and right eyes are not calibrated together and must be calibrated separately.
     * @param[out] out_coe Calibration coefficient.
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
     * -251 Eye is not set.
     * -253 Calibration coefficient parameter is null.
	 * -251252 Parameter error
     */
    int ComputeCalibration(int eye, xv::XV_ET_COEFFICIENT* out_coe);

    /**
     * @brief Complete calibration
     *
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
     */
    int CompleteCalibration();

    /**
     * @brief Set the calibration range and default calibration factor.
     *
     * @param[in] eye Calibrate the left and right eyes, 1-left eye and 2-right eye. Note that the left and right eyes are not calibrated together and must be calibrated separately.
     * @param[in] minX Minimum value of X coordinate system.
     * @param[in] maxX Maximum value of X coordinate system.
     * @param[in] minY Minimum value of Y coordinate system.
     * @param[in] maxY Maximum value of Y coordinate system.
     * @param[in] coe Default coefficient, can be null.
     * @return int 
	 *  0 success
	 * -1 Failed to start calibration, no permission.
	 * -2 Initialization failed.
     * -3 Wrong sdk version.
     * -271 Eye is not set.
	 * -272 Parameter error
     */
    int SetDefaultCalibration(int eye, float minX, float maxX, float minY, float maxY, const xv::XV_ET_COEFFICIENT* coe);

    /**
     * @brief Input camera image
     *
     * @param[in] image Input image.
     * @param[in] size Image size.
     * @param[in] width Image width.
     * @param[in] height Image height.
     * @param[in] timestamp Image timestamp.
     * @return int 
	 *  0 success
     */
    int InputCameraImage(const unsigned char* image, int size, int width, int height, long long timestamp) ;

    /**
    @brief Enter calibration mode.

    This function should be called before other calibration API.
    @param eyetracker: Eyetracker handle.
    @returns A @ref code.
    */
    GazeStatus CalibrationEnter();

    /**
    @brief Leave calibration mode.

    This function should be called when calibration process is finished.
    @param eyetracker: Eyetracker handle.
    @param result: Returned result of the latest calibration routine.
    @returns A @ref code.
    */
    GazeStatus CalibrationLeave(int* result);

    /**
    @brief Collect 3D gaze point position based on the eye tracking coordinates system gazed by user for calculating calibration parameters.

    @param eyetracker: Eyetracker handle.
    @param x: The x coordinate of the collected point. (unit: mm)
    @param y: The y coordinate of the collected point. (unit: mm)
    @param z: The z coordinate of the collected point. (unit: mm)
    @param index: The index of the collected point.
    @param status: The return status for the collection.
    @returns A @ref code.
    */
    GazeStatus CalibrationCollect(float x, float y, float z, int index, int *status);

    /**
    @brief Retrieve the calibration parameters data.

    After retrieve the calibration parameters, you can save them and restore the calibration parameters by CalibrationApply in the next time to skip calibration process.

    @param eyetracker: Eyetracker handle.
    @param data: The calibration parameters data returned.
    @returns A @ref code.
    */
    GazeStatus CalibrationRetrieve(GazeCalibrationData** data);

    /**
    @brief Apply the calibration parameters data.

    In order to restore the calibration parameters, you can apply the data got from @ref CalibrationRetrieve.

    After that, you can skip calibration process.
    @param eyetracker: Eyetracker handle.
    @param data: The calibration parameters data to restore.
    @returns A @ref code.
    */
    GazeStatus CalibrationApply(GazeCalibrationData* data);

    /**
    @brief Clear all the collected calibration points and reset the calibration parameters.
    @param eyetracker: Eyetracker handle.
    @returns A @ref code.
    */
    GazeStatus CalibrationReset();

    /**
    @brief Compute calibration parameters and apply it to device.
    This function should be called after all the calibration points are collected.
    @param eyetracker: Eyetracker handle.
    @returns A @ref code.
    */
    GazeStatus CalibrationComputeApply();

    /**
    @brief Setup for calibration process
    This function should be called before collect calibration points.
    @param eyetracker: Eyetracker handle.
    @returns A @ref code.
    */
    GazeStatus CalibrationSetup();

    /**
    @brief Query calibration routine internal status
    This function could be called anytime to query the status of calibration routine API.
    @param eyetracker: Eyetracker handle.
    @param calib_status: Returned a struct represented each calibration API's status.
    @returns A @ref code.
    */
    GazeStatus CalibrationQueryStatus(CalibrationStatus *calib_status);

};

/**
 * @brief Obtain a virtual #Device, and input sensor data to the sdk from caller.
 * @param deviceId: identifier of the device to create
 * @return A #Device.
 */
std::shared_ptr<Device> getVirtualDevice(std::string const& deviceId="");

}

```